{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: translating number words into numbers (e.g. one two three -> 1 2 3)\n",
    "\n",
    "* Base Seq2seq with training-wheels on, i.e. decoder gets manually fed inputs (e.g. 1 2 3) to produce results (e.g. 1 2 3). For architecture testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Pang2 Bai2\n",
    "\n",
    "# First-step enc-dec model\n",
    "#\n",
    "#                     decoder \n",
    "#                     target\n",
    "#\n",
    "#  [] -> [] -> [#] -> [] -> []\n",
    "#\n",
    "#  encoder            decoder\n",
    "#  inputs             inputs\n",
    "#\n",
    "# Next step: decoder_input = f(encoder_last_state) or f(prev_decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(['PAD','EOS','1','2','3','4','5','one','two','three','four','five'])\n",
    "word2idx = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,\n",
    "            'one':7,'two':8,'three':9,'four':10,'five':11}\n",
    "idx2word = {idx:word for word,idx in word2idx.iteritems()}\n",
    "word2digit_translate = {'one':'1','two':'2','three':'3',\n",
    "                        'four':'4','five':'5'}\n",
    "word2digit_translate_byidx = {7:2,8:3,9:4,10:5,11:6}\n",
    "\n",
    "def code_sequence(s):\n",
    "    '''\n",
    "    Take a sentence, convert it to a list of words (in vocab), \n",
    "    then return idx encoding.\n",
    "    '''\n",
    "    seq = s.split()\n",
    "    return [word2idx[word] for word in seq]\n",
    "\n",
    "def decode_sequence(l):\n",
    "    '''\n",
    "    Take a list of indices, return words.\n",
    "    '''\n",
    "    return ' '.join([idx2word[idx] for idx in l])\n",
    "\n",
    "def encode(data):\n",
    "    '''\n",
    "    Take sentence data, encode it.\n",
    "    '''\n",
    "    return [code_sequence(dat) for dat in data]\n",
    "\n",
    "def to_readable(batch):\n",
    "    '''\n",
    "    Take a time-major batch of data, \n",
    "    return a list of translated words.\n",
    "    '''\n",
    "    batch_t = batch.transpose() # time-major -> batch-major\n",
    "    return [decode_sequence(dat) for dat in batch_t]\n",
    "\n",
    "# To transform data (i.e. list of sentences as wordlists) into\n",
    "# input data, feed it to utils.batch\n",
    "# sample results:\n",
    "# utils.batch([code_sequence('1 2 3')])\n",
    "# (array([[1],\n",
    "#         [2],\n",
    "#         [3]], dtype=int32), [3])\n",
    "# that is, a tuple (time major with shape [max_time, batch_size])\n",
    "# so with a batch of two, we get\n",
    "# (array([[1, 4],\n",
    "#         [2, 5],\n",
    "#         [3, 5]], dtype=int32), [3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data generator\n",
    "\n",
    "def random_length(len_from, len_to):\n",
    "    if len_from == len_to:\n",
    "        return len_from\n",
    "    return np.random.randint(len_from, len_to)\n",
    "\n",
    "def random_batch(input_length_from, input_length_to,\n",
    "                 output_length_from, output_length_to,\n",
    "                 seq_length_from, seq_length_to,\n",
    "                 batch_size):\n",
    "    if input_length_from > input_length_to or \\\n",
    "        output_length_from > output_length_to:\n",
    "        raise ValueError('length_from > length_to')\n",
    "\n",
    "    input_batch = [np.random.randint(low=input_length_from,\n",
    "                                     high=input_length_to,\n",
    "                                     size=random_length(seq_length_from,\n",
    "                                                        seq_length_to)).tolist()\n",
    "                   for _ in range(batch_size)]\n",
    "    output_batch = [[word2digit_translate_byidx[idx] for idx in input_dat]\n",
    "                     for input_dat in input_batch]\n",
    "    return input_batch, output_batch\n",
    "      \n",
    "# Example:\n",
    "# digit_from = 2\n",
    "# digit_to = 6+1\n",
    "# word_from = 7\n",
    "# word_to = 11+1\n",
    "# a,b = random_batch(word_from,word_to,digit_from,digit_to,batch_size=2)\n",
    "# print a\n",
    "# [[11, 7, 11, 11, 10, 11, 9, 10, 10, 8, 9], [10, 7, 7, 9, 8, 9, 7, 8, 11]]\n",
    "# print b\n",
    "# [[6, 2, 6, 6, 5, 6, 4, 5, 5, 3, 4], [5, 2, 2, 4, 3, 4, 2, 3, 6]]\n",
    "# print [decode_sequence(a_) for a_ in a]\n",
    "# ['five one five five four five three four four two three', 'four one one three two three one two five']\n",
    "# print [decode_sequence(b_) for b_ in b]\n",
    "# ['5 1 5 5 4 5 3 4 4 2 3', '4 1 1 3 2 3 1 2 5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = decoder_hidden_units = 20\n",
    "\n",
    "#                    decoder \n",
    "#                    target\n",
    "# \n",
    "# [] -> [] -> [#] -> [] -> []\n",
    "# \n",
    "# encoder            decoder\n",
    "# inputs             inputs\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs') # [max_time, batch_size]\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs) # [max_time, batch_size, emb_size]\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs_embedded,\n",
    "    dtype=tf.float32, time_major=True\n",
    ") # encoder_outputs will not be used,\n",
    "  # cuz we only care about the outputs of decoder cells\n",
    "  # encoder_final_state is the input to the decoder, i.e. h_enc^F.\n",
    "  # below, it is fed in decoder as the init.\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_inputs_embedded,\n",
    "    initial_state=encoder_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope='plain_decoder'\n",
    ") # decoder_outputs: [max_time, batch_size, decoder_hidden_size]\n",
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "    # this creates a linear layer for softmax.\n",
    "    # the conversion: [*,*,decoder_hidden_size] -> [*,*,vocab_size] \n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits\n",
    ") # tf.one_hot(indices, depth), where indices is the input (e.g. [2,3] matrix),\n",
    "  # depth is the length of the one-hot vector (usually set as vocab size).\n",
    "  # cross-ent shape: [max_time, batch_size], tells you the entropy at each\n",
    "  # input cell (or, word in this case).\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[ 7  9  7]\n",
      " [ 8 11  9]\n",
      " [ 9 10  7]]\n",
      "\n",
      "readable:  ['one two three', 'three five four', 'one three one']\n",
      "\n",
      "decoder_inputs:\n",
      "[[2 4 2]\n",
      " [3 6 4]\n",
      " [4 5 2]]\n",
      "\n",
      "readable:  ['1 2 3', '3 5 4', '1 3 1']\n",
      "\n",
      "decoder_predictions:\n",
      "[[ 5  5  6]\n",
      " [10 11  5]\n",
      " [ 5  8  5]]\n",
      "\n",
      "readable:  ['4 four 4', '4 five two', '5 4 4']\n"
     ]
    }
   ],
   "source": [
    "# Make some test input\n",
    "\n",
    "input_batch, _ = utils.batch(encode(['one two three', 'three five four', 'one three one']))\n",
    "output_batch, _ = utils.batch(encode(['1 2 3', '3 5 4', '1 3 1']))\n",
    "# _ is seqlen, which we don't use yet\n",
    "\n",
    "# Example (time-major conversion):\n",
    "# input_data = ['one two three', 'three five four', 'one three one']\n",
    "# output_data = ['1 2 3', '3 5 4', '1 3 1']\n",
    "# inputs = encode(input_data)\n",
    "# outputs = encode(output_data)\n",
    "# print inputs\n",
    "# print outputs\n",
    "# [[7, 8, 9], [9, 11, 10], [7, 9, 7]]\n",
    "# [[2, 3, 4], [4, 6, 5], [2, 4, 2]]\n",
    "# utils.batch(inputs=inputs)\n",
    "# (array([[ 7,  9,  7],\n",
    "#         [ 8, 11,  9],\n",
    "#         [ 9, 10,  7]], dtype=int32), [3, 3, 3])\n",
    "\n",
    "print 'batch_encoded:\\n', str(input_batch)\n",
    "print\n",
    "print 'readable: ', to_readable(input_batch)\n",
    "print\n",
    "print 'decoder_inputs:\\n', str(output_batch)\n",
    "print\n",
    "print 'readable: ', to_readable(output_batch)\n",
    "print \n",
    "\n",
    "pred_ = sess.run(decoder_prediction, feed_dict={encoder_inputs: input_batch,\n",
    "                                                decoder_inputs: output_batch})\n",
    "print 'decoder_predictions:\\n', str(pred_)\n",
    "print \n",
    "print 'readable: ', to_readable(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 0.112800441682\n",
      "  sample 1:\n",
      "    input     > two four four one PAD PAD PAD\n",
      "    predicted > 4 4 4 1 EOS PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > one two two five four one PAD\n",
      "    predicted > 1 2 2 5 4 1 EOS PAD\n",
      "  sample 3:\n",
      "    input     > one three five four PAD PAD PAD\n",
      "    predicted > 1 3 5 4 EOS PAD PAD PAD\n",
      "()\n",
      "batch 1000\n",
      "  minibatch loss: 0.105462908745\n",
      "  sample 1:\n",
      "    input     > four five four five PAD PAD PAD\n",
      "    predicted > 4 5 4 5 EOS PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > two five four four three one PAD\n",
      "    predicted > 2 5 4 4 3 1 EOS PAD\n",
      "  sample 3:\n",
      "    input     > one four five PAD PAD PAD PAD\n",
      "    predicted > 1 4 5 EOS PAD PAD PAD PAD\n",
      "()\n",
      "batch 2000\n",
      "  minibatch loss: 0.0850664749742\n",
      "  sample 1:\n",
      "    input     > four five five four three four PAD\n",
      "    predicted > 4 5 5 4 3 4 EOS PAD\n",
      "  sample 2:\n",
      "    input     > one two four two PAD PAD PAD\n",
      "    predicted > 1 2 4 2 EOS PAD PAD PAD\n",
      "  sample 3:\n",
      "    input     > two four four one three two PAD\n",
      "    predicted > 4 4 4 1 3 2 EOS PAD\n",
      "()\n",
      "batch 3000\n",
      "  minibatch loss: 0.052608333528\n",
      "  sample 1:\n",
      "    input     > four two five PAD PAD PAD PAD\n",
      "    predicted > 4 2 5 EOS PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > two five two five three PAD PAD\n",
      "    predicted > 2 5 2 5 3 EOS PAD PAD\n",
      "  sample 3:\n",
      "    input     > one five two one PAD PAD PAD\n",
      "    predicted > 1 5 2 1 EOS PAD PAD PAD\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "digit_from = 2\n",
    "digit_to = 6+1\n",
    "word_from = 7\n",
    "word_to = 11+1\n",
    "seqlen_from = 3\n",
    "seqlen_to = 8\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "def next_feed(batch_size):\n",
    "    batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "    encoder_inputs_, _ = utils.batch(batch_enc)\n",
    "    decoder_targets_, _ = utils.batch([seq + [word2idx['EOS']] for seq in batch_dec])\n",
    "    decoder_inputs_, _ = utils.batch([[word2idx['EOS']] + seq for seq in batch_dec])\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed(batch_size)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(decode_sequence(inp)))\n",
    "                print('    predicted > {}'.format(decode_sequence(pred)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]]]\n"
     ]
    }
   ],
   "source": [
    "# # Experiment block\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# a = tf.placeholder(tf.int32, shape=(None,None))\n",
    "# b = tf.one_hot(a, depth=7)\n",
    "\n",
    "# a_ = np.array([[1,2,3],[4,5,6]])\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# print sess.run(b, feed_dict={a:a_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
