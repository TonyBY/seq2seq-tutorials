{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest Seq2Seq\n",
    "\n",
    "* **Task**: toy \"translation\" task --- translating a list of letters (from A to H) to the next-letter-list (e.g. ['A', 'B', 'C'] translates as ['B', 'C', 'D']. \n",
    "* **Type**: Sutskever et al. (2014). No attention, no bidirection or stacking. Clear-to-the-boot step-by-step demo.\n",
    "* **PyTorch Version**: 0.3.1\n",
    "* **Rant**: showy people on Github write convoluted tutorial code (although efficient, sophisticated and all). Doesn't help for beginners at all! This tutorial tells you all you need to know!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \"\"\"Token-Index mapping.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name: name of the indexer.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1} # str -> int\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {\"SOS\": 0, \"EOS\": 0} # str -> int\n",
    "        self.nWords = 0  # Count SOS and EOS\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"Add a sentence to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of tokens (in string).\n",
    "        \"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a word to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (in string).\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.nWords\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.nWords] = word\n",
    "            self.nWords += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1 \n",
    "            \n",
    "    def get_index(self, word):\n",
    "        \"\"\"Word->Index lookup.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (string).\n",
    "        Returns:\n",
    "            The index of the word.\n",
    "        \"\"\"\n",
    "        return self.word2index[word] if word in self.word2index else -1\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        \"\"\"Index->Word lookup.\n",
    "        \n",
    "        Args:\n",
    "            index: index of a token.\n",
    "        Returns:\n",
    "            The token under the index. -1 if the index is out of bound.\n",
    "        \"\"\"\n",
    "        return self.index2word[index] if index<self.nWords else -1\n",
    "    \n",
    "    def get_sentence_index(self, sentence):\n",
    "        \"\"\"Words->Indexs lookup.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of token (string).\n",
    "        Returns:\n",
    "            A list of indices.\n",
    "        \"\"\"\n",
    "        return [self.get_index(word) for word in sentence]\n",
    "    \n",
    "    def get_sentence_word(self, indexSentence):\n",
    "        \"\"\"Indexs->Words lookup.\n",
    "        \n",
    "        Args:\n",
    "            indexSentence: a list of indices.\n",
    "        Returns:\n",
    "            A list of tokens (string).\n",
    "        \"\"\"\n",
    "        return [self.get_word(index) for index in indexSentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data generation\n",
    "#   vocab -> A to I\n",
    "#   length -> 3 to 8\n",
    "#   task -> translate for the next letter (e.g. A -> B)\n",
    "\n",
    "VOCAB = [chr(i) for i in range(65,74)] # 'A' -> 'I'\n",
    "FROM_LEN, TO_LEN = 3, 8\n",
    "MAX_LENGTH = TO_LEN + 2\n",
    "SOS, EOS = 'SOS', 'EOS'\n",
    "INDEXER = Indexer('LetterTranslator')\n",
    "DATA_SIZE = 3000\n",
    "\n",
    "def translate_word(word):\n",
    "    \"\"\"Find the next letter.\n",
    "    \n",
    "    Args:\n",
    "        word: a letter word (e.g. 'A').\n",
    "    Returns:\n",
    "        The next letter to word.\n",
    "    \"\"\"\n",
    "    return VOCAB[VOCAB.index(word)+1]\n",
    "\n",
    "def translate_sent(sent):\n",
    "    \"\"\"Find the next-letter translation of a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sent: a list of letter words.\n",
    "    Returns:\n",
    "        The next letters.\n",
    "    \"\"\"\n",
    "    return [translate_word(word) for word in sent]\n",
    "\n",
    "def generate_pair():\n",
    "    \"\"\"Randomly generate a pair of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        randInput: a list of letter words.\n",
    "        randTarget: a list of translation letter words of randInput.\n",
    "        randInputLen, randTargetLen: lengths of the lists above.\n",
    "    \"\"\"\n",
    "    randInput = list(np.random.choice(VOCAB[:-1], size=random.randint(FROM_LEN,TO_LEN)))\n",
    "    randTarget = translate_sent(randInput)\n",
    "    randInputLen, randTargetLen = len(randInput), len(randTarget)\n",
    "    return randInput, randTarget+[str('EOS')], \\\n",
    "           randInputLen, randTargetLen+1\n",
    "        # str(): default is utf-8\n",
    "\n",
    "def generate_data():\n",
    "    \"\"\"Randomly generate a set of pairs of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        pairs: a pair of lists of torch Variables (torch.LongTensor).\n",
    "        lengths: lengths of the corresponding lists in pairs.\n",
    "    \"\"\"\n",
    "    pairs, lengths = [], []\n",
    "    for _ in range(DATA_SIZE):\n",
    "        randInput,randTarget,randInputLen,randTargetLen = generate_pair()\n",
    "        INDEXER.add_sentence(randInput)\n",
    "        INDEXER.add_sentence(randTarget)\n",
    "        pairs.append([Variable(torch.LongTensor(INDEXER.get_sentence_index(randInput)).view(-1,1)),\n",
    "                      Variable(torch.LongTensor(INDEXER.get_sentence_index(randTarget)).view(-1,1))])\n",
    "            # convert sentences to <mt,bc> shape.\n",
    "            # here bc=1.\n",
    "        lengths.append([randInputLen,randTargetLen])\n",
    "    return pairs, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, lengths = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest Seq2Seq similar to Sutskever et al. (2014)\n",
    "\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, hiddenSize, nLayers=1):\n",
    "        # inputSize: vocabulary size.\n",
    "        # hiddenSize: size for both embedding and GRU hidden.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers)\n",
    "    \n",
    "    def forward(self, inputs, inputsLen, hidden):\n",
    "        # inputs: <mt,bc>\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(inputs).view(inputsLen,1,-1) # <mt,bc=1,h>\n",
    "        output,hidden = self.gru(embedded, hidden)\n",
    "            # output: <mt,bc=1,h>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.nLayers,1,self.hiddenSize))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize, outputSize, nLayers=1):\n",
    "        # hiddenSize: encoder final hidden size.\n",
    "        # outputSize: vocabulary size.\n",
    "        # NB: nLayers here is related to the size of the encoder.\n",
    "        #     left as 1 for simplicity here.\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(outputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers)\n",
    "        self.out = nn.Linear(hiddenSize, outputSize)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input: <mt=1,bc=1>\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(input).view(1,1,-1) # <mt=1,bc=1,h>\n",
    "        output, hidden = self.gru(embedded, hidden) \n",
    "            # output: <mt,bc,h>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        output = F.log_softmax(self.out(output.squeeze(0)), dim=-1)\n",
    "            # squeeze: get rid of mt=1 for computing loss.\n",
    "            # out-Linear: <mt,bc,h> * <h,vocab> -> <mt,bc,vocab>\n",
    "            # softmax: get log softmax along vocab dim\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder IN -- (inputs, inputsLen) \n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 4\n",
      "[torch.LongTensor of size 6x1]\n",
      "\n",
      "6 \n",
      "\n",
      "Encoder OUT -- (output, hidden) \n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0643 -0.3000 -0.0849  0.2040  0.4047\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.3694  0.2474 -0.0463  0.4804  0.3955\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.1277 -0.3935  0.1632  0.5834  0.2498\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.3257 -0.5005  0.1710 -0.1195  0.0906\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.0885  0.2160  0.6881  0.1680  0.3916\n",
      "\n",
      "(5 ,.,.) = \n",
      " -0.1838  0.3584  0.8152  0.4037  0.4428\n",
      "[torch.FloatTensor of size 6x1x5]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1838  0.3584  0.8152  0.4037  0.4428\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      ", '\\n')\n",
      "=======================\n",
      "\n",
      "Decoder IN -- (input, hidden) \n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1838  0.3584  0.8152  0.4037  0.4428\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n",
      "Decoder (1 step) OUT -- (output, hidden) \n",
      "\n",
      "Variable containing:\n",
      "-2.2933 -2.4409 -1.4867 -1.8607 -2.8049 -2.2017 -2.3026 -2.3134 -2.8091\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.4031  0.4379  0.8260 -0.0935 -0.0683\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n",
      "\n",
      "Decoder (1 step) LOSS = 1.860702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: how things working inside the encoder and decoder.\n",
    "\n",
    "sent = pairs[0][0]\n",
    "sentLen = lengths[0][0]\n",
    "print(\"Encoder IN -- (inputs, inputsLen) \\n\")\n",
    "print(sent)\n",
    "print(sentLen), '\\n'\n",
    "e = EncoderRNN(INDEXER.nWords, HIDDEN_SIZE)\n",
    "eh = e.init_hidden()\n",
    "eo,eh = e(sent,sentLen,eh)\n",
    "print(\"Encoder OUT -- (output, hidden) \\n\")\n",
    "print(eo)\n",
    "print(eh, '\\n')\n",
    "print(\"=======================\\n\")\n",
    "loss = 0\n",
    "crit = nn.NLLLoss()\n",
    "di = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "dh = eh\n",
    "print(\"Decoder IN -- (input, hidden) \\n\")\n",
    "print(di)\n",
    "print(dh)\n",
    "d = DecoderRNN(HIDDEN_SIZE, INDEXER.nWords)\n",
    "target = pairs[0][1]\n",
    "targetLen = lengths[0][1]\n",
    "for i in range(targetLen):\n",
    "#     print('HAHAHA', di.shape, dh.shape)\n",
    "    do,dh = d(di,dh)\n",
    "    loss += crit(do,target[i]) \n",
    "        # do: <bc,h>\n",
    "        # target[i]: <bc,>\n",
    "    tv,ti = do.data.topk(1) # ti: 1x1\n",
    "    di = Variable(ti) # next input is the index predicted at this step.\n",
    "    if i==0:\n",
    "        print(\"Decoder (1 step) OUT -- (output, hidden) \\n\")\n",
    "        print(do)\n",
    "        print(dh); print\n",
    "        print(\"Decoder (1 step) LOSS = %f\\n\" % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, inputsLen, targets, targetsLen,\n",
    "               encoder, decoder, \n",
    "               encoderOptim, decoderOptim, criterion,\n",
    "               enforcingRatio, clip):\n",
    "    \"\"\"One training step (on a single pair of sentences).\"\"\"\n",
    "    # Clear previous grads\n",
    "    # WHY: Since the backward() function accumulates gradients, \n",
    "    #      and you don’t want to mix up gradients between minibatches, \n",
    "    #      you have to zero them out at the start of a new minibatch. \n",
    "    #      This is exactly like how a general (additive) accumulator \n",
    "    #      variable is initialized to 0 in code.\n",
    "    encoderOptim.zero_grad()\n",
    "    decoderOptim.zero_grad()\n",
    "    # Set up loss\n",
    "    loss = 0\n",
    "    # Run encoder\n",
    "    encoderHidden = encoder.init_hidden()\n",
    "    encoderOutput, encoderHidden = encoder(inputs, inputsLen, encoderHidden)\n",
    "    # Run decoder\n",
    "    decoderInput = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "    decoderHidden = encoderHidden\n",
    "    enforce = random.random() < enforcingRatio\n",
    "    for di in range(targetsLen):\n",
    "        decoderOutput, decoderHidden = decoder(decoderInput, decoderHidden)\n",
    "        loss += criterion(decoderOutput, targets[di])\n",
    "        if enforce: # i.e. feed gold target tokens in training.\n",
    "            decoderInput = targets[di] # decoderInput can be 1 or 1x1 \n",
    "        else:\n",
    "            topValue,topIndex = decoderOutput.data.topk(1)\n",
    "            decoderInput = Variable(topIndex)\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoderOptim.step()\n",
    "    decoderOptim.step()\n",
    "    return loss.data[0] / targetsLen\n",
    "\n",
    "def train(pairs, lengths,\n",
    "          nEpochs=1, epochSize=1000, lr=1e-4,\n",
    "          enforcingRatio=0.5, clip=5.0,\n",
    "          printEvery=100):\n",
    "    \"\"\"Train multiple steps.\"\"\"\n",
    "    dataSize = len(pairs)\n",
    "    encoder = EncoderRNN(INDEXER.nWords, HIDDEN_SIZE)\n",
    "    decoder = DecoderRNN(HIDDEN_SIZE, INDEXER.nWords)\n",
    "    encoderOptim = optim.Adam(encoder.parameters(),lr)\n",
    "    decoderOptim = optim.Adam(decoder.parameters(),lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    averageLoss = 0\n",
    "    for e in range(nEpochs):\n",
    "        start = time.time()\n",
    "        epochLoss = 0\n",
    "        for step in range(epochSize):\n",
    "            i = random.choice(range(0,dataSize))\n",
    "            inputs, targets = pairs[i]\n",
    "            inputsLen, targetsLen = lengths[i]\n",
    "            loss = train_step(inputs, inputsLen, targets, targetsLen,\n",
    "                              encoder, decoder,\n",
    "                              encoderOptim, decoderOptim, criterion,\n",
    "                              enforcingRatio, clip)\n",
    "            if step!=0 and step%printEvery==0:\n",
    "                print(\"Step %d average loss = %.4f\" % (step, loss))\n",
    "            epochLoss += loss\n",
    "        epochLoss /= epochSize\n",
    "        averageLoss += epochLoss\n",
    "        print(\"\\nEpoch %d loss = %.4f (time: %.2f)\\n\" % (e+1,epochLoss,\n",
    "                                                         time.time()-start))\n",
    "    averageLoss /= nEpochs\n",
    "    print(\"\\nGrand average loss = %.4f\\n\" % averageLoss)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 average loss = 2.0463\n",
      "Step 1000 average loss = 2.0567\n",
      "Step 1500 average loss = 1.8398\n",
      "Step 2000 average loss = 1.8864\n",
      "Step 2500 average loss = 1.8702\n",
      "\n",
      "Epoch 1 loss = 2.0020 (time: 27.18)\n",
      "\n",
      "Step 500 average loss = 1.7942\n",
      "Step 1000 average loss = 1.8996\n",
      "Step 1500 average loss = 2.0343\n",
      "Step 2000 average loss = 1.7231\n",
      "Step 2500 average loss = 2.0002\n",
      "\n",
      "Epoch 2 loss = 1.8077 (time: 26.90)\n",
      "\n",
      "Step 500 average loss = 1.5698\n",
      "Step 1000 average loss = 1.6615\n",
      "Step 1500 average loss = 1.6546\n",
      "Step 2000 average loss = 1.5510\n",
      "Step 2500 average loss = 1.6964\n",
      "\n",
      "Epoch 3 loss = 1.6946 (time: 27.44)\n",
      "\n",
      "Step 500 average loss = 1.8457\n",
      "Step 1000 average loss = 1.4318\n",
      "Step 1500 average loss = 1.4433\n",
      "Step 2000 average loss = 2.1736\n",
      "Step 2500 average loss = 1.7652\n",
      "\n",
      "Epoch 4 loss = 1.5972 (time: 27.38)\n",
      "\n",
      "Step 500 average loss = 1.6271\n",
      "Step 1000 average loss = 1.7220\n",
      "Step 1500 average loss = 0.7872\n",
      "Step 2000 average loss = 1.2104\n",
      "Step 2500 average loss = 1.4867\n",
      "\n",
      "Epoch 5 loss = 1.5428 (time: 27.27)\n",
      "\n",
      "Step 500 average loss = 1.8749\n",
      "Step 1000 average loss = 1.3755\n",
      "Step 1500 average loss = 1.5523\n",
      "Step 2000 average loss = 1.5641\n",
      "Step 2500 average loss = 1.6903\n",
      "\n",
      "Epoch 6 loss = 1.4988 (time: 27.20)\n",
      "\n",
      "Step 500 average loss = 2.0273\n",
      "Step 1000 average loss = 0.7316\n",
      "Step 1500 average loss = 1.1489\n",
      "Step 2000 average loss = 1.4805\n",
      "Step 2500 average loss = 1.1115\n",
      "\n",
      "Epoch 7 loss = 1.4506 (time: 27.13)\n",
      "\n",
      "Step 500 average loss = 1.6533\n",
      "Step 1000 average loss = 1.9721\n",
      "Step 1500 average loss = 1.0663\n",
      "Step 2000 average loss = 1.5123\n",
      "Step 2500 average loss = 1.1199\n",
      "\n",
      "Epoch 8 loss = 1.4141 (time: 28.10)\n",
      "\n",
      "Step 500 average loss = 1.1797\n",
      "Step 1000 average loss = 2.0122\n",
      "Step 1500 average loss = 1.7001\n",
      "Step 2000 average loss = 1.5310\n",
      "Step 2500 average loss = 1.4290\n",
      "\n",
      "Epoch 9 loss = 1.3650 (time: 27.26)\n",
      "\n",
      "Step 500 average loss = 1.0820\n",
      "Step 1000 average loss = 0.9284\n",
      "Step 1500 average loss = 1.3065\n",
      "Step 2000 average loss = 1.3935\n",
      "Step 2500 average loss = 1.0925\n",
      "\n",
      "Epoch 10 loss = 1.3166 (time: 27.62)\n",
      "\n",
      "Step 500 average loss = 1.2223\n",
      "Step 1000 average loss = 1.3842\n",
      "Step 1500 average loss = 1.3596\n",
      "Step 2000 average loss = 1.7277\n",
      "Step 2500 average loss = 1.7324\n",
      "\n",
      "Epoch 11 loss = 1.3045 (time: 27.75)\n",
      "\n",
      "Step 500 average loss = 1.4800\n",
      "Step 1000 average loss = 1.2651\n",
      "Step 1500 average loss = 1.3497\n",
      "Step 2000 average loss = 0.8779\n",
      "Step 2500 average loss = 1.4797\n",
      "\n",
      "Epoch 12 loss = 1.2380 (time: 27.65)\n",
      "\n",
      "Step 500 average loss = 1.1162\n",
      "Step 1000 average loss = 1.5521\n",
      "Step 1500 average loss = 1.3671\n",
      "Step 2000 average loss = 0.7301\n",
      "Step 2500 average loss = 1.5725\n",
      "\n",
      "Epoch 13 loss = 1.1942 (time: 27.39)\n",
      "\n",
      "Step 500 average loss = 0.6782\n",
      "Step 1000 average loss = 0.8268\n",
      "Step 1500 average loss = 0.7643\n",
      "Step 2000 average loss = 1.1968\n",
      "Step 2500 average loss = 1.2290\n",
      "\n",
      "Epoch 14 loss = 1.1603 (time: 27.94)\n",
      "\n",
      "Step 500 average loss = 1.8682\n",
      "Step 1000 average loss = 1.5117\n",
      "Step 1500 average loss = 1.8292\n",
      "Step 2000 average loss = 0.6634\n",
      "Step 2500 average loss = 0.8086\n",
      "\n",
      "Epoch 15 loss = 1.1166 (time: 27.39)\n",
      "\n",
      "Step 500 average loss = 0.4686\n",
      "Step 1000 average loss = 0.2674\n",
      "Step 1500 average loss = 0.6632\n",
      "Step 2000 average loss = 1.0170\n",
      "Step 2500 average loss = 0.4946\n",
      "\n",
      "Epoch 16 loss = 1.0923 (time: 27.73)\n",
      "\n",
      "Step 500 average loss = 1.6321\n",
      "Step 1000 average loss = 1.1355\n",
      "Step 1500 average loss = 1.2028\n",
      "Step 2000 average loss = 1.0349\n",
      "Step 2500 average loss = 0.4326\n",
      "\n",
      "Epoch 17 loss = 1.0808 (time: 27.54)\n",
      "\n",
      "Step 500 average loss = 0.3871\n",
      "Step 1000 average loss = 1.4435\n",
      "Step 1500 average loss = 0.5549\n",
      "Step 2000 average loss = 0.7181\n",
      "Step 2500 average loss = 0.8620\n",
      "\n",
      "Epoch 18 loss = 1.0476 (time: 27.69)\n",
      "\n",
      "Step 500 average loss = 0.7592\n",
      "Step 1000 average loss = 0.5074\n",
      "Step 1500 average loss = 1.1392\n",
      "Step 2000 average loss = 0.7385\n",
      "Step 2500 average loss = 1.0335\n",
      "\n",
      "Epoch 19 loss = 1.0268 (time: 27.44)\n",
      "\n",
      "Step 500 average loss = 1.3415\n",
      "Step 1000 average loss = 1.6229\n",
      "Step 1500 average loss = 1.6071\n",
      "Step 2000 average loss = 1.4727\n",
      "Step 2500 average loss = 0.9924\n",
      "\n",
      "Epoch 20 loss = 1.0152 (time: 27.96)\n",
      "\n",
      "\n",
      "Grand average loss = 1.3483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = train(pairs, lengths, \n",
    "                         nEpochs=20, epochSize=len(pairs),\n",
    "                         printEvery=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sent, sentLen, target, targetLen,\n",
    "             encoder, decoder, \n",
    "             maxLength):\n",
    "    encoderHidden = encoder.init_hidden()\n",
    "    encoderOutput, encoderHidden = encoder(sent, sentLen, encoderHidden)\n",
    "    decoderInput = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "    decoderHidden = encoderHidden\n",
    "    prediction = []\n",
    "    lengthGen = 0\n",
    "    while True:\n",
    "        lengthGen += 1\n",
    "        decoderOutput, decoderHidden = decoder(decoderInput, decoderHidden)\n",
    "        topValue,topIndex = decoderOutput.data.topk(1)\n",
    "        decoderInput = Variable(topIndex)\n",
    "        prediction += list(topIndex.squeeze().numpy())\n",
    "        if prediction[0] == INDEXER.get_index('EOS') or lengthGen>=maxLength:\n",
    "            break\n",
    "    sent = list(sent.data.squeeze().numpy())\n",
    "    target = list(target.data.squeeze().numpy())\n",
    "    print(\"INPUT >> %s\" % ' '.join(INDEXER.get_sentence_word(sent)))\n",
    "    print(\"PRED >> %s\" % ' '.join(INDEXER.get_sentence_word(prediction[:targetLen])))\n",
    "    print(\"TRUE >> %s\" % ' '.join(INDEXER.get_sentence_word(target)))\n",
    "    \n",
    "def random_evaluate(pairs, lengths,\n",
    "                    encoder, decoder,\n",
    "                    maxLength=15):\n",
    "    i = random.choice(range(0,len(pairs)))\n",
    "    sent, target = pairs[i]\n",
    "    sentLen, targetLen = lengths[i]\n",
    "    evaluate(sent, sentLen, target, targetLen, encoder, decoder, maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT >> G D A C\n",
      "PRED >> H E B D D\n",
      "TRUE >> H E B D D\n"
     ]
    }
   ],
   "source": [
    "random_evaluate(pairs, lengths, encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
