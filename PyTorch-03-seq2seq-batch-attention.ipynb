{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq with Batching\n",
    "\n",
    "* **Task**: toy \"translation\" task --- translating a list of letters (from A to H) to the next-letter-list (e.g. ['A', 'B', 'C'] translates as ['B', 'C', 'D']. \n",
    "* **Type**: Luong et al. (2016). No bidirection or stacking. Clear-to-the-boot step-by-step demo.\n",
    "* **PyTorch Version**: 0.3.1\n",
    "* **Rant**: showy people on Github write convoluted tutorial code (although efficient, sophisticated and all). Doesn't help for beginners at all! This tutorial tells you all you need to know!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \"\"\"Token-Index mapping.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name: name of the indexer.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1} # str -> int\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {\"SOS\": 0, \"EOS\": 0} # str -> int\n",
    "        self.nWords = 2  # Count SOS and EOS\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"Add a sentence to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of tokens (in string).\n",
    "        \"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a word to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (in string).\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.nWords\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.nWords] = word\n",
    "            self.nWords += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1 \n",
    "            \n",
    "    def get_index(self, word):\n",
    "        \"\"\"Word->Index lookup.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (string).\n",
    "        Returns:\n",
    "            The index of the word.\n",
    "        \"\"\"\n",
    "        return self.word2index[word] if word in self.word2index else -1\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        \"\"\"Index->Word lookup.\n",
    "        \n",
    "        Args:\n",
    "            index: index of a token.\n",
    "        Returns:\n",
    "            The token under the index. -1 if the index is out of bound.\n",
    "        \"\"\"\n",
    "        return self.index2word[index] if index<self.nWords else -1\n",
    "    \n",
    "    def get_sentence_index(self, sentence):\n",
    "        \"\"\"Words->Indexs lookup.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of token (string).\n",
    "        Returns:\n",
    "            A list of indices.\n",
    "        \"\"\"\n",
    "        return [self.get_index(word) for word in sentence]\n",
    "    \n",
    "    def get_sentence_word(self, indexSentence):\n",
    "        \"\"\"Indexs->Words lookup.\n",
    "        \n",
    "        Args:\n",
    "            indexSentence: a list of indices.\n",
    "        Returns:\n",
    "            A list of tokens (string).\n",
    "        \"\"\"\n",
    "        return [self.get_word(index) for index in indexSentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data generation\n",
    "#   vocab -> A to I\n",
    "#   length -> 3 to 8\n",
    "#   task -> translate for the next letter (e.g. A -> B)\n",
    "\n",
    "VOCAB = [chr(i) for i in range(65,74)] # 'A' -> 'I'\n",
    "FROM_LEN, TO_LEN = 3, 8\n",
    "MAX_LENGTH = TO_LEN + 2\n",
    "SOS, EOS = 'SOS', 'EOS'\n",
    "INDEXER = Indexer('LetterTranslator')\n",
    "DATA_SIZE = 3000\n",
    "\n",
    "def translate_word(word):\n",
    "    \"\"\"Find the next letter.\n",
    "    \n",
    "    Args:\n",
    "        word: a letter word (e.g. 'A').\n",
    "    Returns:\n",
    "        The next letter to word.\n",
    "    \"\"\"\n",
    "    return VOCAB[VOCAB.index(word)+1]\n",
    "\n",
    "def translate_sent(sent):\n",
    "    \"\"\"Find the next-letter translation of a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sent: a list of letter words.\n",
    "    Returns:\n",
    "        The next letters.\n",
    "    \"\"\"\n",
    "    return [translate_word(word) for word in sent]\n",
    "\n",
    "def generate_pair():\n",
    "    \"\"\"Randomly generate a pair of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        randInput: a list of letter words.\n",
    "        randTarget: a list of translation letter words of randInput.\n",
    "        randInputLen, randTargetLen: lengths of the lists above.\n",
    "    \"\"\"\n",
    "    randInput = list(np.random.choice(VOCAB[:-1], size=random.randint(FROM_LEN,TO_LEN)))\n",
    "    randTarget = translate_sent(randInput)\n",
    "    randInputLen, randTargetLen = len(randInput), len(randTarget)\n",
    "    return randInput, randTarget+[str('EOS')], \\\n",
    "           randInputLen, randTargetLen+1\n",
    "        # str(): default is utf-8\n",
    "\n",
    "def generate_data():\n",
    "    \"\"\"Randomly generate a set of pairs of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        pairs: a pair of lists, where each is a list of token indices.\n",
    "        lengths: lengths of the corresponding lists in pairs.\n",
    "    \"\"\"\n",
    "    pairs, lengths = [], []\n",
    "    for _ in range(DATA_SIZE):\n",
    "        randInput,randTarget,randInputLen,randTargetLen = generate_pair()\n",
    "        INDEXER.add_sentence(randInput)\n",
    "        INDEXER.add_sentence(randTarget)\n",
    "        pairs.append([INDEXER.get_sentence_index(randInput),\n",
    "                      INDEXER.get_sentence_index(randTarget)])\n",
    "        \n",
    "            # convert sentences to <mt,bc> shape.\n",
    "            # here bc=1.\n",
    "        lengths.append([randInputLen,randTargetLen])\n",
    "    return pairs, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, lengths = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "class DataIterator:\n",
    "    \"\"\"Data feeder by batch.\"\"\"\n",
    "    \n",
    "    def __init__(self, pairs, lengths):\n",
    "        self.pairs = pairs\n",
    "        self.lengths = lengths\n",
    "        self.size = len(pairs)\n",
    "        self.indices = range(self.size)\n",
    "        \n",
    "    def _get_padded_sentence(self, index, maxSentLen, maxTargetLen):\n",
    "        \"\"\"Pad a sentence pair by EOS (pad both to the largest length of respective batch).\n",
    "        \n",
    "        Args:\n",
    "            index: index of a sentence & length pair in self.pairs, self.lengths.\n",
    "            maxSentLen: the length of the longest source sentence.\n",
    "            maxTargetLen: the length of the longest target sentence.\n",
    "        Returns:\n",
    "            padded source sentence (list), its length (int), \n",
    "            padded target sentence (list), its length (int).\n",
    "        \"\"\"\n",
    "        sent1,sent2 = self.pairs[index][0], self.pairs[index][1]\n",
    "        length1,length2 = self.lengths[index][0], self.lengths[index][1]\n",
    "        paddedSent1 = sent1[:maxSentLen] if length1>maxSentLen else sent1+[INDEXER.get_index('EOS')]*(maxSentLen-length1)\n",
    "        paddedSent2 = sent2[:maxTargetLen] if length2>maxTargetLen else sent2+[INDEXER.get_index('EOS')]*(maxTargetLen-length2)\n",
    "        return paddedSent1,length1,paddedSent2,length2\n",
    "    \n",
    "    def random_batch(self, batchSize=BATCH_SIZE):\n",
    "        \"\"\"Random batching.\n",
    "        \n",
    "        Args:\n",
    "            batchSize: size of a batch of sentence pairs and respective lengths.\n",
    "        Returns:\n",
    "            the batch of source sentence (Variable(torch.LongTensor())),\n",
    "            the lengths of source sentences (numpy.array())\n",
    "            and the same for target sentences and lengths.\n",
    "        \"\"\"\n",
    "        batchIndices = np.random.choice(self.indices, size=batchSize, replace=False)\n",
    "        batchSents,batchTargets,batchSentLens,batchTargetLens = [], [], [], []\n",
    "        maxSentLen, maxTargetLen = np.array([self.lengths[index] for index in batchIndices]).max(axis=0)\n",
    "        for index in batchIndices:\n",
    "            paddedSent1,length1,paddedSent2,length2 = self._get_padded_sentence(index, maxSentLen, maxTargetLen)\n",
    "            batchSents.append(paddedSent1)\n",
    "            batchTargets.append(paddedSent2)\n",
    "            batchSentLens.append(length1)\n",
    "            batchTargetLens.append(length2)\n",
    "        batchIndices = range(batchSize) # reindex from 0 for sorting.\n",
    "        batchIndices = [i for i,l in sorted(zip(batchIndices,batchSentLens),key=lambda p:p[1],reverse=True)]\n",
    "        batchSents = Variable(torch.LongTensor(np.array(batchSents)[batchIndices])).transpose(0,1) # <bc,mt> -> <mt,bc>\n",
    "        batchTargets = Variable(torch.LongTensor(np.array(batchTargets)[batchIndices])).transpose(0,1)\n",
    "        batchSentLens = np.array(batchSentLens)[batchIndices]\n",
    "        batchTargetLens = np.array(batchTargetLens)[batchIndices]\n",
    "        return batchSents, batchSentLens, batchTargets, batchTargetLens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  3   9\n",
      "  9   9\n",
      "  4  10\n",
      "[torch.LongTensor of size 3x2]\n",
      " [3 3] 3\n",
      "Variable containing:\n",
      " 7  3\n",
      " 3  3\n",
      " 8  5\n",
      " 1  1\n",
      "[torch.LongTensor of size 4x2]\n",
      " [4 4] 4\n"
     ]
    }
   ],
   "source": [
    "dataIter = DataIterator(pairs, lengths)\n",
    "a1,a2,b1,b2 = dataIter.random_batch(2)\n",
    "print a1, a2, a1.size(0)\n",
    "print b1, b2, b1.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq (batch) with attention, similar to Luong et al. (2016)\n",
    "#   Comment notation: mt = max-time; bc = batch-size; h = hidden-size.\n",
    "\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, hiddenSize, nLayers=1):\n",
    "        # inputSize: vocabulary size.\n",
    "        # hiddenSize: size for both embedding and GRU hidden.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers)\n",
    "    \n",
    "    def forward(self, inputs, inputsLen, hidden=None):\n",
    "        # inputs: <mt,bc>\n",
    "        # inputsLen: <bc,> (a list).\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(inputs) # <mt,bc,h>\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, inputsLen)\n",
    "            # 'packed' has a 'data' and a 'batch_sizes' field.\n",
    "            #   'data' is a <sum(len),h> matrix (len is real lengths, not padded).\n",
    "            #   'batch_sizes' has the number of non-zero batches at each time-step.\n",
    "            # e.g. for this 'inputs'\n",
    "            #    2     1     3     0     2\n",
    "            #    6     8     1     6     2\n",
    "            #    0     7     0     8     8\n",
    "            #    6     4     2     1     1\n",
    "            #    1     8     1     1     1\n",
    "            #    6     1     1     1     1\n",
    "            #    0     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1  \n",
    "            # 'data' = 22 = 7+5+4+3+3 (1's are pads corresponding to 'EOS').\n",
    "            # 'batch_sizes' = [5, 5, 5, 3, 2, 1, 1].\n",
    "        outputs,hidden = self.gru(packed, hidden)\n",
    "            # outputs: same format as 'packed'.\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        outputs, outputsLen = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "            # outputs: <mt,bc,h>\n",
    "            # outputsLen: same as the 'batch_sizes' field of 'packed'.   \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"Basic linear attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.attention = nn.Linear(hiddenSize, hiddenSize)\n",
    "    \n",
    "    def forward(self, hidden, encoderOutput):\n",
    "        # hidden: <1,bc,h>\n",
    "        # encoderOutput: <mt,bc,h>\n",
    "        encoderOutputLen, batchSize = encoderOutput.size(0), encoderOutput.size(1)\n",
    "        encoderOutputLen = len(encoderOutput)\n",
    "        attentionEnergies = Variable(torch.zeros(batchSize, encoderOutputLen)) # <bc,mt>\n",
    "        for b in range(batchSize):\n",
    "            for i in range(encoderOutputLen):\n",
    "                attentionEnergies[b,i] = self.score(hidden[:,b],encoderOutput[i,b].unsqueeze(0))\n",
    "                    # hidden[:,b] selects a <1,h> from <1,bc,h>\n",
    "                    # encoderOutput[i,b] selects a <h,> from <mt,bc,h>\n",
    "                    #   then unsqueeze(0) to add a first dimension to make <1,h>\n",
    "                    # score thus takes <1,h> and <1,h>\n",
    "        return F.softmax(attentionEnergies, dim=-1).unsqueeze(1)\n",
    "            # first softmax along the mt dimension of <bc,mt>,\n",
    "            # then unsqueeze(1) to make <bc,1,mt>, technical convenience.\n",
    "        \n",
    "    def score(self, hidden, encoderOutput):\n",
    "            # hidden: <bc=1,h>\n",
    "            # encoderOutput: <bc=1,h> (1 time step).\n",
    "        energy = self.attention(encoderOutput)\n",
    "            # linear attention: <bc,h> * <h,h> -> <bc,h>   \n",
    "        energy = hidden.dot(energy)\n",
    "            # dot: <bc,h> * <bc,h> -> <bc,h>\n",
    "            # .dot smartly find fitting dimensions.\n",
    "        return energy\n",
    "    \n",
    "class LuongDecoderRNN(nn.Module):\n",
    "    \"\"\"Luong attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize, outputSize, nLayers=1):\n",
    "        super(LuongDecoderRNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(outputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(2*hiddenSize, hiddenSize) \n",
    "        self.out = nn.Linear(2*hiddenSize, outputSize)\n",
    "            # inputSize doubles because concatted context of same hiddenSize.\n",
    "        self.linearAttention = LinearAttention(hiddenSize)\n",
    "        \n",
    "    def forward(self, inputs, hidden, context, encoderOutput):\n",
    "            # inputs: <bc,>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "            # context: <bc,h>\n",
    "            # encoderOutput: <mt,bc,h>  \n",
    "        batchSize = inputs.size(0)\n",
    "        embedded = self.embedding(inputs).view(1,batchSize,self.hiddenSize) # <mt=1,bc,h>\n",
    "        inputs = torch.cat((embedded,context.unsqueeze(0)),2)\n",
    "            # unsqueeze: <bc,h> -> <mt=1,bc,h>\n",
    "            # concat: <mt,bc,h> & <mt,bc,h> @2 -> <mt,bc,2h>\n",
    "        output, hidden = self.gru(inputs, hidden)\n",
    "            # IN: <mt=1,bc,2h>, <n_layer*n_direction,bc,h>\n",
    "            # OUT: <mt=1,bc,h>, <n_layer*n_direction,bc,h>         \n",
    "        attentionWeights = self.linearAttention(output,\n",
    "                                                encoderOutput)\n",
    "            # squeeze: <mt=1,bc,h> -> <bc,h>\n",
    "            # attentionWeights: <bc=1,1,mt>\n",
    "        context = attentionWeights.bmm(encoderOutput.transpose(0,1))\n",
    "            # transpose: <mt,bc,h> -> <bc,mt,h>\n",
    "            # bmm (batched matrix multiplication): \n",
    "            #   <bc,1,mt> & <bc,mt,h> -> <bc,1,h>\n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "            # output squeeze: <mt=1,bc=1,h> -> <bc,h>\n",
    "            # context squeeze: <bc=1,1,h> -> <bc,h>\n",
    "        output = F.log_softmax(F.tanh(self.out(torch.cat((output,context),1))),dim=-1)\n",
    "            # concat: <bc,h> & <bc,h> @1 -> <bc,2h>\n",
    "            # linear->tahn/out: <bc,2h> * <2h,vocab> -> <bc,vocab>\n",
    "            # softmax: along dim=-1, i.e. vocab.\n",
    "        return output, hidden, context, attentionWeights\n",
    "            # full output for visualization:\n",
    "            #   output: <bc,vocab>\n",
    "            #   hidden: <n_layer*n_direction,bc,h>\n",
    "            #   context: <bc,h>\n",
    "            #   attentionWeights: <bc,1,mt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cross_entropy(decoderOutputAll, targets, targetsLen, batchSize=BATCH_SIZE):\n",
    "    # decoderOutputAll: <bc,mt,vocab> (transposed in train function).\n",
    "    # targets: <bc,mt>\n",
    "    # targetsLen: <bc,> (a list).\n",
    "    logitsFlat = decoderOutputAll.view(-1, decoderOutputAll.size(-1))\n",
    "        # <bc,mt,vocab> -> <bc*mt,vocab>\n",
    "    logProbsFlat = F.log_softmax(logitsFlat,dim=-1)\n",
    "        # <bc,mt,vocab>, with dim vocab has log probs.\n",
    "    targetsFlat = targets.view(-1,1)\n",
    "        # <bc,mt> -> <bc*mt,1>\n",
    "    lossesFlat = -torch.gather(logProbsFlat,dim=1,index=targetsFlat)\n",
    "        # <bc,mt,vocab> -> <bc*mt,1>\n",
    "    losses = lossesFlat.view(*targets.size())\n",
    "        # reshape: <bc*mt,1> -> <bc,mt>\n",
    "    # Make a mask\n",
    "    #   requires: lengths, maxLen\n",
    "    maxLen = max(targetsLen)\n",
    "    seqRange = torch.arange(maxLen).long()\n",
    "        # generate a maxLen tensor of long type, <max-len,>\n",
    "    seqRangeExpand = Variable(seqRange.unsqueeze(0).expand(batchSize,maxLen))\n",
    "        # unsqueeze: <1,max-len>\n",
    "        # expand: copy BATCH_SIZE times along first dimension\n",
    "        #         second dim won't change as they are of the same length.\n",
    "        #   e.g. for expand:\n",
    "        #     >>> x = torch.Tensor([[1], [2], [3]])\n",
    "        #     >>> x.size()\n",
    "        #     torch.Size([3, 1])\n",
    "        #     >>> x.expand(3, 4)\n",
    "        #      1  1  1  1\n",
    "        #      2  2  2  2\n",
    "        #      3  3  3  3\n",
    "        #     [torch.FloatTensor of size 3x4]\n",
    "        #   finally we got <bc,max-len>.\n",
    "    seqLenExpand = (Variable(torch.LongTensor(targetsLen)).unsqueeze(1).expand_as(seqRangeExpand))\n",
    "        # unsqueeze: <bc,> -> <bc,1>\n",
    "        # expand_as: <bc,1> -> <bc,max-len> \n",
    "    mask = seqRangeExpand < seqLenExpand\n",
    "        # e.g. batch=2 case:\n",
    "        #   seqRangeExpand is\n",
    "        #     0 1 2 3\n",
    "        #     0 1 2 3\n",
    "        #   seqLenExpand is\n",
    "        #     3 3 3 3 <= length of this sentence is 3\n",
    "        #     2 2 2 2\n",
    "        #   then we got a matrix that's elementwise results from the comparison.\n",
    "        #     1 1 1 0\n",
    "        #     1 1 0 0\n",
    "        #   which means an elem=1 if it doesn't correspond to a padder.\n",
    "    # Compute final loss\n",
    "    losses = losses * mask.float() # zeroify all 0 elem in the mask.\n",
    "    loss = losses.sum() / sum(targetsLen)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, inputsLen, targets, targetsLen,\n",
    "               encoder, decoder, \n",
    "               encoderOptim, decoderOptim,\n",
    "               enforcingRatio, clip,\n",
    "               batchSize=BATCH_SIZE):\n",
    "    \"\"\"One training step (on a **batch** of pairs of sentences).\"\"\"\n",
    "    # Clear previous grads\n",
    "    # WHY: Since the backward() function accumulates gradients, \n",
    "    #      and you don’t want to mix up gradients between minibatches, \n",
    "    #      you have to zero them out at the start of a new minibatch. \n",
    "    #      This is exactly like how a general (additive) accumulator \n",
    "    #      variable is initialized to 0 in code.\n",
    "    encoderOptim.zero_grad()\n",
    "    decoderOptim.zero_grad()\n",
    "    # Set up loss\n",
    "    loss = 0\n",
    "    # Run encoder\n",
    "    encoderHidden = None\n",
    "    encoderOutput, encoderHidden = encoder(inputs, inputsLen, encoderHidden)\n",
    "    # Run decoder\n",
    "    decoderInput = Variable(torch.LongTensor([INDEXER.get_index('SOS')]*batchSize))\n",
    "    decoderContext = Variable(torch.zeros(batchSize,decoder.hiddenSize))\n",
    "    decoderHidden = encoderHidden\n",
    "    enforce = random.random() < enforcingRatio\n",
    "    maxTargetLen = max(targetsLen)\n",
    "    decoderOutputAll = Variable(torch.zeros(maxTargetLen,batchSize,decoder.outputSize))\n",
    "        # <mt-max,bc,vocab>\n",
    "    for di in range(maxTargetLen):\n",
    "        decoderOutput,decoderHidden,decoderContext,attentionWeights = decoder(decoderInput,\n",
    "                                                                              decoderHidden,\n",
    "                                                                              decoderContext, \n",
    "                                                                              encoderOutput)\n",
    "        decoderOutputAll[di] = decoderOutput\n",
    "        if enforce:\n",
    "            decoderInput = targets[di] # <== targets is <mt,bc>\n",
    "        else:\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()) # <bc,1> -> <bc,>\n",
    "    # Sequence cross entropy\n",
    "    loss = batch_cross_entropy(decoderOutputAll.transpose(0,1).contiguous(), \n",
    "                               targets.transpose(0,1).contiguous(), \n",
    "                               targetsLen)\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoderOptim.step()\n",
    "    decoderOptim.step()\n",
    "    return loss.data[0] / targetsLen\n",
    "\n",
    "def train(pairs, lengths,\n",
    "          nEpochs=1, epochSize=100, lr=1e-4,\n",
    "          enforcingRatio=0.5, clip=5.0,\n",
    "          printEvery=5):\n",
    "    \"\"\"Train multiple **batch** steps.\"\"\"\n",
    "    dataSize = len(pairs)\n",
    "    encoder = EncoderRNN(INDEXER.nWords, HIDDEN_SIZE)\n",
    "    decoder = LuongDecoderRNN(HIDDEN_SIZE, INDEXER.nWords)\n",
    "    encoderOptim = optim.Adam(encoder.parameters(),lr)\n",
    "    decoderOptim = optim.Adam(decoder.parameters(),lr)\n",
    "    averageLoss = 0\n",
    "    start = time.time()\n",
    "    for e in range(nEpochs):\n",
    "        epochLoss = 0\n",
    "        for step in range(epochSize):\n",
    "            inputs, inputsLen, targets, targetsLen = dataIter.random_batch()\n",
    "            loss = train_step(inputs, inputsLen, targets, targetsLen,\n",
    "                              encoder, decoder,\n",
    "                              encoderOptim, decoderOptim,\n",
    "                              enforcingRatio, clip) \n",
    "            if step!=0 and step%printEvery==0:\n",
    "                print(\"Step %d average loss = %.4f (time: %.2f)\" % (step, loss.mean(), # batch mean.\n",
    "                                                                    time.time()-start)) \n",
    "                start = time.time()\n",
    "            epochLoss += loss.mean()\n",
    "        epochLoss /= epochSize\n",
    "        averageLoss += epochLoss\n",
    "        print(\"\\nEpoch %d loss = %.4f\\n\" % (e+1,epochLoss))\n",
    "    averageLoss /= nEpochs\n",
    "    print(\"\\nGrand average loss = %.4f\\n\" % averageLoss)\n",
    "    return encoder, decoder\n",
    "            # READ BATCH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 average loss = 0.3159 (time: 6.34)\n",
      "Step 100 average loss = 0.3489 (time: 6.28)\n",
      "Step 150 average loss = 0.3132 (time: 6.20)\n",
      "Step 200 average loss = 0.3983 (time: 6.17)\n",
      "Step 250 average loss = 0.3404 (time: 6.07)\n",
      "Step 300 average loss = 0.3516 (time: 6.30)\n",
      "Step 350 average loss = 0.3445 (time: 6.59)\n",
      "Step 400 average loss = 0.3620 (time: 6.20)\n",
      "Step 450 average loss = 0.3759 (time: 6.74)\n",
      "Step 500 average loss = 0.4541 (time: 6.21)\n",
      "Step 550 average loss = 0.3790 (time: 6.11)\n",
      "Step 600 average loss = 0.5045 (time: 6.07)\n",
      "Step 650 average loss = 0.3989 (time: 7.15)\n",
      "Step 700 average loss = 0.3261 (time: 12.40)\n",
      "Step 750 average loss = 0.3721 (time: 11.60)\n",
      "Step 800 average loss = 0.3192 (time: 6.25)\n",
      "Step 850 average loss = 0.2596 (time: 5.90)\n",
      "Step 900 average loss = 0.3286 (time: 5.89)\n",
      "Step 950 average loss = 0.3857 (time: 5.84)\n",
      "\n",
      "Epoch 1 loss = 0.3693\n",
      "\n",
      "Step 50 average loss = 0.3666 (time: 11.58)\n",
      "Step 100 average loss = 0.2794 (time: 6.46)\n",
      "Step 150 average loss = 0.3357 (time: 5.97)\n",
      "Step 200 average loss = 0.3790 (time: 6.00)\n",
      "Step 250 average loss = 0.3616 (time: 5.87)\n",
      "Step 300 average loss = 0.3322 (time: 6.30)\n",
      "Step 350 average loss = 0.3343 (time: 5.96)\n",
      "Step 400 average loss = 0.3202 (time: 6.16)\n",
      "Step 450 average loss = 0.4030 (time: 6.01)\n",
      "Step 500 average loss = 0.4036 (time: 6.22)\n",
      "Step 550 average loss = 0.2521 (time: 6.17)\n",
      "Step 600 average loss = 0.3259 (time: 5.93)\n",
      "Step 650 average loss = 0.3078 (time: 6.24)\n",
      "Step 700 average loss = 0.3081 (time: 5.86)\n",
      "Step 750 average loss = 0.2660 (time: 5.92)\n",
      "Step 800 average loss = 0.3786 (time: 5.57)\n",
      "Step 850 average loss = 0.3046 (time: 4.84)\n",
      "Step 900 average loss = 0.3431 (time: 4.82)\n",
      "Step 950 average loss = 0.2935 (time: 5.26)\n",
      "\n",
      "Epoch 2 loss = 0.3263\n",
      "\n",
      "Step 50 average loss = 0.3406 (time: 10.54)\n",
      "Step 100 average loss = 0.3649 (time: 4.87)\n",
      "Step 150 average loss = 0.2770 (time: 5.01)\n",
      "Step 200 average loss = 0.2790 (time: 5.10)\n",
      "Step 250 average loss = 0.3558 (time: 4.96)\n",
      "Step 300 average loss = 0.3356 (time: 5.13)\n",
      "Step 350 average loss = 0.2978 (time: 4.77)\n",
      "Step 400 average loss = 0.2499 (time: 4.96)\n",
      "Step 450 average loss = 0.3285 (time: 5.21)\n",
      "Step 500 average loss = 0.3127 (time: 5.14)\n",
      "Step 550 average loss = 0.2602 (time: 5.10)\n",
      "Step 600 average loss = 0.2595 (time: 5.00)\n",
      "Step 650 average loss = 0.2968 (time: 5.01)\n",
      "Step 700 average loss = 0.2686 (time: 5.06)\n",
      "Step 750 average loss = 0.2792 (time: 5.15)\n",
      "Step 800 average loss = 0.2753 (time: 5.07)\n",
      "Step 850 average loss = 0.3059 (time: 5.05)\n",
      "Step 900 average loss = 0.2878 (time: 5.08)\n",
      "Step 950 average loss = 0.2854 (time: 5.12)\n",
      "\n",
      "Epoch 3 loss = 0.2994\n",
      "\n",
      "Step 50 average loss = 0.2955 (time: 9.98)\n",
      "Step 100 average loss = 0.3236 (time: 5.17)\n",
      "Step 150 average loss = 0.2639 (time: 4.82)\n",
      "Step 200 average loss = 0.3055 (time: 5.07)\n",
      "Step 250 average loss = 0.2534 (time: 4.77)\n",
      "Step 300 average loss = 0.2416 (time: 5.10)\n",
      "Step 350 average loss = 0.3099 (time: 4.99)\n",
      "Step 400 average loss = 0.2770 (time: 5.02)\n",
      "Step 450 average loss = 0.2806 (time: 4.91)\n",
      "Step 500 average loss = 0.3485 (time: 5.01)\n",
      "Step 550 average loss = 0.2664 (time: 5.32)\n",
      "Step 600 average loss = 0.3334 (time: 4.87)\n",
      "Step 650 average loss = 0.2593 (time: 5.14)\n",
      "Step 700 average loss = 0.2885 (time: 5.00)\n",
      "Step 750 average loss = 0.2622 (time: 5.09)\n",
      "Step 800 average loss = 0.2671 (time: 5.37)\n",
      "Step 850 average loss = 0.2793 (time: 4.95)\n",
      "Step 900 average loss = 0.2842 (time: 5.18)\n",
      "Step 950 average loss = 0.2159 (time: 5.21)\n",
      "\n",
      "Epoch 4 loss = 0.2746\n",
      "\n",
      "Step 50 average loss = 0.2084 (time: 10.52)\n",
      "Step 100 average loss = 0.3068 (time: 5.13)\n",
      "Step 150 average loss = 0.2506 (time: 5.00)\n",
      "Step 200 average loss = 0.3167 (time: 5.20)\n",
      "Step 250 average loss = 0.2192 (time: 5.14)\n",
      "Step 300 average loss = 0.2672 (time: 5.23)\n",
      "Step 350 average loss = 0.2774 (time: 5.19)\n",
      "Step 400 average loss = 0.2064 (time: 4.92)\n",
      "Step 450 average loss = 0.2517 (time: 4.85)\n",
      "Step 500 average loss = 0.2331 (time: 4.98)\n",
      "Step 550 average loss = 0.2922 (time: 5.07)\n",
      "Step 600 average loss = 0.2767 (time: 4.93)\n",
      "Step 650 average loss = 0.2948 (time: 5.23)\n",
      "Step 700 average loss = 0.2014 (time: 5.15)\n",
      "Step 750 average loss = 0.2510 (time: 5.18)\n",
      "Step 800 average loss = 0.2958 (time: 5.01)\n",
      "Step 850 average loss = 0.2570 (time: 4.97)\n",
      "Step 900 average loss = 0.2461 (time: 5.35)\n",
      "Step 950 average loss = 0.2048 (time: 5.09)\n",
      "\n",
      "Epoch 5 loss = 0.2520\n",
      "\n",
      "\n",
      "Grand average loss = 0.3043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = train(pairs, lengths, \n",
    "                         nEpochs=5, epochSize=1000,\n",
    "                         printEvery=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def random_evaluation(self, pairs, lengths, batchSize=2):\n",
    "        \"\"\"Randomly pick batchSize sentences from a given corpus and translate.\n",
    "        \n",
    "        Args: \n",
    "            pairs, lengths: input data, elements in list(list(),list()).\n",
    "            batchSize: size of pairs & lengths.\n",
    "        \"\"\"\n",
    "        dataIter = DataIterator(pairs, lengths)\n",
    "        inputs, inputsLen, targets, targetsLen = dataIter.random_batch(batchSize)\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(inputs, inputsLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([INDEXER.get_index('SOS')]*batchSize))\n",
    "        decoderContext = Variable(torch.zeros(batchSize,decoder.hiddenSize))\n",
    "        decoderHidden = encoderHidden\n",
    "        maxTargetLen = max(targetsLen)\n",
    "        predictions = []\n",
    "        for di in range(maxTargetLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()) # <bc,1> -> <bc,>\n",
    "            predictions.append(topIndices.view(-1).numpy())\n",
    "        inputs = inputs.data.numpy().transpose()\n",
    "        predictions = np.array(predictions).transpose() # <mt,bc> -> <bc,mt>\n",
    "        targets = targets.data.numpy().transpose()\n",
    "        for i,(input,pred,target) in enumerate(zip(inputs,predictions,targets)):\n",
    "            print(\"Example %d\" % (i+1))\n",
    "            print(\"INPUT >> %s\" % ' '.join(INDEXER.get_sentence_word(input)))\n",
    "            print(\"PRED >> %s\" % ' '.join(INDEXER.get_sentence_word(pred)))\n",
    "            print(\"TRUE >> %s\\n\" % ' '.join(INDEXER.get_sentence_word(target))) \n",
    "\n",
    "    def evaluate_sentence(self, sent, maxLen=10):\n",
    "        \"\"\"Evaluate a given sentence.\n",
    "        \n",
    "        Args:\n",
    "            sent: a sentence in string, where words are separated by whitespaces.\n",
    "            maxLen: the threshold at which the decoder stops.\n",
    "        \"\"\"\n",
    "        # Reformat data to the same as dataIter.random_batch(1)\n",
    "        sent = sent.split()\n",
    "        sentCode = INDEXER.get_sentence_index(sent)\n",
    "        if any(i==-1 for i in sentCode):\n",
    "            raise Exception(\"This sentence contains out of vocabulary words!\")\n",
    "        input = Variable(torch.LongTensor(sentCode)).view(-1,1)\n",
    "        inputLen = np.array([len(sentCode)])\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(input, inputLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([INDEXER.get_index('SOS')]*1))\n",
    "        decoderContext = Variable(torch.zeros(1,decoder.hiddenSize))\n",
    "        decoderHidden = encoderHidden\n",
    "        pred = []\n",
    "        for di in range(maxLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()) # <bc,1> -> <bc,>\n",
    "            predIndex = topIndices.view(-1).numpy()[0]\n",
    "            if predIndex == INDEXER.get_index('EOS'):\n",
    "                break\n",
    "            pred.append(predIndex)\n",
    "        print(\"INPUT >> %s\" % ' '.join(sent))\n",
    "        print(\"PRED >> %s\\n\" % ' '.join(INDEXER.get_sentence_word(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = Evaluator(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT >> B B C E F\n",
      "PRED >> C C C F G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ev.evaluate_sentence('B B C E F')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
