{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq\n",
    "\n",
    "* **Task**: toy \"translation\" task --- translating a list of letters (from A to H) to the next-letter-list (e.g. ['A', 'B', 'C'] translates as ['B', 'C', 'D']. \n",
    "* **Type**: Luong et al. (2016). No bidirection or stacking. Clear-to-the-boot step-by-step demo.\n",
    "* **PyTorch Version**: 0.3.1\n",
    "* **Rant**: showy people on Github write convoluted tutorial code (although efficient, sophisticated and all). Doesn't help for beginners at all! This tutorial tells you all you need to know!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \"\"\"Token-Index mapping.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name: name of the indexer.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1} # str -> int\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {\"SOS\": 0, \"EOS\": 0} # str -> int\n",
    "        self.nWords = 0  # Count SOS and EOS\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"Add a sentence to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of tokens (in string).\n",
    "        \"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a word to the dictionary.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (in string).\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.nWords\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.nWords] = word\n",
    "            self.nWords += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1 \n",
    "            \n",
    "    def get_index(self, word):\n",
    "        \"\"\"Word->Index lookup.\n",
    "        \n",
    "        Args:\n",
    "            word: a token (string).\n",
    "        Returns:\n",
    "            The index of the word.\n",
    "        \"\"\"\n",
    "        return self.word2index[word] if word in self.word2index else -1\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        \"\"\"Index->Word lookup.\n",
    "        \n",
    "        Args:\n",
    "            index: index of a token.\n",
    "        Returns:\n",
    "            The token under the index. -1 if the index is out of bound.\n",
    "        \"\"\"\n",
    "        return self.index2word[index] if index<self.nWords else -1\n",
    "    \n",
    "    def get_sentence_index(self, sentence):\n",
    "        \"\"\"Words->Indexs lookup.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a list of token (string).\n",
    "        Returns:\n",
    "            A list of indices.\n",
    "        \"\"\"\n",
    "        return [self.get_index(word) for word in sentence]\n",
    "    \n",
    "    def get_sentence_word(self, indexSentence):\n",
    "        \"\"\"Indexs->Words lookup.\n",
    "        \n",
    "        Args:\n",
    "            indexSentence: a list of indices.\n",
    "        Returns:\n",
    "            A list of tokens (string).\n",
    "        \"\"\"\n",
    "        return [self.get_word(index) for index in indexSentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data generation\n",
    "#   vocab -> A to I\n",
    "#   length -> 3 to 8\n",
    "#   task -> translate for the next letter (e.g. A -> B)\n",
    "\n",
    "VOCAB = [chr(i) for i in range(65,74)] # 'A' -> 'I'\n",
    "FROM_LEN, TO_LEN = 3, 8\n",
    "MAX_LENGTH = TO_LEN + 2\n",
    "SOS, EOS = 'SOS', 'EOS'\n",
    "INDEXER = Indexer('LetterTranslator')\n",
    "DATA_SIZE = 3000\n",
    "\n",
    "def translate_word(word):\n",
    "    \"\"\"Find the next letter.\n",
    "    \n",
    "    Args:\n",
    "        word: a letter word (e.g. 'A').\n",
    "    Returns:\n",
    "        The next letter to word.\n",
    "    \"\"\"\n",
    "    return VOCAB[VOCAB.index(word)+1]\n",
    "\n",
    "def translate_sent(sent):\n",
    "    \"\"\"Find the next-letter translation of a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sent: a list of letter words.\n",
    "    Returns:\n",
    "        The next letters.\n",
    "    \"\"\"\n",
    "    return [translate_word(word) for word in sent]\n",
    "\n",
    "def generate_pair():\n",
    "    \"\"\"Randomly generate a pair of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        randInput: a list of letter words.\n",
    "        randTarget: a list of translation letter words of randInput.\n",
    "        randInputLen, randTargetLen: lengths of the lists above.\n",
    "    \"\"\"\n",
    "    randInput = list(np.random.choice(VOCAB[:-1], size=random.randint(FROM_LEN,TO_LEN)))\n",
    "    randTarget = translate_sent(randInput)\n",
    "    randInputLen, randTargetLen = len(randInput), len(randTarget)\n",
    "    return randInput, randTarget+[str('EOS')], \\\n",
    "           randInputLen, randTargetLen+1\n",
    "        # str(): default is utf-8\n",
    "\n",
    "def generate_data():\n",
    "    \"\"\"Randomly generate a set of pairs of sentences (arg1 translates to arg2).\n",
    "    \n",
    "    Returns:\n",
    "        pairs: a pair of lists of torch Variables (torch.LongTensor).\n",
    "        lengths: lengths of the corresponding lists in pairs.\n",
    "    \"\"\"\n",
    "    pairs, lengths = [], []\n",
    "    for _ in range(DATA_SIZE):\n",
    "        randInput,randTarget,randInputLen,randTargetLen = generate_pair()\n",
    "        INDEXER.add_sentence(randInput)\n",
    "        INDEXER.add_sentence(randTarget)\n",
    "        pairs.append([Variable(torch.LongTensor(INDEXER.get_sentence_index(randInput)).view(-1,1)),\n",
    "                      Variable(torch.LongTensor(INDEXER.get_sentence_index(randTarget)).view(-1,1))])\n",
    "            # convert sentences to <mt,bc> shape.\n",
    "            # here bc=1.\n",
    "        lengths.append([randInputLen,randTargetLen])\n",
    "    return pairs, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, lengths = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq with attention, similar to Luong et al. (2016)\n",
    "#   Comment notation: mt = max-time; bc = batch-size; h = hidden-size.\n",
    "\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, hiddenSize, nLayers=1):\n",
    "        # inputSize: vocabulary size.\n",
    "        # hiddenSize: size for both embedding and GRU hidden.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers)\n",
    "    \n",
    "    def forward(self, inputs, inputsLen, hidden):\n",
    "        # inputs: <mt,bc>\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(inputs).view(inputsLen,1,-1) # <mt,bc=1,h>\n",
    "        output,hidden = self.gru(embedded, hidden)\n",
    "            # output: <mt,bc=1,h>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.nLayers,1,self.hiddenSize))\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"Basic linear attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.attention = nn.Linear(hiddenSize, hiddenSize)\n",
    "    \n",
    "    def forward(self, hidden, encoderOutput):\n",
    "        encoderOutputLen = len(encoderOutput)\n",
    "        attentionEnergies = Variable(torch.zeros(encoderOutputLen))\n",
    "        for i in range(encoderOutputLen):\n",
    "            attentionEnergies[i] = self.score(hidden, encoderOutput[i])\n",
    "                # encoderOutput[i]: 1 time step from <mt,bc,h>\n",
    "        return F.softmax(attentionEnergies,dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            # <mt,> -> <bc=1,1,mt>, technical convenience.\n",
    "        \n",
    "    def score(self, hidden, encoderOutput):\n",
    "            # hidden: <bc=1,h>\n",
    "            # encoderOutput: <bc=1,h> (1 time step).\n",
    "        energy = self.attention(encoderOutput)\n",
    "            # linear attention: <bc,h> * <h,h> -> <bc,h>\n",
    "        energy = hidden.dot(energy)\n",
    "            # dot: <bc,h> * <bc,h> -> <bc,h>\n",
    "            # .dot smartly find fitting dimensions.\n",
    "        return energy\n",
    "    \n",
    "class LuongDecoderRNN(nn.Module):\n",
    "    \"\"\"Luong attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize, outputSize, nLayers=1):\n",
    "        super(LuongDecoderRNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(outputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(2*hiddenSize, hiddenSize) \n",
    "        self.out = nn.Linear(2*hiddenSize, outputSize)\n",
    "            # inputSize doubles because concatted context of same hiddenSize.\n",
    "        self.linearAttention = LinearAttention(hiddenSize)\n",
    "        \n",
    "    def forward(self, input, hidden, context, encoderOutput):\n",
    "            # input: <mt=1,bc=1>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "            # context: <bc=1,h>\n",
    "            # encoderOutput: <mt,bc,h>\n",
    "        embedded = self.embedding(input).view(1,1,-1) # <mt=1,bc=1,h>\n",
    "        input = torch.cat((embedded,context.unsqueeze(0)),2)\n",
    "            # unsqueeze: <bc,h> -> <mt=1,bc,h>\n",
    "            # concat: <mt,bc,h> & <mt,bc,h> @2 -> <mt,bc,2h>\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "            # IN: <mt=1,bc,2h>, <n_layer*n_direction,bc,h>\n",
    "            # OUT: <mt=1,bc,h>, <n_layer*n_direction,bc,h>\n",
    "        attentionWeights = self.linearAttention(output.squeeze(0),\n",
    "                                                encoderOutput)\n",
    "            # squeeze: <mt=1,bc,h> -> <bc,h>\n",
    "            # attentionWeights: <bc=1,1,mt>\n",
    "        context = attentionWeights.bmm(encoderOutput.transpose(0,1))\n",
    "            # transpose: <mt,bc,h> -> <bc,mt,h>\n",
    "            # bmm (batched matrix multiplication): \n",
    "            #   <bc=1,1,mt> & <bc,mt,h> -> <bc=1,1,h>\n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "            # output squeeze: <mt=1,bc=1,h> -> <bc,h>\n",
    "            # context squeeze: <bc=1,1,h> -> <bc,h>\n",
    "        output = F.log_softmax(F.tanh(self.out(torch.cat((output,context),1))),dim=-1)\n",
    "            # concat: <bc,h> & <bc,h> @1 -> <bc,2h>\n",
    "            # linear->tahn/out: <bc,2h> * <2h,vocab> -> <bc,vocab>\n",
    "            # softmax: along dim=-1, i.e. vocab.\n",
    "        return output, hidden, context, attentionWeights\n",
    "            # full output for visualization:\n",
    "            #   output: <bc,vocab>\n",
    "            #   hidden: <n_layer*n_direction,bc,h>\n",
    "            #   context: <bc,h>\n",
    "            #   attentionWeights: <bc=1,1,mt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, inputsLen, targets, targetsLen,\n",
    "               encoder, decoder, \n",
    "               encoderOptim, decoderOptim, criterion,\n",
    "               enforcingRatio, clip):\n",
    "    \"\"\"One training step (on a single pair of sentences).\"\"\"\n",
    "    # Clear previous grads\n",
    "    # WHY: Since the backward() function accumulates gradients, \n",
    "    #      and you don’t want to mix up gradients between minibatches, \n",
    "    #      you have to zero them out at the start of a new minibatch. \n",
    "    #      This is exactly like how a general (additive) accumulator \n",
    "    #      variable is initialized to 0 in code.\n",
    "    encoderOptim.zero_grad()\n",
    "    decoderOptim.zero_grad()\n",
    "    # Set up loss\n",
    "    loss = 0\n",
    "    # Run encoder\n",
    "    encoderHidden = encoder.init_hidden()\n",
    "    encoderOutput, encoderHidden = encoder(inputs, inputsLen, encoderHidden)\n",
    "    # Run decoder\n",
    "    decoderInput = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "    decoderContext = Variable(torch.zeros(1,decoder.hiddenSize))\n",
    "    decoderHidden = encoderHidden\n",
    "    enforce = random.random() < enforcingRatio\n",
    "    for di in range(targetsLen):\n",
    "        decoderOutput,decoderHidden,decoderContext,attentionWeights = decoder(decoderInput,\n",
    "                                                                              decoderHidden,\n",
    "                                                                              decoderContext, \n",
    "                                                                              encoderOutput)\n",
    "        loss += criterion(decoderOutput, targets[di])\n",
    "        if enforce: # i.e. feed gold target tokens in training.\n",
    "            decoderInput = targets[di] # decoderInput can be 1 or 1x1 \n",
    "        else:\n",
    "            topValue,topIndex = decoderOutput.data.topk(1)\n",
    "            decoderInput = Variable(topIndex)\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoderOptim.step()\n",
    "    decoderOptim.step()\n",
    "    return loss.data[0] / targetsLen\n",
    "\n",
    "def train(pairs, lengths,\n",
    "          nEpochs=1, epochSize=1000, lr=1e-4,\n",
    "          enforcingRatio=0.5, clip=5.0,\n",
    "          printEvery=100):\n",
    "    \"\"\"Train multiple steps.\"\"\"\n",
    "    dataSize = len(pairs)\n",
    "    encoder = EncoderRNN(INDEXER.nWords, HIDDEN_SIZE)\n",
    "    decoder = LuongDecoderRNN(HIDDEN_SIZE, INDEXER.nWords)\n",
    "    encoderOptim = optim.Adam(encoder.parameters(),lr)\n",
    "    decoderOptim = optim.Adam(decoder.parameters(),lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    averageLoss = 0\n",
    "    for e in range(nEpochs):\n",
    "        start = time.time()\n",
    "        epochLoss = 0\n",
    "        for step in range(epochSize):\n",
    "            i = random.choice(range(0,dataSize))\n",
    "            inputs, targets = pairs[i]\n",
    "            inputsLen, targetsLen = lengths[i]\n",
    "            loss = train_step(inputs, inputsLen, targets, targetsLen,\n",
    "                              encoder, decoder,\n",
    "                              encoderOptim, decoderOptim, criterion,\n",
    "                              enforcingRatio, clip)\n",
    "            if step!=0 and step%printEvery==0:\n",
    "                print(\"Step %d average loss = %.4f\" % (step, loss))\n",
    "            epochLoss += loss\n",
    "        epochLoss /= epochSize\n",
    "        averageLoss += epochLoss\n",
    "        print(\"\\nEpoch %d loss = %.4f (time: %.2f)\\n\" % (e+1,epochLoss,\n",
    "                                                         time.time()-start))\n",
    "    averageLoss /= nEpochs\n",
    "    print(\"\\nGrand average loss = %.4f\\n\" % averageLoss)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 average loss = 1.9816\n",
      "Step 1000 average loss = 2.0165\n",
      "Step 1500 average loss = 2.1684\n",
      "Step 2000 average loss = 1.9741\n",
      "Step 2500 average loss = 1.8433\n",
      "\n",
      "Epoch 1 loss = 1.9717 (time: 72.23)\n",
      "\n",
      "Step 500 average loss = 1.7264\n",
      "Step 1000 average loss = 1.4872\n",
      "Step 1500 average loss = 1.4274\n",
      "Step 2000 average loss = 1.6146\n",
      "Step 2500 average loss = 1.4620\n",
      "\n",
      "Epoch 2 loss = 1.6652 (time: 67.30)\n",
      "\n",
      "Step 500 average loss = 1.6495\n",
      "Step 1000 average loss = 1.4078\n",
      "Step 1500 average loss = 1.6295\n",
      "Step 2000 average loss = 1.7015\n",
      "Step 2500 average loss = 1.2893\n",
      "\n",
      "Epoch 3 loss = 1.4910 (time: 66.06)\n",
      "\n",
      "Step 500 average loss = 1.2579\n",
      "Step 1000 average loss = 1.5437\n",
      "Step 1500 average loss = 1.7422\n",
      "Step 2000 average loss = 1.1936\n",
      "Step 2500 average loss = 1.2038\n",
      "\n",
      "Epoch 4 loss = 1.3755 (time: 64.57)\n",
      "\n",
      "Step 500 average loss = 1.2677\n",
      "Step 1000 average loss = 1.2960\n",
      "Step 1500 average loss = 1.3592\n",
      "Step 2000 average loss = 1.3739\n",
      "Step 2500 average loss = 0.9951\n",
      "\n",
      "Epoch 5 loss = 1.2489 (time: 64.97)\n",
      "\n",
      "\n",
      "Grand average loss = 1.5504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = train(pairs, lengths, \n",
    "                         nEpochs=5, epochSize=len(pairs),\n",
    "                         printEvery=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, inputsLen, targets, targetsLen,\n",
    "               encoder, decoder, \n",
    "               encoderOptim, decoderOptim, criterion,\n",
    "               enforcingRatio, clip):\n",
    "    \"\"\"One training step (on a single pair of sentences).\"\"\"\n",
    "    # Clear previous grads\n",
    "    # WHY: Since the backward() function accumulates gradients, \n",
    "    #      and you don’t want to mix up gradients between minibatches, \n",
    "    #      you have to zero them out at the start of a new minibatch. \n",
    "    #      This is exactly like how a general (additive) accumulator \n",
    "    #      variable is initialized to 0 in code.\n",
    "    encoderOptim.zero_grad()\n",
    "    decoderOptim.zero_grad()\n",
    "    # Set up loss\n",
    "    loss = 0\n",
    "    # Run encoder\n",
    "    encoderHidden = encoder.init_hidden()\n",
    "    encoderOutput, encoderHidden = encoder(inputs, inputsLen, encoderHidden)\n",
    "    # Run decoder\n",
    "    decoderInput = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "    decoderContext = Variable(torch.zeros(1,decoder.hiddenSize))\n",
    "    decoderHidden = encoderHidden\n",
    "    enforce = random.random() < enforcingRatio\n",
    "    for di in range(targetsLen):\n",
    "        decoderOutput,decoderHidden,decoderContext,attentionWeights = decoder(decoderInput,\n",
    "                                                                              decoderHidden,\n",
    "                                                                              decoderContext, \n",
    "                                                                              encoderOutput)\n",
    "        loss += criterion(decoderOutput, targets[di])\n",
    "        if enforce: # i.e. feed gold target tokens in training.\n",
    "            decoderInput = targets[di] # decoderInput can be 1 or 1x1 \n",
    "        else:\n",
    "            topValue,topIndex = decoderOutput.data.topk(1)\n",
    "            decoderInput = Variable(topIndex)\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoderOptim.step()\n",
    "    decoderOptim.step()\n",
    "    return loss.data[0] / targetsLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sent, sentLen, target, targetLen,\n",
    "             encoder, decoder, \n",
    "             maxLength):\n",
    "    encoderHidden = encoder.init_hidden()\n",
    "    encoderOutput, encoderHidden = encoder(sent, sentLen, encoderHidden)\n",
    "    decoderInput = Variable(torch.LongTensor([[INDEXER.get_index('SOS')]]))\n",
    "    decoderContext = Variable(torch.zeros(1,decoder.hiddenSize))\n",
    "    decoderHidden = encoderHidden\n",
    "    prediction = []\n",
    "    lengthGen = 0\n",
    "    while True:\n",
    "        lengthGen += 1\n",
    "        decoderOutput,decoderHidden,decoderContext,attentionWeights = decoder(decoderInput,\n",
    "                                                                              decoderHidden,\n",
    "                                                                              decoderContext, \n",
    "                                                                              encoderOutput)\n",
    "        topValue,topIndex = decoderOutput.data.topk(1)\n",
    "        decoderInput = Variable(topIndex)\n",
    "        prediction += list(topIndex.squeeze().numpy())\n",
    "        if prediction[0] == INDEXER.get_index('EOS') or lengthGen>=maxLength:\n",
    "            break\n",
    "    sent = list(sent.data.squeeze().numpy())\n",
    "    target = list(target.data.squeeze().numpy())\n",
    "    print(\"INPUT >> %s\" % ' '.join(INDEXER.get_sentence_word(sent)))\n",
    "    print(\"PRED >> %s\" % ' '.join(INDEXER.get_sentence_word(prediction[:targetLen])))\n",
    "    print(\"TRUE >> %s\" % ' '.join(INDEXER.get_sentence_word(target)))\n",
    "    \n",
    "def random_evaluate(pairs, lengths,\n",
    "                    encoder, decoder,\n",
    "                    maxLength=15):\n",
    "    i = random.choice(range(0,len(pairs)))\n",
    "    sent, target = pairs[i]\n",
    "    sentLen, targetLen = lengths[i]\n",
    "    evaluate(sent, sentLen, target, targetLen, encoder, decoder, maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT >> B F A D E E H\n",
      "PRED >> C G B F B F I B\n",
      "TRUE >> C G B E F F I B\n"
     ]
    }
   ],
   "source": [
    "random_evaluate(pairs, lengths, encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
