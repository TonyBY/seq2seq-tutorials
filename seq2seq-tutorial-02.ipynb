{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: translating number words into numbers (e.g. one two three -> 1 2 3)\n",
    "\n",
    "* Seq2seq as in Sutskever et al. (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data generation\n",
    "\n",
    "import utils\n",
    "\n",
    "vocab = set(['PAD','EOS','1','2','3','4','5','one','two','three','four','five'])\n",
    "word2idx = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,\n",
    "            'one':7,'two':8,'three':9,'four':10,'five':11}\n",
    "idx2word = {idx:word for word,idx in word2idx.iteritems()}\n",
    "word2digit_translate = {'one':'1','two':'2','three':'3',\n",
    "                        'four':'4','five':'5'}\n",
    "word2digit_translate_byidx = {7:2,8:3,9:4,10:5,11:6}\n",
    "\n",
    "def code_sequence(s):\n",
    "    '''\n",
    "    Take a sentence, convert it to a list of words (in vocab), \n",
    "    then return idx encoding.\n",
    "    '''\n",
    "    seq = s.split()\n",
    "    return [word2idx[word] for word in seq]\n",
    "\n",
    "def decode_sequence(l):\n",
    "    '''\n",
    "    Take a list of indices, return words.\n",
    "    '''\n",
    "    return ' '.join([idx2word[idx] for idx in l])\n",
    "\n",
    "def encode(data):\n",
    "    '''\n",
    "    Take sentence data, encode it.\n",
    "    '''\n",
    "    return [code_sequence(dat) for dat in data]\n",
    "\n",
    "def to_readable(batch):\n",
    "    '''\n",
    "    Take a time-major batch of data, \n",
    "    return a list of translated words.\n",
    "    '''\n",
    "    batch_t = batch.transpose() # time-major -> batch-major\n",
    "    return [decode_sequence(dat) for dat in batch_t]\n",
    "\n",
    "# To transform data (i.e. list of sentences as wordlists) into\n",
    "# input data, feed it to utils.batch\n",
    "# sample results:\n",
    "# utils.batch([code_sequence('1 2 3')])\n",
    "# (array([[1],\n",
    "#         [2],\n",
    "#         [3]], dtype=int32), [3])\n",
    "# that is, a tuple (time major with shape [max_time, batch_size])\n",
    "# so with a batch of two, we get\n",
    "# (array([[1, 4],\n",
    "#         [2, 5],\n",
    "#         [3, 5]], dtype=int32), [3, 3])\n",
    "\n",
    "# Data generator\n",
    "\n",
    "def random_length(len_from, len_to):\n",
    "    if len_from == len_to:\n",
    "        return len_from\n",
    "    return np.random.randint(len_from, len_to)\n",
    "\n",
    "def random_batch(input_length_from, input_length_to,\n",
    "                 output_length_from, output_length_to,\n",
    "                 seq_length_from, seq_length_to,\n",
    "                 batch_size):\n",
    "    if input_length_from > input_length_to or \\\n",
    "        output_length_from > output_length_to:\n",
    "        raise ValueError('length_from > length_to')\n",
    "\n",
    "    input_batch = [np.random.randint(low=input_length_from,\n",
    "                                     high=input_length_to,\n",
    "                                     size=random_length(seq_length_from,\n",
    "                                                        seq_length_to)).tolist()\n",
    "                   for _ in range(batch_size)]\n",
    "    output_batch = [[word2digit_translate_byidx[idx] for idx in input_dat]\n",
    "                     for input_dat in input_batch]\n",
    "    return input_batch, output_batch\n",
    "      \n",
    "# Example:\n",
    "# digit_from = 2\n",
    "# digit_to = 6+1\n",
    "# word_from = 7\n",
    "# word_to = 11+1\n",
    "# a,b = random_batch(word_from,word_to,digit_from,digit_to,batch_size=2)\n",
    "# print a\n",
    "# [[11, 7, 11, 11, 10, 11, 9, 10, 10, 8, 9], [10, 7, 7, 9, 8, 9, 7, 8, 11]] <- indices of num words\n",
    "# print b\n",
    "# [[6, 2, 6, 6, 5, 6, 4, 5, 5, 3, 4], [5, 2, 2, 4, 3, 4, 2, 3, 6]] <- indices of their digit translation\n",
    "# print [decode_sequence(a_) for a_ in a]\n",
    "# ['five one five five four five three four four two three', 'four one one three two three one two five']\n",
    "# print [decode_sequence(b_) for b_ in b]\n",
    "# ['5 1 5 5 4 5 3 4 4 2 3', '4 1 1 3 2 3 1 2 5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2 # because encoder is going to be bidirectional.\n",
    "\n",
    "#                    decoder \n",
    "#                    target\n",
    "# \n",
    "# [] -> [] -> [#] -> [] -> []\n",
    "#                     |    ^\n",
    "# encoder             |____|\n",
    "# inputs             \n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs') # [max_time, batch_size]\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length') \n",
    "    # this takes a vector (length=batch_size), where each cell is the length of the\n",
    "    # correponding data entry. this doesn't affect time_major op.\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs) # [max_time, batch_size, emb_size]\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,encoder_bw_outputs), # both have [max_time, batch_size, emb_size]\n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = ( # state tuples: (c=[batch_size,emb_size],h=same)\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "# concat fw-bw separately, then make a combined final state!\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2) # concat on emb dim.\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1) # same thing.\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "    # getting the shape of a tensor [max_time, batch_size].\n",
    "    # doc: Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n",
    "    # WHY: ?dynamically keeping track of the shape?\n",
    "decoder_lengths = encoder_inputs_length + 3 # +2 steps, +1 for EOS.\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32) # for dec only!\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "    # shared weights in the dynamic unrolling of the decoder.\n",
    "    # W shape: [emb_concat, vocab]\n",
    "    # it will be matmuled in output * W: [batch_size, emb_concat] * [emb_concat, vocab]\n",
    "    # get: [batch_size, vocab], where we have allthe predictions (as multinomials)\n",
    "\n",
    "# prepare tokens for each time step\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS') # [batch_size]\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice) # [max_time, batch_size]\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "# Loop feed (doc: tf.nn.raw_rnn?)\n",
    "# (time, prev_cell_output, prev_cell_state, prev_loop_state) ->\n",
    "# (elem_finished, input, cell_state, output, loop_state)\n",
    "\n",
    "# handles first state (i.e. corresponds to the last state of the encoder)\n",
    "#\n",
    "#     state feed only (enc final state)\n",
    "#         |\n",
    "#         v\n",
    "#      # --> #\n",
    "#   last     first \n",
    "#   of enc   of dec\n",
    "#            ^\n",
    "#            |\n",
    "#           EOS\n",
    "#\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths) # all false (i.e. not done) at the init step.\n",
    "    initial_input = eos_step_embedded                  # it's a [batch_size] length boolean vector.\n",
    "        # \"input\": it's the input for the next state.\n",
    "        # in this case, the first cell of the decoder.\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None # these two None help us\n",
    "    initial_loop_state = None  # checking whether we are at the init step.\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "# handles the transitions in decoder after the first state\n",
    "#             ___\n",
    "#  output ->  |  |\n",
    "#             # -|-> #\n",
    "#              / |   ^\n",
    "#         state  |___| <- next_input (inpt)\n",
    "#\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    def get_next_input():\n",
    "        # at the first cell of the decoder, we take the feed from \n",
    "        # the final state of the encoder (handled by loop_fn_init),\n",
    "        # feed = EOS embedding\n",
    "        # and compute the first prediction. \n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "            # output * W: [batch_size, emb_concat] * [emb_concat, vocab]\n",
    "            # get: [batch_size, vocab], where we have all the predictions (as multinomials)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    elements_finished = (time >= decoder_lengths) # again a [batch_size] boolean vector.\n",
    "        # this returns a boolean tensor, e.g. [1, 1, 1, 0]\n",
    "        # this means the first three steps are done, but not the last.\n",
    "        # when all the steps are done, i.e. time (the real time) is larger than\n",
    "        # the specified max decoding steps, the vector is all 1.\n",
    "        # then the next line will return 1.\n",
    "    finished = tf.reduce_all(elements_finished) # maps to boolean scalar.\n",
    "    inpt = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "        # if finished, return a pad for next input (i.e. the feed to next step)\n",
    "        # otherwise, return get_next_input as usual.\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    # outputs:\n",
    "    # elements_finished: a [batch_size] boolean vector.\n",
    "    # inpt: [batch_size, emb_size] tensor for the next cell.\n",
    "    # state: (c,h) tuole, raw_rnn takes care of it.\n",
    "    # output: stored [batch_size, emb_size] tensor.\n",
    "    # loop_state: rnn_raw takes care of it.\n",
    "    return (elements_finished,\n",
    "            inpt, \n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "# combine the two fns above for a single looping function.\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    # time: an int32 scalar raw_rnn uses to keep track of time-steps internally.\n",
    "    # previous_output: [max_time, batch_size, emb_size] tensor.\n",
    "    # previous_state: (c,h) tuple.\n",
    "    # previous_loop_state: raw_rnn uses to keep track of where it is in the loop (automatic).\n",
    "    if previous_state is None: # time = 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn) # we have an LSTM cell.\n",
    "    # *_ta: the RNN output (TensorArray <- for dynamic use)\n",
    "    # *_final_state: 2-tuple of [batch_size, emb_size] (i.e. c and h). of no use for seq2seq.\n",
    "    # _: final_loop_state, which no one gives a fuck (used internally by *.raw_rnn backend).\n",
    "decoder_outputs = decoder_outputs_ta.stack() # [max_time, batch_size, emb_concat]\n",
    "\n",
    "decoder_max_step, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "    # for matmul, we do\n",
    "    # [max_time, batch_size, emb_concat], [max_time*batch_size, emb_concat]\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_step, decoder_batch_size, vocab_size))\n",
    "    # put it back into the original shaping scheme.\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2) # [max_time, batch_size]\n",
    "\n",
    "# Optimization\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits\n",
    ")\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.41256952286\n",
      "  sample 1:\n",
      "    input     > two two five four PAD PAD PAD\n",
      "    predicted > EOS EOS EOS EOS five five five PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > one three five one five five PAD\n",
      "    predicted > 5 5 4 4 4 4 4 4 4 PAD\n",
      "  sample 3:\n",
      "    input     > five three two one one one two\n",
      "    predicted > EOS EOS EOS EOS five five five five five five\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.519072413445\n",
      "  sample 1:\n",
      "    input     > one three five PAD PAD PAD PAD\n",
      "    predicted > 1 3 5 EOS PAD PAD PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > one five two four two five three\n",
      "    predicted > 1 5 2 2 2 5 3 EOS PAD PAD\n",
      "  sample 3:\n",
      "    input     > four four two one five one four\n",
      "    predicted > 4 4 2 1 5 4 4 EOS PAD PAD\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.198304265738\n",
      "  sample 1:\n",
      "    input     > five three five five PAD PAD PAD\n",
      "    predicted > 5 3 5 5 EOS PAD PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > two two two four one three two\n",
      "    predicted > 2 2 4 2 1 3 2 EOS PAD PAD\n",
      "  sample 3:\n",
      "    input     > five two five four one one two\n",
      "    predicted > 5 2 4 5 1 1 2 EOS PAD PAD\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.134261623025\n",
      "  sample 1:\n",
      "    input     > three three one four three PAD PAD\n",
      "    predicted > 3 3 1 4 3 EOS PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    input     > four three two five two PAD PAD\n",
      "    predicted > 4 3 2 5 2 EOS PAD PAD PAD PAD\n",
      "  sample 3:\n",
      "    input     > four four two five five four three\n",
      "    predicted > 4 4 2 5 5 4 3 EOS PAD PAD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digit_from = 2\n",
    "digit_to = 6+1\n",
    "word_from = 7\n",
    "word_to = 11+1\n",
    "seqlen_from = 3\n",
    "seqlen_to = 8\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "def next_feed(batch_size):\n",
    "    batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "    encoder_inputs_, encoder_inputs_lengths_ = utils.batch(batch_enc)\n",
    "    decoder_targets_, _ = utils.batch([seq + [word2idx['EOS']] + [word2idx['PAD']]*2 for seq in batch_dec])\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_inputs_lengths_,\n",
    "        decoder_targets: decoder_targets_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed(batch_size)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(decode_sequence(inp)))\n",
    "                print('    predicted > {}'.format(decode_sequence(pred)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment block\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# a = tf.placeholder(tf.int32, shape=(None,None))\n",
    "# # b = tf.one_hot(a, depth=7)\n",
    "# # b = tf.unstack(tf.shape(a), axis=0)\n",
    "# # c = tf.unstack(tf.shape(a), axis=1)\n",
    "# # c = tf.shape(a)\n",
    "# b = tf.stack(a)\n",
    "# c = tf.unstack(b)\n",
    "\n",
    "# a_ = np.array([[1,2,3],[4,5,6]])\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# # t1 = sess.run(b, feed_dict={a:a_})\n",
    "# # print t1, type(t1)\n",
    "# # t2,t3 = sess.run(c, feed_dict={a:a_})\n",
    "# # print t2,t3\n",
    "# # print t2, type(t2)\n",
    "# print sess.run(b, feed_dict={a:a_})\n",
    "# print sess.run(c, feed_dict={a:a_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
