{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Attention Residual Seq2Seq\n",
    "\n",
    "* Task: toy \"translation\" task --- translating a list of letters (from A to H) to the next-letter-list (e.g. ['A', 'B', 'C'] translates as ['B', 'C', 'D'].\n",
    "* Type: Bahdanau et al. (2015); Prakash et al. (2016). Bahdanau/15 attention, . Clear-to-the-boot step-by-step demo.\n",
    "* PyTorch Version: 0.3.1\n",
    "\n",
    "**NB:** the code should run _much_ faster on your machine. The printed results are done while multiple other things are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from nltk.translate.bleu_score import sentence_bleu as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "SMOOTH = SmoothingFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \"\"\"Word <-> Index mapper.\"\"\"\n",
    "\n",
    "    def __init__(self, specialTokenList=None):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.wordSet = set()\n",
    "        self.size = 0\n",
    "        if specialTokenList is not None:\n",
    "            assert type(specialTokenList)==list\n",
    "            for token in specialTokenList:\n",
    "                self.get_index(token, add=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"The indexer currently has %d words\" % self.size\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index] if index<self.size else 'UNK'\n",
    "\n",
    "    def get_index(self, word, add=True):\n",
    "        if add and word not in self.wordSet:\n",
    "            self.word2index[word] = self.size\n",
    "            self.index2word[self.size] = word\n",
    "            self.wordSet.add(word)\n",
    "            self.size += 1\n",
    "        return self.word2index[word] if word in self.wordSet else self.word2index['UNK']\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.wordSet\n",
    "\n",
    "    def add_sentence(self, sentence, returnIndices=True):\n",
    "        indices = [self.get_index(word, add=True) for word in sentence.split()]\n",
    "        return (indices,len(indices)) if returnIndices else None\n",
    "\n",
    "    def add_document(self, docPath, returnIndices=True):\n",
    "        with open(docPath, 'r') as doc:\n",
    "            if returnIndices:\n",
    "                indicesList, lengthList = [], []\n",
    "                for line in doc:\n",
    "                    indices,length = self.add_sentence(line,returnIndices)\n",
    "                    if length<=0: continue # handle bad sentences in .txt.\n",
    "                    indicesList.append(indices)\n",
    "                    lengthList.append(length)\n",
    "                return indicesList, lengthList\n",
    "            else:\n",
    "                for line in doc:\n",
    "                    self.add_sentence(line,returnIndices=False)\n",
    "                return None\n",
    "    \n",
    "    def to_words(self, indices):\n",
    "        return [self.get_word(index) for index in indices]\n",
    "    \n",
    "    def to_sent(self, indices):\n",
    "        return ' '.join(self.to_words(indices))\n",
    "    \n",
    "    def to_indices(self, words):\n",
    "        return [self.get_index(word) for word in words]\n",
    "\n",
    "class DataIterator:\n",
    "    \"\"\"Data feeder by batch.\"\"\"\n",
    "\n",
    "    def __init__(self, indexer, pairs, lengths, maxTargetLen=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            indexer: an Indexer object.\n",
    "            pairs: a list of pairs of token index lists.\n",
    "            lengths: a list of pairs of sentence length lists.\n",
    "            maxTargetLen: uniform to decoding length later (for cross entropy computing).\n",
    "        \"\"\"\n",
    "        self.indexer = indexer\n",
    "        self.pairs = pairs\n",
    "        self.lengths = lengths\n",
    "        self.maxTargetLen = maxTargetLen\n",
    "        self.size = len(pairs)\n",
    "        self.indices = range(self.size)\n",
    "\n",
    "    def _get_padded_sentence(self, index, maxSentLen, maxTargetLen):\n",
    "        \"\"\"Pad a sentence pair by EOS (pad both to the largest length of respective batch).\n",
    "\n",
    "        Args:\n",
    "            index: index of a sentence & length pair in self.pairs, self.lengths.\n",
    "            maxSentLen: the length of the longest source sentence.\n",
    "            maxTargetLen: the length of the longest target sentence.\n",
    "        Returns:\n",
    "            padded source sentence (list), its length (int), \n",
    "            padded target sentence (list), its length (int).\n",
    "        \"\"\"\n",
    "        sent1,sent2 = self.pairs[index][0], self.pairs[index][1]\n",
    "        length1,length2 = self.lengths[index][0], self.lengths[index][1]\n",
    "        paddedSent1 = sent1[:maxSentLen] if length1>maxSentLen else sent1+[self.indexer.get_index('EOS')]*(maxSentLen-length1)\n",
    "        paddedSent2 = sent2[:maxTargetLen] if length2>maxTargetLen else sent2+[self.indexer.get_index('EOS')]*(maxTargetLen-length2)\n",
    "        return paddedSent1,length1,paddedSent2,length2\n",
    "\n",
    "    def random_batch(self, batchSize):\n",
    "        \"\"\"Random batching.\n",
    "\n",
    "        Args:\n",
    "            batchSize: size of a batch of sentence pairs and respective lengths.\n",
    "        Returns:\n",
    "            the batch of source sentence (Variable(torch.LongTensor())),\n",
    "            the lengths of source sentences (numpy.array())\n",
    "            and the same for target sentences and lengths.\n",
    "        \"\"\"\n",
    "        batchIndices = np.random.choice(self.indices, size=batchSize, replace=False)\n",
    "        batchSents,batchTargets,batchSentLens,batchTargetLens = [], [], [], []\n",
    "        maxSentLen, _ = np.array([self.lengths[index] for index in batchIndices]).max(axis=0)\n",
    "        for index in batchIndices:\n",
    "            paddedSent1,length1,paddedSent2,length2 = self._get_padded_sentence(index, maxSentLen, self.maxTargetLen)\n",
    "            batchSents.append(paddedSent1)\n",
    "            batchTargets.append(paddedSent2)\n",
    "            batchSentLens.append(length1)\n",
    "            batchTargetLens.append(length2)\n",
    "        batchIndices = range(batchSize) # reindex from 0 for sorting.\n",
    "        batchIndices = [i for i,l in sorted(zip(batchIndices,batchSentLens),key=lambda p:p[1],reverse=True)]\n",
    "        batchSents = Variable(torch.LongTensor(np.array(batchSents)[batchIndices])).transpose(0,1) # <bc,mt> -> <mt,bc>\n",
    "        batchTargets = Variable(torch.LongTensor(np.array(batchTargets)[batchIndices])).transpose(0,1)\n",
    "        batchSentLens = np.array(batchSentLens)[batchIndices]\n",
    "        batchTargetLens = np.array(batchTargetLens)[batchIndices]\n",
    "        return batchSents, batchSentLens, batchTargets, batchTargetLens\n",
    "        \n",
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, dataDir):\n",
    "        self.dataDir = dataDir # str\n",
    "        self.dataDict = {path.split('.')[0]:self.dataDir+path\n",
    "                         for path in os.listdir(self.dataDir) if path.endswith('.txt')}\n",
    "        self.filenames = set(['train_source', 'train_target',\n",
    "                              'test_source', 'test_target'])\n",
    "        if self._filename_mismatch():\n",
    "            raise Exception(\"Expected filenames under the directory:\\n\"+str(self.filenames)+\n",
    "                            '\\nGot:\\n'+str(self.dataDict.keys())+'\\n')\n",
    "    \n",
    "    def _filename_mismatch(self):\n",
    "        return self.filenames - set(self.dataDict.keys()) != set([])\n",
    "    \n",
    "class TranslationDataLoad(DataLoader):\n",
    "    \n",
    "    def __init__(self, dataDir):\n",
    "        DataLoader.__init__(self, dataDir)\n",
    "    \n",
    "    def load(self, specialTokenList=None):\n",
    "        indexer = Indexer(specialTokenList)\n",
    "        print \"... loading training data.\"\n",
    "        trainPairs,trainLens = self._load_pairs(indexer,\n",
    "                                                self.dataDict['train_source'],\n",
    "                                                self.dataDict['train_target'])\n",
    "        print \"... loading test data.\"\n",
    "        testPairs,testLens = self._load_pairs(indexer,\n",
    "                                              self.dataDict['test_source'],\n",
    "                                              self.dataDict['test_target'])\n",
    "        print \"Done!\\n\"\n",
    "        return indexer,trainPairs,trainLens,testPairs,testLens\n",
    "\n",
    "    def _load_pairs(self, indexer, sourcePath, targetPath):\n",
    "        sourceSents,sourceLens = indexer.add_document(sourcePath,returnIndices=True)\n",
    "        targetSents,targetLens = indexer.add_document(targetPath,returnIndices=True)\n",
    "        return zip(sourceSents, targetSents), zip(sourceLens, targetLens)\n",
    "\n",
    "class ToyDataGenerator:\n",
    "    \"\"\"Generate toy translation dataset: translation to the next letter.\n",
    "       E.g. ['A','B','C'] -> ['B','C','D'].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, dataSize, fromLen, toLen, cutoff,\n",
    "                 trainSourcePath, trainTargetPath, testSourcePath, testTargetPath):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            vocab: a list of vocabulary words (in strings).\n",
    "            dataSize: #entries for train + test.\n",
    "            fromLen, toLen: length range of generated sentences.\n",
    "            cutoff: a float between 0. and 1., proportion of training data.\n",
    "            train/testSource/Path: path to saved generation.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.dataSize = dataSize\n",
    "        self.fromLen = fromLen\n",
    "        self.toLen = toLen\n",
    "        self.cutoffIndex = (int)(cutoff*dataSize)\n",
    "        self.trainSourcePath = trainSourcePath\n",
    "        self.trainTargetPath = trainTargetPath\n",
    "        self.testSourcePath = testSourcePath\n",
    "        self.testTargetPath = testTargetPath\n",
    "    \n",
    "    def translate_word(self, word):\n",
    "        \"\"\"Find the next letter.\n",
    "\n",
    "        Args:\n",
    "            word: a letter word (e.g. 'A').\n",
    "        Returns:\n",
    "            The next letter to word.\n",
    "        \"\"\"\n",
    "        return self.vocab[self.vocab.index(word)+1]\n",
    "    \n",
    "    def translate_sent(self, sent):\n",
    "        \"\"\"Find the next-letter translation of a sentence.\n",
    "\n",
    "        Args:\n",
    "            sent: a list of letter words.\n",
    "        Returns:\n",
    "            The next letters.\n",
    "        \"\"\"\n",
    "        return [self.translate_word(word) for word in sent]\n",
    "    \n",
    "    def writeSentToFile(self, path, sent, mode):\n",
    "        \"\"\"Write one sentence to a file (given path).\n",
    "        \n",
    "        Args:\n",
    "            path: path to file.\n",
    "            sent: a single-string sentence.\n",
    "            mode: either 'a' (append) or 'w' (write).\n",
    "        \"\"\"\n",
    "        with open(path, mode) as f:\n",
    "            f.write(sent)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    def writeDataSet(self, sourcePath, targetPath, size):\n",
    "        \"\"\"Write train/test dataset to given source/target path (given size).\n",
    "        \n",
    "        Args:\n",
    "            SourcePath, targetPath: paths to files.\n",
    "            size: #entries.\n",
    "        \"\"\"\n",
    "        for i in range(size):\n",
    "            randInput,randTarget = self.generate_pair()\n",
    "            self.writeSentToFile(sourcePath, ' '.join(randInput), mode='a' if os.path.exists(sourcePath) else 'w')\n",
    "            self.writeSentToFile(targetPath, ' '.join(randTarget), mode='a' if os.path.exists(targetPath) else 'w')            \n",
    "    \n",
    "    def generate_pair(self):\n",
    "        \"\"\"Randomly generate a pair of sentences (arg1 translates to arg2).\n",
    "\n",
    "        Returns:\n",
    "            randInput: a list of letter words.\n",
    "            randTarget: a list of translation letter words of randInput.\n",
    "        \"\"\"        \n",
    "        randInput = list(np.random.choice(self.vocab[:-1], size=random.randint(self.fromLen,self.toLen)))\n",
    "        randTarget = self.translate_sent(randInput)\n",
    "        return randInput, randTarget+[str('EOS')]\n",
    "    \n",
    "    def generate_data(self):\n",
    "        \"\"\"Randomly generate a set of pairs of sentences (arg1 translates to arg2).\n",
    "\n",
    "        Returns:\n",
    "            pairs: a pair of lists of torch Variables (torch.LongTensor).\n",
    "            lengths: lengths of the corresponding lists in pairs.\n",
    "        \"\"\"\n",
    "        print '... generating training data'\n",
    "        self.writeDataSet(self.trainSourcePath, self.trainTargetPath, self.cutoffIndex)\n",
    "        print '... generating test data'\n",
    "        self.writeDataSet(self.testSourcePath, self.testTargetPath, self.dataSize-self.cutoffIndex)\n",
    "        print 'DONE!\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... generating training data\n",
      "... generating test data\n",
      "DONE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('toy'):\n",
    "    os.makedirs('toy') \n",
    "tg = ToyDataGenerator(vocab=[chr(i) for i in range(65,74)],\n",
    "                      dataSize=3000,\n",
    "                      fromLen=3, toLen=8,\n",
    "                      cutoff=0.8,\n",
    "                      trainSourcePath='toy/train_source.txt',\n",
    "                      trainTargetPath='toy/train_target.txt',\n",
    "                      testSourcePath='toy/test_source.txt',\n",
    "                      testTargetPath='toy/test_target.txt')\n",
    "tg.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading training data.\n",
      "... loading test data.\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = TranslationDataLoad('toy/')\n",
    "indexer,trainPairs,trainLens,testPairs,testLens = trans.load(specialTokenList=['EOS','PAD','UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 4, 5, 6, 5, 6], [4, 8, 9, 5, 9, 5, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPairs[0][0], trainPairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D E A F', 'E F B G EOS')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print indexer.to_sent(trainPairs[0][0]), indexer.to_sent(trainPairs[0][1])\n",
    "print indexer.to_sent(testPairs[0][0]), indexer.to_sent(testPairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, hiddenSize, nLayers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputSize: vocabulary size.\n",
    "            hiddenSize: size of RNN hidden state.\n",
    "            nLayers: number of stacked layers.\n",
    "            dropout: dropout rate.\n",
    "        \"\"\"\n",
    "        # inputSize: vocabulary size.\n",
    "        # hiddenSize: size for both embedding and GRU hidden.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.dropoutLayer = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers, dropout=dropout)\n",
    "    \n",
    "    def forward(self, inputs, inputsLen, hidden=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputs: Variable(torch.LongTensor()) of the shape <max-time,batch-size>.\n",
    "            inputsLen: a list of input lengths with the shape <batch-size,>.\n",
    "            hidden: input hidden state (initialized as None).\n",
    "        \"\"\"\n",
    "        # inputs: <mt,bc>\n",
    "        # inputsLen: <bc,> (a list).\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(inputs) # <mt,bc,h>\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, inputsLen)\n",
    "            # 'packed' has a 'data' and a 'batch_sizes' field.\n",
    "            #   'data' is a <sum(len),h> matrix (len is real lengths, not padded).\n",
    "            #   'batch_sizes' has the number of non-zero batches at each time-step.\n",
    "            # e.g. for this 'inputs'\n",
    "            #    2     1     3     0     2\n",
    "            #    6     8     1     6     2\n",
    "            #    0     7     0     8     8\n",
    "            #    6     4     2     1     1\n",
    "            #    1     8     1     1     1\n",
    "            #    6     1     1     1     1\n",
    "            #    0     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1  \n",
    "            # 'data' = 22 = 7+5+4+3+3 (1's are pads corresponding to 'EOS').\n",
    "            # 'batch_sizes' = [5, 5, 5, 3, 2, 1, 1].\n",
    "        outputs,hidden = self.gru(packed, hidden)#, dropout=dropout)\n",
    "            # outputs: same format as 'packed'.\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        outputs, outputsLen = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "            # outputs: <mt,bc,h>\n",
    "            # outputsLen: same as the 'batch_sizes' field of 'packed'. \n",
    "        outputs = self.dropoutLayer(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "\n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU decoder (Bahdanau attention).\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize, outputSize, nLayers=2, dropout=0.1, residual=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            hiddenSize: GRU hidden state size.\n",
    "            outputSize: vocabulary size.\n",
    "            nLayers: number of stacked layers.\n",
    "            dropout: dropout rate.\n",
    "            residual: boolean, whether establish residual link or not.\n",
    "        \"\"\"\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.nLayers = nLayers\n",
    "        self.residual = residual\n",
    "        self.embedding = nn.Embedding(outputSize, hiddenSize)\n",
    "        self.dropoutLayer = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(2*hiddenSize, hiddenSize, nLayers) \n",
    "        self.out = nn.Linear(2*hiddenSize, outputSize)\n",
    "            # inputSize doubles because concatted context of same hiddenSize.\n",
    "        self.linear = nn.Linear(hiddenSize, hiddenSize)\n",
    "\n",
    "    def forward(self, inputs, hidden, context, encoderOutput):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputs: inputs to decoder, of the shape <batch-size,> (1 time-step).\n",
    "            hidden: <n_layers*n_directions,batch-size,hidden-size>.\n",
    "            context: context vector made using attention, <batch-size,hidden-size>.\n",
    "            encoderOutput: <max-time,batch-size,hidden-size>.\n",
    "            NB: all are Variable(torch.LongTensor()).\n",
    "        Returns:\n",
    "            output: <batch-size,vocab-size>.\n",
    "            hidden: <n_layers*n_directions,batch-size,hidden-size>.\n",
    "            context: <batch-size,hidden-size>.\n",
    "            attentionWeights: <batch-size,max-time>.\n",
    "        \"\"\"\n",
    "            # inputs: <bc,>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "            # context: <bc,h>\n",
    "            # encoderOutput: <mt,bc,h>  \n",
    "        batchSize = inputs.size(0)\n",
    "        encoderOutputLen = encoderOutput.size(0)\n",
    "        embedded = self.embedding(inputs).view(1,batchSize,self.hiddenSize) # <mt=1,bc,h>\n",
    "        inputs = torch.cat((embedded,context.unsqueeze(0)),2)\n",
    "            # unsqueeze: <bc,h> -> <mt=1,bc,h>\n",
    "            # concat: <mt,bc,h> & <mt,bc,h> @2 -> <mt,bc,2h>\n",
    "        output, hidden = self.gru(inputs, hidden)#, dropout=dropout)\n",
    "            # IN: <mt=1,bc,2h>, <n_layer*n_direction,bc,h>\n",
    "            # OUT: <mt=1,bc,h>, <n_layer*n_direction,bc,h>\n",
    "        output = self.dropoutLayer(output)\n",
    "        hidden = hidden + embedded if self.residual else hidden\n",
    "        attentionWeights = Variable(torch.zeros(batchSize,encoderOutputLen))\n",
    "        for b in range(batchSize):\n",
    "            rawAttentionWeight = torch.mm(self.linear(encoderOutput[:,b,:]), \n",
    "                                          hidden[:,b,:][-1].unsqueeze(1)).squeeze()\n",
    "                # op1. linear transformation on encoderOutput (dot energy).\n",
    "                # op2. select <mt,h> and <1,h> slices (from <mt,bc,h> and <1,bc,h>).\n",
    "                # op3. sel hidden last dim <h,> and expand -> <mt,h> & <h,1> now.\n",
    "                # op4. matmul -> <mt,1>.\n",
    "                # op5. squeeze -> <mt,>\n",
    "            attentionWeights[b] = F.softmax(rawAttentionWeight, dim=-1)\n",
    "                # normalize to get a distribution.\n",
    "            # result: <bc,mt> attention matrix, normalized along mt.\n",
    "        multiDiag = Variable(torch.eye(batchSize).expand(self.hiddenSize,batchSize,batchSize),\n",
    "                             requires_grad=False)\n",
    "            # op1. eye -> <bc,bc> diagonal matrix mask.\n",
    "            # op2. expand -> <h,bc,bc>, same shape as attended encoderOutput.\n",
    "            # op3. Variable/grad=false: same type as attended encoderOutput.\n",
    "        context = (torch.matmul(attentionWeights, encoderOutput.permute(2,0,1)) * multiDiag).sum(dim=2).transpose(0,1)\n",
    "            # op1. masking -> <h,bc,bc>, with the last 2 dims only have non-zero diag elems.\n",
    "            # op2. compress 1 bc dimension (useless, because its diag).\n",
    "            # op3. <h,bc> -> <bc,h>, keep input shape.\n",
    "        output = output.squeeze(0)\n",
    "            # output squeeze: <mt=1,bc=1,h> -> <bc,h>, to concat with context\n",
    "        output = F.log_softmax(F.tanh(self.out(torch.cat((output,context),1))),dim=-1)\n",
    "            # concat: <bc,h> & <bc,h> @1 -> <bc,2h>\n",
    "            # linear->tahn/out: <bc,2h> * <2h,vocab> -> <bc,vocab>\n",
    "            # softmax: along dim=-1, i.e. vocab.  \n",
    "        return output, hidden, context, attentionWeights\n",
    "            # full output for visualization:\n",
    "            #   output: <bc,vocab>\n",
    "            #   hidden: <n_layer*n_direction,bc,h>\n",
    "            #   context: <bc,h>\n",
    "            #   attentionWeights: <bc,mt> \n",
    "\n",
    "class Seq2Seq:\n",
    "    \"\"\"Encoder-Decoder model with Bahdanau attention, stacking and residual links.\"\"\"\n",
    "    \n",
    "    def __init__(self, indexer, trainPairs, trainLens, testPairs, testLens, \n",
    "                 batchSize=5, hiddenSize=10,\n",
    "                 nLayers=2, dropout=0.1, residual=True, \n",
    "                 lr=1e-4, enforcingRatio=0.5, clip=5.0,\n",
    "                 maxDecodingLen=10,\n",
    "                 resultSavePath='toy/results.txt'):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            indexer: an Indexer object.\n",
    "            trainPairs, testPairs: each is a list of pairs of word index list.\n",
    "            trainLens, testLens: each is a list of pairs of length of word index list.\n",
    "            batchSize: int. (default=5)\n",
    "            hiddenSize: int. (default=10)\n",
    "            nLayers: number of GRU stacking layers. (default=2)\n",
    "            dropout: dropout rate. (default=0.1)\n",
    "            residual: boolean, whether to establish residual links. (default=True)\n",
    "            lr: learning rate, float. (default=1e-4 with Adam)\n",
    "            enforcingRatio: the percentage of teacher-enforced training. (default=0.5)\n",
    "            clip: gradient clip cap, float. (default=5.0)\n",
    "            resultSavePath: (input,prediction,target) sentence triples file path.\n",
    "        \"\"\"\n",
    "        self.indexer = indexer\n",
    "        self.trainIter = DataIterator(indexer, trainPairs, trainLens)\n",
    "        self.testIter = DataIterator(indexer, testPairs, testLens)\n",
    "        self.batchSize = batchSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.lr = lr\n",
    "        self.enforcingRatio = enforcingRatio\n",
    "        self.clip = clip\n",
    "        self.maxDecodingLen = maxDecodingLen\n",
    "        self.resultSavePath = resultSavePath\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Specify computational graph.\"\"\"\n",
    "        self.encoder = EncoderRNN(self.indexer.size, self.hiddenSize, \n",
    "                                  nLayers=self.nLayers, dropout=self.dropout)\n",
    "        self.decoder = AttentionDecoderRNN(self.hiddenSize, self.indexer.size,\n",
    "                                           nLayers=self.nLayers, dropout=self.dropout, residual=self.residual)\n",
    "        self.encoderOptim = optim.Adam(self.encoder.parameters(), self.lr)\n",
    "        self.decoderOptim = optim.Adam(self.decoder.parameters(), self.lr)\n",
    "    \n",
    "    def _model_config(self):\n",
    "        return 'Vocab Size = ' + str(self.indexer.size) + '\\n' + \\\n",
    "               'Train/Test Size = ' + str(self.trainIter.size)+'/'+str(self.testIter.size) + '\\n' + \\\n",
    "               'batchSize = ' + str(self.batchSize) + '; hiddenSize = ' + str(self.hiddenSize) + '\\n' + \\\n",
    "               'nLayers = ' + str(self.nLayers) + '; dropout = ' + str(self.dropout) + '\\n' + \\\n",
    "               'residual = ' + str(self.residual) + '; learning rate = ' + str(self.lr) + '\\n' + \\\n",
    "               'teacher enforce ratio = ' + str(self.enforcingRatio) + '; clip = ' + str(self.clip) + '\\nn'\n",
    "    \n",
    "    def _train_step(self):\n",
    "        \"\"\"One step of training.\"\"\"\n",
    "        inputs, inputsLen, targets, targetsLen = self.trainIter.random_batch(self.batchSize)\n",
    "        self.encoderOptim.zero_grad()\n",
    "        self.decoderOptim.zero_grad()\n",
    "        loss = 0\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(inputs, inputsLen, encoderHidden)    \n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*self.batchSize))\n",
    "        decoderContext = Variable(torch.zeros(self.batchSize,self.decoder.hiddenSize))\n",
    "        decoderHidden = encoderHidden\n",
    "        enforce = random.random() < self.enforcingRatio\n",
    "        decoderOutputAll = Variable(torch.zeros(self.maxDecodingLen,self.batchSize,self.decoder.outputSize))\n",
    "            # <mt-max,bc,vocab>\n",
    "        mask = torch.LongTensor([1]*self.batchSize) \n",
    "            # start with 1, a cell turns 0 to mask out generation after an EOS is seen.\n",
    "        for di in range(self.maxDecodingLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            decoderOutputAll[di] = decoderOutput\n",
    "            if enforce:\n",
    "                decoderInput = targets[di] # <== targets is <mt,bc>\n",
    "            else:\n",
    "                topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "                topIndices = topIndices.squeeze()# topIndices = <bc,>\n",
    "                for b in range(self.batchSize):\n",
    "                    if topIndices[b] == 0: # EOS\n",
    "                        mask[b] = 0\n",
    "                topIndices = topIndices * mask\n",
    "                decoderInput = Variable(topIndices)\n",
    "        # Batch cross entropy\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "            # requires arg1/pred = <#entries,vocab>, arg2/target = <#entries,>\n",
    "        decoderOutputAll = decoderOutputAll.view(-1, self.decoder.outputSize)\n",
    "            # reshape to <mt*bc,vocab>\n",
    "        targets = targets.contiguous().view(-1)\n",
    "            # reshape to <mt*bc>\n",
    "        loss = criterion(decoderOutputAll, targets)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.encoder.parameters(), self.clip)\n",
    "        torch.nn.utils.clip_grad_norm(self.decoder.parameters(), self.clip)\n",
    "        self.encoderOptim.step()\n",
    "        self.decoderOptim.step()\n",
    "        return loss.data[0] / targetsLen        \n",
    "    \n",
    "    def train(self, nEpochs=1, epochSize=100, printEvery=5):\n",
    "        \"\"\"Train on loaded data upon construction.\n",
    "        \n",
    "        Args:\n",
    "            nEpochs: number of epochs.\n",
    "            epochSize: number of batches trained in an epoch.\n",
    "            printEvery: frequency of results report.\n",
    "        \"\"\"\n",
    "        averageLoss = 0\n",
    "        start = time.time()\n",
    "        for e in range(nEpochs):\n",
    "            epochLoss = 0\n",
    "            for step in range(epochSize):\n",
    "                loss = self._train_step()\n",
    "                if step!=0 and step%printEvery==0:\n",
    "                    print(\"Step %d average loss = %.4f (time: %.2f)\" % (step, loss.mean(), # batch mean.\n",
    "                                                                        time.time()-start))\n",
    "                    start = time.time()\n",
    "                epochLoss += loss.mean()\n",
    "            epochLoss /= epochSize\n",
    "            averageLoss += epochLoss\n",
    "            print(\"\\nEpoch %d loss = %.4f\\n\" % (e+1,epochLoss))\n",
    "            averageBleu = self.evaluate_random(size=self.batchSize, saveResults=False, printResults=True)\n",
    "        averageLoss /= nEpochs\n",
    "        print(\"\\nGrand average loss = %.4f\\n\" % averageLoss) \n",
    "        \n",
    "    def _clear_special_tokens(self, words):\n",
    "        \"\"\"Clear all the PAD, UNK, EOS to avoid inflated BLEU.\n",
    "        \n",
    "        Args:\n",
    "            words: a list of tokens.\n",
    "        Returns:\n",
    "            a list of tokens which are not special tokens.\n",
    "        \"\"\"\n",
    "        return [word for word in words if word not in set([\"PAD\",\"UNK\",\"EOS\"])]\n",
    "\n",
    "    def evaluate_pair(self, predWords, targetWords):\n",
    "        \"\"\"Compute the BLEU score of a prediction given a reference.\n",
    "        \n",
    "        Args:\n",
    "            predWords: predicted words (a list of strings).\n",
    "            targetWords: reference, same type as preWords.\n",
    "        Returns:\n",
    "            The BLEU score (uses = nltk.translate.bleu_score.sentence_bleu).\n",
    "        \"\"\"\n",
    "        return bleu([self._clear_special_tokens(targetWords)], \n",
    "                     self._clear_special_tokens(predWords), smoothing_function=SMOOTH.method3)\n",
    "\n",
    "        \n",
    "    def evaluate_random(self, size, saveResults, printResults=True):\n",
    "        \"\"\"Randomly evaluate samples from the test set (which is loaded upon construction).\n",
    "        \n",
    "        Args:\n",
    "            size: number of samples evaluated (as a single batch).\n",
    "            printResults: print input, prediction and gold translation to console. (default=True)\n",
    "        Returns:\n",
    "            The average BLEU score in the batch.\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        inputs, inputsLen, targets, targetsLen = self.testIter.random_batch(size)\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(inputs, inputsLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*size))\n",
    "        decoderContext = Variable(torch.zeros(size,self.decoder.hiddenSize))\n",
    "        decoderHidden = encoderHidden\n",
    "        predictions = []\n",
    "        for di in range(self.maxDecodingLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()) # <bc,1> -> <bc,>\n",
    "            predictions.append(topIndices.view(-1).numpy())\n",
    "        inputs = inputs.data.numpy().transpose()\n",
    "        predictions = np.array(predictions).transpose() # <mt,bc> -> <bc,mt>\n",
    "        targets = targets.data.numpy().transpose()\n",
    "        bleuList = []\n",
    "        results = []\n",
    "        for i,(input,pred,target) in enumerate(zip(inputs,predictions,targets)):\n",
    "            inputWords = self._clear_special_tokens(self.indexer.to_words(input))\n",
    "            predWords = self._clear_special_tokens(self.indexer.to_words(pred))\n",
    "            targetWords = self._clear_special_tokens(self.indexer.to_words(target))\n",
    "            bleuCurr = self.evaluate_pair(predWords, targetWords)\n",
    "            bleuList.append(bleuCurr)\n",
    "            inputSent = ' '.join(inputWords)\n",
    "            predSent = ' '.join(predWords)\n",
    "            targetSent = ' '.join(targetWords)\n",
    "            results.append([inputSent, predSent, targetSent])\n",
    "            if printResults:\n",
    "                print(\"Example %d\" % (i+1))\n",
    "                print(\"INPUT >> %s\" % inputSent)\n",
    "                print(\"PRED >> %s\" % predSent)\n",
    "                print(\"TRUE >> %s\" % targetSent)\n",
    "                print(\"[BLEU] %.2f\\n\" % bleuCurr)\n",
    "        averageBleu = np.mean(bleuList)\n",
    "        if saveResults:\n",
    "            return averageBleu, results\n",
    "        return averageBleu\n",
    "\n",
    "    def evaluate(self, nBatches=10, saveResults=True):\n",
    "        \"\"\"Randomly evaluate a given number of batches.\n",
    "        \n",
    "        Args:\n",
    "            nBatches: the number of random batches to be evaluated.\n",
    "        \"\"\"\n",
    "        averageBleuList = []\n",
    "        for i in range(nBatches):\n",
    "            if saveResults:\n",
    "                averageBleu, results = self.evaluate_random(self.batchSize, saveResults, printResults=False)\n",
    "                averageBleuList.append(averageBleu)\n",
    "                with open(self.resultSavePath, 'a') as f:\n",
    "                    if i==0:\n",
    "                        f.write(self._model_config())\n",
    "                        f.write('=================================\\n')\n",
    "                    for input,pred,target in results:\n",
    "                        f.write('INPUT  >> ' + input + '\\n')\n",
    "                        f.write('PRED   >> ' + pred + '\\n')\n",
    "                        f.write('TARGET >> ' + target + '\\n\\n')\n",
    "            else:\n",
    "                averageBleuList.append(self.evaluate_random(self.batchSize, saveResults, printResults=False))\n",
    "        message = \"Average BLEU score over %d examples is %.4f\" % (self.batchSize*nBatches, \n",
    "                                                                   np.mean(averageBleuList))\n",
    "        with open(self.resultSavePath, 'a') as f:\n",
    "            f.write('=================================\\n')\n",
    "            f.write(message)\n",
    "        print message\n",
    "            \n",
    "    def evaluate_given(self, sent, maxLen=20):\n",
    "        \"\"\"Evaluate a give sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a single string. OOVs are treated as UNKs.\n",
    "            maxLen: the max number of decoding steps.\n",
    "        \"\"\"\n",
    "        sent = sent.split()\n",
    "        sentCode = [self.indexer.get_index(word,add=False) for word in sent]\n",
    "        if any(i==-1 for i in sentCode):\n",
    "            raise Exception(\"This sentence contains out of vocabulary words!\")\n",
    "        input = Variable(torch.LongTensor(sentCode)).view(-1,1)\n",
    "        inputLen = np.array([len(sentCode)])\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(input, inputLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*1))\n",
    "        decoderContext = Variable(torch.zeros(1,self.decoder.hiddenSize))\n",
    "        decoderHidden = encoderHidden\n",
    "        pred = []\n",
    "        for di in range(maxLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()) # <bc,1> -> <bc,>\n",
    "            predIndex = topIndices.view(-1).numpy()[0]\n",
    "            if predIndex == self.indexer.get_index('EOS'):\n",
    "                break\n",
    "            pred.append(predIndex)\n",
    "        print(\"INPUT >> %s\" % ' '.join(sent))\n",
    "        print(\"PRED >> %s\\n\" % ' '.join(self.indexer.to_words(pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 average loss = 0.4358 (time: 4.23)\n",
      "Step 200 average loss = 0.2970 (time: 3.99)\n",
      "Step 300 average loss = 0.3574 (time: 4.03)\n",
      "Step 400 average loss = 0.3439 (time: 3.99)\n",
      "Step 500 average loss = 0.3837 (time: 3.96)\n",
      "\n",
      "Epoch 1 loss = 0.3578\n",
      "\n",
      "Example 1\n",
      "INPUT >> A H A E F C H G\n",
      "PRED >> \n",
      "TRUE >> B I B F G D I H\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> F A A A A G C C\n",
      "PRED >> \n",
      "TRUE >> G B B B B H D D\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E C H D C F E A\n",
      "PRED >> \n",
      "TRUE >> F D I E D G F B\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C C H B F C\n",
      "PRED >> \n",
      "TRUE >> D D I C G D\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> C E G B\n",
      "PRED >> \n",
      "TRUE >> D F H C\n",
      "[BLEU] 0.00\n",
      "\n",
      "Step 100 average loss = 0.3384 (time: 8.08)\n",
      "Step 200 average loss = 0.2910 (time: 3.96)\n",
      "Step 300 average loss = 0.3001 (time: 4.10)\n",
      "Step 400 average loss = 0.3095 (time: 3.90)\n",
      "Step 500 average loss = 0.3167 (time: 3.98)\n",
      "\n",
      "Epoch 2 loss = 0.3150\n",
      "\n",
      "Example 1\n",
      "INPUT >> C E A B H H B\n",
      "PRED >> \n",
      "TRUE >> D F B C I I C\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H F A G E B\n",
      "PRED >> \n",
      "TRUE >> I G B H F C\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> F E B A D\n",
      "PRED >> \n",
      "TRUE >> G F C B E\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B G B\n",
      "PRED >> \n",
      "TRUE >> C H C\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B H F\n",
      "PRED >> \n",
      "TRUE >> C I G\n",
      "[BLEU] 0.00\n",
      "\n",
      "Step 100 average loss = 0.3353 (time: 8.30)\n",
      "Step 200 average loss = 0.2962 (time: 4.15)\n",
      "Step 300 average loss = 0.2646 (time: 3.93)\n",
      "Step 400 average loss = 0.3219 (time: 3.92)\n",
      "Step 500 average loss = 0.2832 (time: 3.90)\n",
      "\n",
      "Epoch 3 loss = 0.2992\n",
      "\n",
      "Example 1\n",
      "INPUT >> C D F C H A B D\n",
      "PRED >> D\n",
      "TRUE >> D E G D I B C E\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C E F F H D E\n",
      "PRED >> G\n",
      "TRUE >> D F G G I E F\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H D F C E F\n",
      "PRED >> G\n",
      "TRUE >> I E G D F G\n",
      "[BLEU] 0.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> G D H B F\n",
      "PRED >> I\n",
      "TRUE >> H E I C G\n",
      "[BLEU] 0.01\n",
      "\n",
      "Example 5\n",
      "INPUT >> G H F H D\n",
      "PRED >> I\n",
      "TRUE >> H I G I E\n",
      "[BLEU] 0.01\n",
      "\n",
      "Step 100 average loss = 0.3252 (time: 8.24)\n",
      "Step 200 average loss = 0.2689 (time: 3.89)\n",
      "Step 300 average loss = 0.2882 (time: 4.12)\n",
      "Step 400 average loss = 0.3247 (time: 3.95)\n",
      "Step 500 average loss = 0.3082 (time: 3.94)\n",
      "\n",
      "Epoch 4 loss = 0.2837\n",
      "\n",
      "Example 1\n",
      "INPUT >> D B H A E A B A\n",
      "PRED >> E E H B\n",
      "TRUE >> E C I B F B C B\n",
      "[BLEU] 0.07\n",
      "\n",
      "Example 2\n",
      "INPUT >> G E H E E D\n",
      "PRED >> F F F F\n",
      "TRUE >> H F I F F E\n",
      "[BLEU] 0.21\n",
      "\n",
      "Example 3\n",
      "INPUT >> B B H B G\n",
      "PRED >> C C\n",
      "TRUE >> C C I C H\n",
      "[BLEU] 0.13\n",
      "\n",
      "Example 4\n",
      "INPUT >> F E A G A\n",
      "PRED >> F B B\n",
      "TRUE >> G F B H B\n",
      "[BLEU] 0.26\n",
      "\n",
      "Example 5\n",
      "INPUT >> F E B A D\n",
      "PRED >> F B E\n",
      "TRUE >> G F C B E\n",
      "[BLEU] 0.26\n",
      "\n",
      "Step 100 average loss = 0.2886 (time: 8.09)\n",
      "Step 200 average loss = 0.2499 (time: 3.85)\n",
      "Step 300 average loss = 0.2738 (time: 3.92)\n",
      "Step 400 average loss = 0.2812 (time: 3.96)\n",
      "Step 500 average loss = 0.2414 (time: 4.08)\n",
      "\n",
      "Epoch 5 loss = 0.2699\n",
      "\n",
      "Example 1\n",
      "INPUT >> F A C F G D A H\n",
      "PRED >> G G D D\n",
      "TRUE >> G B D G H E B I\n",
      "[BLEU] 0.08\n",
      "\n",
      "Example 2\n",
      "INPUT >> E A D H G G F H\n",
      "PRED >> F F E I\n",
      "TRUE >> F B E I H H G I\n",
      "[BLEU] 0.13\n",
      "\n",
      "Example 3\n",
      "INPUT >> G G A G C A B\n",
      "PRED >> H B B B\n",
      "TRUE >> H H B H D B C\n",
      "[BLEU] 0.17\n",
      "\n",
      "Example 4\n",
      "INPUT >> G D H B F\n",
      "PRED >> E I I\n",
      "TRUE >> H E I C G\n",
      "[BLEU] 0.23\n",
      "\n",
      "Example 5\n",
      "INPUT >> F H E\n",
      "PRED >> I I I\n",
      "TRUE >> G I F\n",
      "[BLEU] 0.23\n",
      "\n",
      "Step 100 average loss = 0.2542 (time: 7.94)\n",
      "Step 200 average loss = 0.2274 (time: 3.91)\n",
      "Step 300 average loss = 0.2537 (time: 3.99)\n",
      "Step 400 average loss = 0.2460 (time: 3.94)\n",
      "Step 500 average loss = 0.2609 (time: 4.11)\n",
      "\n",
      "Epoch 6 loss = 0.2588\n",
      "\n",
      "Example 1\n",
      "INPUT >> H B B F H G H A\n",
      "PRED >> I I I I\n",
      "TRUE >> I C C G I H I B\n",
      "[BLEU] 0.08\n",
      "\n",
      "Example 2\n",
      "INPUT >> F A C F G D A H\n",
      "PRED >> G G G D\n",
      "TRUE >> G B D G H E B I\n",
      "[BLEU] 0.08\n",
      "\n",
      "Example 3\n",
      "INPUT >> F E D E C F A\n",
      "PRED >> F F F E\n",
      "TRUE >> G F E F D G B\n",
      "[BLEU] 0.17\n",
      "\n",
      "Example 4\n",
      "INPUT >> F E F H E\n",
      "PRED >> G G F F\n",
      "TRUE >> G F G I F\n",
      "[BLEU] 0.30\n",
      "\n",
      "Example 5\n",
      "INPUT >> B F D E\n",
      "PRED >> G E E\n",
      "TRUE >> C G E F\n",
      "[BLEU] 0.32\n",
      "\n",
      "Step 100 average loss = 0.2397 (time: 8.08)\n",
      "Step 200 average loss = 0.2515 (time: 3.93)\n",
      "Step 300 average loss = 0.2402 (time: 4.40)\n",
      "Step 400 average loss = 0.2497 (time: 3.94)\n",
      "Step 500 average loss = 0.2373 (time: 3.99)\n",
      "\n",
      "Epoch 7 loss = 0.2482\n",
      "\n",
      "Example 1\n",
      "INPUT >> A C D F C F G C\n",
      "PRED >> B D E G\n",
      "TRUE >> B D E G D G H D\n",
      "[BLEU] 0.37\n",
      "\n",
      "Example 2\n",
      "INPUT >> F D G H F E G\n",
      "PRED >> G E I F\n",
      "TRUE >> G E H I G F H\n",
      "[BLEU] 0.18\n",
      "\n",
      "Example 3\n",
      "INPUT >> D H F E E C D\n",
      "PRED >> E E I F\n",
      "TRUE >> E I G F F D E\n",
      "[BLEU] 0.18\n",
      "\n",
      "Example 4\n",
      "INPUT >> H B C H A A\n",
      "PRED >> I D D B B\n",
      "TRUE >> I C D I B B\n",
      "[BLEU] 0.21\n",
      "\n",
      "Example 5\n",
      "INPUT >> H B G\n",
      "PRED >> I C C\n",
      "TRUE >> I C H\n",
      "[BLEU] 0.45\n",
      "\n",
      "Step 100 average loss = 0.2194 (time: 8.06)\n",
      "Step 200 average loss = 0.2325 (time: 3.88)\n",
      "Step 300 average loss = 0.2638 (time: 3.98)\n",
      "Step 400 average loss = 0.2600 (time: 4.56)\n",
      "Step 500 average loss = 0.2286 (time: 4.00)\n",
      "\n",
      "Epoch 8 loss = 0.2416\n",
      "\n",
      "Example 1\n",
      "INPUT >> D D E H E H\n",
      "PRED >> E E F F I\n",
      "TRUE >> E E F I F I\n",
      "[BLEU] 0.41\n",
      "\n",
      "Example 2\n",
      "INPUT >> C A F G E\n",
      "PRED >> D B B G\n",
      "TRUE >> D B G H F\n",
      "[BLEU] 0.33\n",
      "\n",
      "Example 3\n",
      "INPUT >> F F D E\n",
      "PRED >> G G E E\n",
      "TRUE >> G G E F\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 4\n",
      "INPUT >> D F F\n",
      "PRED >> G G G G\n",
      "TRUE >> E G G\n",
      "[BLEU] 0.32\n",
      "\n",
      "Example 5\n",
      "INPUT >> C D H\n",
      "PRED >> D E E I\n",
      "TRUE >> D E I\n",
      "[BLEU] 0.42\n",
      "\n",
      "Step 100 average loss = 0.2294 (time: 8.15)\n",
      "Step 200 average loss = 0.2128 (time: 3.88)\n",
      "Step 300 average loss = 0.2345 (time: 3.92)\n",
      "Step 400 average loss = 0.2204 (time: 3.97)\n",
      "Step 500 average loss = 0.2411 (time: 3.94)\n",
      "\n",
      "Epoch 9 loss = 0.2316\n",
      "\n",
      "Example 1\n",
      "INPUT >> D G C F\n",
      "PRED >> E E H D\n",
      "TRUE >> E H D G\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 2\n",
      "INPUT >> A A E C\n",
      "PRED >> B B B\n",
      "TRUE >> B B F D\n",
      "[BLEU] 0.32\n",
      "\n",
      "Example 3\n",
      "INPUT >> A G C B\n",
      "PRED >> B B D C\n",
      "TRUE >> B H D C\n",
      "[BLEU] 0.35\n",
      "\n",
      "Example 4\n",
      "INPUT >> B B A B\n",
      "PRED >> C C B C\n",
      "TRUE >> C C B C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H D E\n",
      "PRED >> I E E F\n",
      "TRUE >> I E F\n",
      "[BLEU] 0.42\n",
      "\n",
      "Step 100 average loss = 0.2608 (time: 7.97)\n",
      "Step 200 average loss = 0.2372 (time: 3.93)\n",
      "Step 300 average loss = 0.2330 (time: 3.88)\n",
      "Step 400 average loss = 0.1858 (time: 4.04)\n",
      "Step 500 average loss = 0.2202 (time: 3.91)\n",
      "\n",
      "Epoch 10 loss = 0.2236\n",
      "\n",
      "Example 1\n",
      "INPUT >> F B D F G G D\n",
      "PRED >> G C E G\n",
      "TRUE >> G C E G H H E\n",
      "[BLEU] 0.47\n",
      "\n",
      "Example 2\n",
      "INPUT >> A D H E\n",
      "PRED >> B E I F\n",
      "TRUE >> B E I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H B A E\n",
      "PRED >> I C B F\n",
      "TRUE >> I C B F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> A F H A\n",
      "PRED >> B G I B\n",
      "TRUE >> B G I B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B B C\n",
      "PRED >> C C C D\n",
      "TRUE >> C C D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.2460 (time: 8.25)\n",
      "Step 200 average loss = 0.2501 (time: 4.03)\n",
      "Step 300 average loss = 0.2073 (time: 4.03)\n",
      "Step 400 average loss = 0.2055 (time: 4.39)\n",
      "Step 500 average loss = 0.2045 (time: 4.15)\n",
      "\n",
      "Epoch 11 loss = 0.2154\n",
      "\n",
      "Example 1\n",
      "INPUT >> B C H H B C B G\n",
      "PRED >> C D I I C\n",
      "TRUE >> C D I I C D C H\n",
      "[BLEU] 0.55\n",
      "\n",
      "Example 2\n",
      "INPUT >> B H G G F\n",
      "PRED >> I H H G\n",
      "TRUE >> C I H H G\n",
      "[BLEU] 0.78\n",
      "\n",
      "Example 3\n",
      "INPUT >> E B D E\n",
      "PRED >> F C E F\n",
      "TRUE >> F C E F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D E A F\n",
      "PRED >> E F B G\n",
      "TRUE >> E F B G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> A G F D\n",
      "PRED >> B H G E E\n",
      "TRUE >> B H G E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Step 100 average loss = 0.2408 (time: 8.21)\n",
      "Step 200 average loss = 0.1845 (time: 4.10)\n",
      "Step 300 average loss = 0.1876 (time: 4.03)\n",
      "Step 400 average loss = 0.2286 (time: 4.16)\n",
      "Step 500 average loss = 0.1965 (time: 4.33)\n",
      "\n",
      "Epoch 12 loss = 0.2072\n",
      "\n",
      "Example 1\n",
      "INPUT >> B E E H G A D E\n",
      "PRED >> C F F I H B\n",
      "TRUE >> C F F I H B E F\n",
      "[BLEU] 0.72\n",
      "\n",
      "Example 2\n",
      "INPUT >> A E B A F F G\n",
      "PRED >> B F C B G\n",
      "TRUE >> B F C B G G H\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 3\n",
      "INPUT >> G E F D\n",
      "PRED >> H F G E E\n",
      "TRUE >> H F G E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 4\n",
      "INPUT >> D B B B\n",
      "PRED >> E C C C C\n",
      "TRUE >> E C C C\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 5\n",
      "INPUT >> D H H\n",
      "PRED >> E I I I\n",
      "TRUE >> E I I\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1818 (time: 8.07)\n",
      "Step 200 average loss = 0.1743 (time: 4.03)\n",
      "Step 300 average loss = 0.2454 (time: 3.99)\n",
      "Step 400 average loss = 0.2145 (time: 3.97)\n",
      "Step 500 average loss = 0.2119 (time: 4.04)\n",
      "\n",
      "Epoch 13 loss = 0.1995\n",
      "\n",
      "Example 1\n",
      "INPUT >> G C B H F B G\n",
      "PRED >> H D C I G\n",
      "TRUE >> H D C I G C H\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 2\n",
      "INPUT >> B E G D B A B\n",
      "PRED >> C F H E C B\n",
      "TRUE >> C F H E C B C\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 3\n",
      "INPUT >> C D H F E H\n",
      "PRED >> D E I G F\n",
      "TRUE >> D E I G F I\n",
      "[BLEU] 0.82\n",
      "\n",
      "Example 4\n",
      "INPUT >> C F C E F B\n",
      "PRED >> D G D F G\n",
      "TRUE >> D G D F G C\n",
      "[BLEU] 0.82\n",
      "\n",
      "Example 5\n",
      "INPUT >> G A F C G G\n",
      "PRED >> H B G D H H\n",
      "TRUE >> H B G D H H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1871 (time: 8.05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 average loss = 0.2145 (time: 4.07)\n",
      "Step 300 average loss = 0.1856 (time: 3.99)\n",
      "Step 400 average loss = 0.1942 (time: 4.04)\n",
      "Step 500 average loss = 0.2345 (time: 3.98)\n",
      "\n",
      "Epoch 14 loss = 0.1953\n",
      "\n",
      "Example 1\n",
      "INPUT >> B A F B B G E C\n",
      "PRED >> C B G C H\n",
      "TRUE >> C B G C C H F D\n",
      "[BLEU] 0.42\n",
      "\n",
      "Example 2\n",
      "INPUT >> H G C E H A G D\n",
      "PRED >> I H D F I B\n",
      "TRUE >> I H D F I B H E\n",
      "[BLEU] 0.72\n",
      "\n",
      "Example 3\n",
      "INPUT >> H E C H E F\n",
      "PRED >> I F D I\n",
      "TRUE >> I F D I F G\n",
      "[BLEU] 0.61\n",
      "\n",
      "Example 4\n",
      "INPUT >> C C A E H\n",
      "PRED >> D D B F I\n",
      "TRUE >> D D B F I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B F B D F\n",
      "PRED >> C G C E G\n",
      "TRUE >> C G C E G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.2006 (time: 8.00)\n",
      "Step 200 average loss = 0.1679 (time: 3.95)\n",
      "Step 300 average loss = 0.1682 (time: 3.93)\n",
      "Step 400 average loss = 0.1966 (time: 4.04)\n",
      "Step 500 average loss = 0.1952 (time: 4.15)\n",
      "\n",
      "Epoch 15 loss = 0.1894\n",
      "\n",
      "Example 1\n",
      "INPUT >> G C C H F H F E\n",
      "PRED >> H D D I G\n",
      "TRUE >> H D D I G I G F\n",
      "[BLEU] 0.55\n",
      "\n",
      "Example 2\n",
      "INPUT >> D B H A E A B A\n",
      "PRED >> E C I B F\n",
      "TRUE >> E C I B F B C B\n",
      "[BLEU] 0.55\n",
      "\n",
      "Example 3\n",
      "INPUT >> C G B H H\n",
      "PRED >> D H C I I I\n",
      "TRUE >> D H C I I\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 4\n",
      "INPUT >> C G E D A\n",
      "PRED >> D H F E B\n",
      "TRUE >> D H F E B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> E G D\n",
      "PRED >> F H E E\n",
      "TRUE >> F H E\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.2050 (time: 7.98)\n",
      "Step 200 average loss = 0.2248 (time: 4.02)\n",
      "Step 300 average loss = 0.1670 (time: 4.02)\n",
      "Step 400 average loss = 0.1745 (time: 4.03)\n",
      "Step 500 average loss = 0.1897 (time: 4.05)\n",
      "\n",
      "Epoch 16 loss = 0.1832\n",
      "\n",
      "Example 1\n",
      "INPUT >> H B B A C F H\n",
      "PRED >> I C C B D G\n",
      "TRUE >> I C C B D G I\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 2\n",
      "INPUT >> G C C H C B\n",
      "PRED >> H D D I D\n",
      "TRUE >> H D D I D C\n",
      "[BLEU] 0.82\n",
      "\n",
      "Example 3\n",
      "INPUT >> E E C E F F\n",
      "PRED >> F F D F G G\n",
      "TRUE >> F F D F G G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> G C F D\n",
      "PRED >> H D G E E\n",
      "TRUE >> H D G E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 5\n",
      "INPUT >> A F C\n",
      "PRED >> B G D D\n",
      "TRUE >> B G D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1894 (time: 8.06)\n",
      "Step 200 average loss = 0.2170 (time: 4.45)\n",
      "Step 300 average loss = 0.1992 (time: 4.14)\n",
      "Step 400 average loss = 0.1552 (time: 3.94)\n",
      "Step 500 average loss = 0.1676 (time: 3.94)\n",
      "\n",
      "Epoch 17 loss = 0.1825\n",
      "\n",
      "Example 1\n",
      "INPUT >> B G D D E\n",
      "PRED >> C H E E F\n",
      "TRUE >> C H E E F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G B H H\n",
      "PRED >> H C I I I\n",
      "TRUE >> H C I I\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 3\n",
      "INPUT >> C E G B\n",
      "PRED >> D F H C C\n",
      "TRUE >> D F H C\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 4\n",
      "INPUT >> E C C E\n",
      "PRED >> F D D F\n",
      "TRUE >> F D D F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> A F F\n",
      "PRED >> B G G G\n",
      "TRUE >> B G G\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1623 (time: 8.13)\n",
      "Step 200 average loss = 0.1473 (time: 3.92)\n",
      "Step 300 average loss = 0.1961 (time: 3.99)\n",
      "Step 400 average loss = 0.1861 (time: 3.91)\n",
      "Step 500 average loss = 0.2151 (time: 4.02)\n",
      "\n",
      "Epoch 18 loss = 0.1794\n",
      "\n",
      "Example 1\n",
      "INPUT >> G H G D\n",
      "PRED >> H I H E E\n",
      "TRUE >> H I H E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 2\n",
      "INPUT >> B H D\n",
      "PRED >> C I E E\n",
      "TRUE >> C I E\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 3\n",
      "INPUT >> G E D\n",
      "PRED >> H F E E\n",
      "TRUE >> H F E\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 4\n",
      "INPUT >> G B G\n",
      "PRED >> H C H H\n",
      "TRUE >> H C H\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 5\n",
      "INPUT >> H E E\n",
      "PRED >> I F F F\n",
      "TRUE >> I F F\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1658 (time: 7.93)\n",
      "Step 200 average loss = 0.1550 (time: 3.93)\n",
      "Step 300 average loss = 0.2055 (time: 3.98)\n",
      "Step 400 average loss = 0.1581 (time: 3.92)\n",
      "Step 500 average loss = 0.2101 (time: 3.88)\n",
      "\n",
      "Epoch 19 loss = 0.1778\n",
      "\n",
      "Example 1\n",
      "INPUT >> E E A D E E E G\n",
      "PRED >> F F B E\n",
      "TRUE >> F F B E F F F H\n",
      "[BLEU] 0.37\n",
      "\n",
      "Example 2\n",
      "INPUT >> C D A H E G H D\n",
      "PRED >> D E B I F\n",
      "TRUE >> D E B I F H I E\n",
      "[BLEU] 0.55\n",
      "\n",
      "Example 3\n",
      "INPUT >> C E A B H H B\n",
      "PRED >> D F B C I\n",
      "TRUE >> D F B C I I C\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 4\n",
      "INPUT >> C F F G H\n",
      "PRED >> D G G H I I\n",
      "TRUE >> D G G H I\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 5\n",
      "INPUT >> A H E\n",
      "PRED >> B I F F\n",
      "TRUE >> B I F\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1691 (time: 7.86)\n",
      "Step 200 average loss = 0.1966 (time: 3.90)\n",
      "Step 300 average loss = 0.1832 (time: 3.89)\n",
      "Step 400 average loss = 0.1738 (time: 3.93)\n",
      "Step 500 average loss = 0.2327 (time: 3.95)\n",
      "\n",
      "Epoch 20 loss = 0.1765\n",
      "\n",
      "Example 1\n",
      "INPUT >> E E A D E E E G\n",
      "PRED >> F F B E\n",
      "TRUE >> F F B E F F F H\n",
      "[BLEU] 0.37\n",
      "\n",
      "Example 2\n",
      "INPUT >> D F H B F D\n",
      "PRED >> E G I C G E\n",
      "TRUE >> E G I C G E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B C H H E B\n",
      "PRED >> C D I I\n",
      "TRUE >> C D I I F C\n",
      "[BLEU] 0.61\n",
      "\n",
      "Example 4\n",
      "INPUT >> B A D G G\n",
      "PRED >> C B E H H H\n",
      "TRUE >> C B E H H\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 5\n",
      "INPUT >> B E G\n",
      "PRED >> C F H H\n",
      "TRUE >> C F H\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1713 (time: 7.91)\n",
      "Step 200 average loss = 0.1768 (time: 3.97)\n",
      "Step 300 average loss = 0.1884 (time: 3.90)\n",
      "Step 400 average loss = 0.1957 (time: 3.94)\n",
      "Step 500 average loss = 0.1595 (time: 3.95)\n",
      "\n",
      "Epoch 21 loss = 0.1734\n",
      "\n",
      "Example 1\n",
      "INPUT >> E G H B A B D D\n",
      "PRED >> F H I C B C\n",
      "TRUE >> F H I C B C E E\n",
      "[BLEU] 0.72\n",
      "\n",
      "Example 2\n",
      "INPUT >> F B H D D B F\n",
      "PRED >> G C I E E C\n",
      "TRUE >> G C I E E C G\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 3\n",
      "INPUT >> F D A D F A\n",
      "PRED >> G E B E G B\n",
      "TRUE >> G E B E G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D H C\n",
      "PRED >> E I D D\n",
      "TRUE >> E I D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 5\n",
      "INPUT >> D E H\n",
      "PRED >> E F I I\n",
      "TRUE >> E F I\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1493 (time: 7.89)\n",
      "Step 200 average loss = 0.1919 (time: 3.85)\n",
      "Step 300 average loss = 0.1703 (time: 3.90)\n",
      "Step 400 average loss = 0.1559 (time: 3.88)\n",
      "Step 500 average loss = 0.1827 (time: 3.91)\n",
      "\n",
      "Epoch 22 loss = 0.1735\n",
      "\n",
      "Example 1\n",
      "INPUT >> H H H B B G B H\n",
      "PRED >> I I I C C\n",
      "TRUE >> I I I C C H C I\n",
      "[BLEU] 0.55\n",
      "\n",
      "Example 2\n",
      "INPUT >> G G F B G F H\n",
      "PRED >> H H G C H G\n",
      "TRUE >> H H G C H G I\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 3\n",
      "INPUT >> A H H A H H D\n",
      "PRED >> B I I B I\n",
      "TRUE >> B I I B I I E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 4\n",
      "INPUT >> C C A B\n",
      "PRED >> D D B C\n",
      "TRUE >> D D B C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> A G F D\n",
      "PRED >> B H G E E\n",
      "TRUE >> B H G E\n",
      "[BLEU] 0.67\n",
      "\n",
      "Step 100 average loss = 0.1983 (time: 7.98)\n",
      "Step 200 average loss = 0.2157 (time: 3.94)\n",
      "Step 300 average loss = 0.1506 (time: 4.02)\n",
      "Step 400 average loss = 0.1794 (time: 4.00)\n",
      "Step 500 average loss = 0.1540 (time: 4.10)\n",
      "\n",
      "Epoch 23 loss = 0.1693\n",
      "\n",
      "Example 1\n",
      "INPUT >> B A F B B G E C\n",
      "PRED >> C B G C C H\n",
      "TRUE >> C B G C C H F D\n",
      "[BLEU] 0.72\n",
      "\n",
      "Example 2\n",
      "INPUT >> D G H F C G D\n",
      "PRED >> E H I G D H E\n",
      "TRUE >> E H I G D H E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> A H H G G\n",
      "PRED >> B I I H H H\n",
      "TRUE >> B I I H H\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 4\n",
      "INPUT >> D G B F C\n",
      "PRED >> E H C G D\n",
      "TRUE >> E H C G D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> A G C B\n",
      "PRED >> B H D C C\n",
      "TRUE >> B H D C\n",
      "[BLEU] 0.67\n",
      "\n",
      "Step 100 average loss = 0.1606 (time: 8.09)\n",
      "Step 200 average loss = 0.1547 (time: 3.99)\n",
      "Step 300 average loss = 0.1642 (time: 4.43)\n",
      "Step 400 average loss = 0.1520 (time: 4.19)\n",
      "Step 500 average loss = 0.1598 (time: 4.28)\n",
      "\n",
      "Epoch 24 loss = 0.1680\n",
      "\n",
      "Example 1\n",
      "INPUT >> E B E C C A A E\n",
      "PRED >> F C F D D B\n",
      "TRUE >> F C F D D B B F\n",
      "[BLEU] 0.72\n",
      "\n",
      "Example 2\n",
      "INPUT >> C D A H E G H D\n",
      "PRED >> D E B I F H I\n",
      "TRUE >> D E B I F H I E\n",
      "[BLEU] 0.87\n",
      "\n",
      "Example 3\n",
      "INPUT >> G C B H F B G\n",
      "PRED >> H D C I G C H\n",
      "TRUE >> H D C I G C H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E G D E F C\n",
      "PRED >> F H E F G D\n",
      "TRUE >> F H E F G D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B F A G F\n",
      "PRED >> C G B H G\n",
      "TRUE >> C G B H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1861 (time: 8.51)\n",
      "Step 200 average loss = 0.1705 (time: 4.30)\n",
      "Step 300 average loss = 0.1746 (time: 4.29)\n",
      "Step 400 average loss = 0.1373 (time: 4.28)\n",
      "Step 500 average loss = 0.1724 (time: 4.05)\n",
      "\n",
      "Epoch 25 loss = 0.1682\n",
      "\n",
      "Example 1\n",
      "INPUT >> A E E B D C E\n",
      "PRED >> B F F C E D\n",
      "TRUE >> B F F C E D F\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 2\n",
      "INPUT >> H D G B G G\n",
      "PRED >> I E H C H H H\n",
      "TRUE >> I E H C H H\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 3\n",
      "INPUT >> D D F C E C\n",
      "PRED >> E E G D F D F\n",
      "TRUE >> E E G D F D\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 4\n",
      "INPUT >> H E C H E F\n",
      "PRED >> I F D I F G\n",
      "TRUE >> I F D I F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B H H B H\n",
      "PRED >> C I I C I\n",
      "TRUE >> C I I C I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1446 (time: 8.00)\n",
      "Step 200 average loss = 0.1930 (time: 4.11)\n",
      "Step 300 average loss = 0.1734 (time: 4.01)\n",
      "Step 400 average loss = 0.1765 (time: 3.98)\n",
      "Step 500 average loss = 0.1586 (time: 4.02)\n",
      "\n",
      "Epoch 26 loss = 0.1673\n",
      "\n",
      "Example 1\n",
      "INPUT >> E D B C D C E H\n",
      "PRED >> F E C D E D F I\n",
      "TRUE >> F E C D E D F I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> B A C E B B H G\n",
      "PRED >> C B D F C I H\n",
      "TRUE >> C B D F C C I H\n",
      "[BLEU] 0.69\n",
      "\n",
      "Example 3\n",
      "INPUT >> G A E F C E\n",
      "PRED >> H B F G D F\n",
      "TRUE >> H B F G D F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D H G C G F\n",
      "PRED >> E I H D H G\n",
      "TRUE >> E I H D H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D C A H\n",
      "PRED >> E D B I\n",
      "TRUE >> E D B I\n",
      "[BLEU] 1.00\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 average loss = 0.1585 (time: 8.05)\n",
      "Step 200 average loss = 0.1388 (time: 4.11)\n",
      "Step 300 average loss = 0.1525 (time: 4.46)\n",
      "Step 400 average loss = 0.1940 (time: 4.43)\n",
      "Step 500 average loss = 0.1522 (time: 4.05)\n",
      "\n",
      "Epoch 27 loss = 0.1663\n",
      "\n",
      "Example 1\n",
      "INPUT >> A C D F C F G C\n",
      "PRED >> B D E G D G H\n",
      "TRUE >> B D E G D G H D\n",
      "[BLEU] 0.87\n",
      "\n",
      "Example 2\n",
      "INPUT >> F E D E G F F\n",
      "PRED >> G F E F H G G\n",
      "TRUE >> G F E F H G G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H B B C F H F\n",
      "PRED >> I C C D G I G\n",
      "TRUE >> I C C D G I G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E E H E H D\n",
      "PRED >> F F I F I E\n",
      "TRUE >> F F I F I E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D F H B F D\n",
      "PRED >> E G I C G E\n",
      "TRUE >> E G I C G E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.2079 (time: 8.57)\n",
      "Step 200 average loss = 0.1257 (time: 3.97)\n",
      "Step 300 average loss = 0.1584 (time: 3.98)\n",
      "Step 400 average loss = 0.1778 (time: 4.36)\n",
      "Step 500 average loss = 0.2008 (time: 4.41)\n",
      "\n",
      "Epoch 28 loss = 0.1653\n",
      "\n",
      "Example 1\n",
      "INPUT >> C H A D F A\n",
      "PRED >> D I B E G B\n",
      "TRUE >> D I B E G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H G C B G D\n",
      "PRED >> I H D C H E E\n",
      "TRUE >> I H D C H E\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 3\n",
      "INPUT >> B B C B D\n",
      "PRED >> C C D C E\n",
      "TRUE >> C C D C E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D B H\n",
      "PRED >> E C I\n",
      "TRUE >> E C I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> G D G\n",
      "PRED >> H E H\n",
      "TRUE >> H E H\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1650 (time: 8.49)\n",
      "Step 200 average loss = 0.1657 (time: 4.03)\n",
      "Step 300 average loss = 0.1799 (time: 4.23)\n",
      "Step 400 average loss = 0.1903 (time: 4.51)\n",
      "Step 500 average loss = 0.1269 (time: 4.16)\n",
      "\n",
      "Epoch 29 loss = 0.1631\n",
      "\n",
      "Example 1\n",
      "INPUT >> D C G C B F A E\n",
      "PRED >> E D H D C G B F\n",
      "TRUE >> E D H D C G B F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C A G G E G\n",
      "PRED >> D B H H F H\n",
      "TRUE >> D B H H F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> F G A A F\n",
      "PRED >> G H B B G\n",
      "TRUE >> G H B B G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> F E F H E\n",
      "PRED >> G F G I F\n",
      "TRUE >> G F G I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B E G\n",
      "PRED >> C F H H\n",
      "TRUE >> C F H\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1801 (time: 8.06)\n",
      "Step 200 average loss = 0.1934 (time: 4.14)\n",
      "Step 300 average loss = 0.1958 (time: 3.97)\n",
      "Step 400 average loss = 0.1657 (time: 3.99)\n",
      "Step 500 average loss = 0.1442 (time: 4.06)\n",
      "\n",
      "Epoch 30 loss = 0.1614\n",
      "\n",
      "Example 1\n",
      "INPUT >> G A F B B A B H\n",
      "PRED >> H B G C C B I\n",
      "TRUE >> H B G C C B C I\n",
      "[BLEU] 0.73\n",
      "\n",
      "Example 2\n",
      "INPUT >> F F A A D G D\n",
      "PRED >> G G B B E H\n",
      "TRUE >> G G B B E H E\n",
      "[BLEU] 0.85\n",
      "\n",
      "Example 3\n",
      "INPUT >> B C A A\n",
      "PRED >> C D B B B\n",
      "TRUE >> C D B B\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 4\n",
      "INPUT >> B E C\n",
      "PRED >> C F D\n",
      "TRUE >> C F D\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> D H C\n",
      "PRED >> E I D D\n",
      "TRUE >> E I D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1554 (time: 9.07)\n",
      "Step 200 average loss = 0.1997 (time: 4.07)\n",
      "Step 300 average loss = 0.1615 (time: 4.20)\n",
      "Step 400 average loss = 0.1638 (time: 4.15)\n",
      "Step 500 average loss = 0.1483 (time: 4.04)\n",
      "\n",
      "Epoch 31 loss = 0.1612\n",
      "\n",
      "Example 1\n",
      "INPUT >> H C H H E A\n",
      "PRED >> I D I I F B\n",
      "TRUE >> I D I I F B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G F D E G B\n",
      "PRED >> H G E F H C C\n",
      "TRUE >> H G E F H C\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 3\n",
      "INPUT >> D B D D F\n",
      "PRED >> E C E E G\n",
      "TRUE >> E C E E G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H C G\n",
      "PRED >> I D H\n",
      "TRUE >> I D H\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> E G D\n",
      "PRED >> F H E E\n",
      "TRUE >> F H E\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1660 (time: 8.00)\n",
      "Step 200 average loss = 0.1331 (time: 4.17)\n",
      "Step 300 average loss = 0.1909 (time: 4.01)\n",
      "Step 400 average loss = 0.1723 (time: 3.92)\n",
      "Step 500 average loss = 0.1698 (time: 4.00)\n",
      "\n",
      "Epoch 32 loss = 0.1622\n",
      "\n",
      "Example 1\n",
      "INPUT >> H B F E G G D\n",
      "PRED >> I C G F H H E\n",
      "TRUE >> I C G F H H E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H H F H F H B\n",
      "PRED >> I I G I G I C\n",
      "TRUE >> I I G I G I C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H C H H E A\n",
      "PRED >> I D I I F B\n",
      "TRUE >> I D I I F B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> F C D C F E\n",
      "PRED >> G D E D G F\n",
      "TRUE >> G D E D G F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> A D G D G\n",
      "PRED >> B E H E H\n",
      "TRUE >> B E H E H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1652 (time: 8.00)\n",
      "Step 200 average loss = 0.1489 (time: 3.91)\n",
      "Step 300 average loss = 0.1282 (time: 4.00)\n",
      "Step 400 average loss = 0.1700 (time: 4.09)\n",
      "Step 500 average loss = 0.1328 (time: 4.07)\n",
      "\n",
      "Epoch 33 loss = 0.1600\n",
      "\n",
      "Example 1\n",
      "INPUT >> G D E B D B H D\n",
      "PRED >> H E F C E C I E\n",
      "TRUE >> H E F C E C I E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> B C C\n",
      "PRED >> C D D D\n",
      "TRUE >> C D D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 3\n",
      "INPUT >> D G E\n",
      "PRED >> E H F F\n",
      "TRUE >> E H F\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 4\n",
      "INPUT >> A H H\n",
      "PRED >> B I I I\n",
      "TRUE >> B I I\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 5\n",
      "INPUT >> G E D\n",
      "PRED >> H F E E\n",
      "TRUE >> H F E\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1778 (time: 8.10)\n",
      "Step 200 average loss = 0.1384 (time: 4.01)\n",
      "Step 300 average loss = 0.1512 (time: 4.02)\n",
      "Step 400 average loss = 0.1465 (time: 4.02)\n",
      "Step 500 average loss = 0.1464 (time: 3.94)\n",
      "\n",
      "Epoch 34 loss = 0.1611\n",
      "\n",
      "Example 1\n",
      "INPUT >> H F F E E E F G\n",
      "PRED >> I G G F F G H\n",
      "TRUE >> I G G F F F G H\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 2\n",
      "INPUT >> B E D A C B F\n",
      "PRED >> C F E B D C G\n",
      "TRUE >> C F E B D C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B G D D E\n",
      "PRED >> C H E E F\n",
      "TRUE >> C H E E F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H A A\n",
      "PRED >> I B B B\n",
      "TRUE >> I B B\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 5\n",
      "INPUT >> H E A\n",
      "PRED >> I F B\n",
      "TRUE >> I F B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1296 (time: 8.82)\n",
      "Step 200 average loss = 0.1440 (time: 4.44)\n",
      "Step 300 average loss = 0.1575 (time: 4.49)\n",
      "Step 400 average loss = 0.1458 (time: 4.27)\n",
      "Step 500 average loss = 0.1303 (time: 4.22)\n",
      "\n",
      "Epoch 35 loss = 0.1580\n",
      "\n",
      "Example 1\n",
      "INPUT >> A D F C B F E G\n",
      "PRED >> B E G D C G F H\n",
      "TRUE >> B E G D C G F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H D G B G G\n",
      "PRED >> I E H C H H H\n",
      "TRUE >> I E H C H H\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 3\n",
      "INPUT >> H A B E D F\n",
      "PRED >> I B C F E G\n",
      "TRUE >> I B C F E G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C B G G E H\n",
      "PRED >> D C H H F I\n",
      "TRUE >> D C H H F I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> F G B A\n",
      "PRED >> G H C B\n",
      "TRUE >> G H C B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1449 (time: 9.28)\n",
      "Step 200 average loss = 0.1955 (time: 5.14)\n",
      "Step 300 average loss = 0.1498 (time: 4.67)\n",
      "Step 400 average loss = 0.1674 (time: 4.18)\n",
      "Step 500 average loss = 0.1919 (time: 3.90)\n",
      "\n",
      "Epoch 36 loss = 0.1593\n",
      "\n",
      "Example 1\n",
      "INPUT >> G C A A F F G A\n",
      "PRED >> H D B B G H B\n",
      "TRUE >> H D B B G G H B\n",
      "[BLEU] 0.69\n",
      "\n",
      "Example 2\n",
      "INPUT >> D A B H G E F\n",
      "PRED >> E B C I H F G\n",
      "TRUE >> E B C I H F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> G C B B E B\n",
      "PRED >> H D C F C F C\n",
      "TRUE >> H D C C F C\n",
      "[BLEU] 0.41\n",
      "\n",
      "Example 4\n",
      "INPUT >> H B D F\n",
      "PRED >> I C E G\n",
      "TRUE >> I C E G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H D E\n",
      "PRED >> I E F\n",
      "TRUE >> I E F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1373 (time: 7.90)\n",
      "Step 200 average loss = 0.1285 (time: 3.96)\n",
      "Step 300 average loss = 0.1348 (time: 3.93)\n",
      "Step 400 average loss = 0.1333 (time: 4.00)\n",
      "Step 500 average loss = 0.2053 (time: 3.95)\n",
      "\n",
      "Epoch 37 loss = 0.1581\n",
      "\n",
      "Example 1\n",
      "INPUT >> D H B A A F A F\n",
      "PRED >> E I C B B G B\n",
      "TRUE >> E I C B B G B G\n",
      "[BLEU] 0.87\n",
      "\n",
      "Example 2\n",
      "INPUT >> D A B H G E F\n",
      "PRED >> E B C I H F G\n",
      "TRUE >> E B C I H F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> G D D D D D\n",
      "PRED >> H E E E E E\n",
      "TRUE >> H E E E E E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B H C D D\n",
      "PRED >> C I D E E E\n",
      "TRUE >> C I D E E\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 5\n",
      "INPUT >> H H B E\n",
      "PRED >> I I C F\n",
      "TRUE >> I I C F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1859 (time: 8.18)\n",
      "Step 200 average loss = 0.1544 (time: 4.21)\n",
      "Step 300 average loss = 0.1337 (time: 4.10)\n",
      "Step 400 average loss = 0.1267 (time: 4.14)\n",
      "Step 500 average loss = 0.1497 (time: 3.95)\n",
      "\n",
      "Epoch 38 loss = 0.1567\n",
      "\n",
      "Example 1\n",
      "INPUT >> H E C D H\n",
      "PRED >> I F D E I\n",
      "TRUE >> I F D E I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C F F G H\n",
      "PRED >> D G G H I\n",
      "TRUE >> D G G H I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> D C B D\n",
      "PRED >> E D C E\n",
      "TRUE >> E D C E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H C G\n",
      "PRED >> I D H\n",
      "TRUE >> I D H\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> B G B\n",
      "PRED >> C H C\n",
      "TRUE >> C H C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1452 (time: 8.41)\n",
      "Step 200 average loss = 0.1485 (time: 4.12)\n",
      "Step 300 average loss = 0.1622 (time: 4.20)\n",
      "Step 400 average loss = 0.1394 (time: 4.12)\n",
      "Step 500 average loss = 0.2061 (time: 4.03)\n",
      "\n",
      "Epoch 39 loss = 0.1560\n",
      "\n",
      "Example 1\n",
      "INPUT >> A D F C B F E G\n",
      "PRED >> B E G D C G F H\n",
      "TRUE >> B E G D C G F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G D F C H B C C\n",
      "PRED >> H E G D I C D D\n",
      "TRUE >> H E G D I C D D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E E C E F F\n",
      "PRED >> F F D F G G G\n",
      "TRUE >> F F D F G G\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 4\n",
      "INPUT >> C B F H H\n",
      "PRED >> D C G I I I\n",
      "TRUE >> D C G I I\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 5\n",
      "INPUT >> E A E\n",
      "PRED >> F B F\n",
      "TRUE >> F B F\n",
      "[BLEU] 0.84\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 average loss = 0.1537 (time: 8.95)\n",
      "Step 200 average loss = 0.1810 (time: 4.44)\n",
      "Step 300 average loss = 0.1517 (time: 4.19)\n",
      "Step 400 average loss = 0.1575 (time: 4.28)\n",
      "Step 500 average loss = 0.1626 (time: 4.22)\n",
      "\n",
      "Epoch 40 loss = 0.1567\n",
      "\n",
      "Example 1\n",
      "INPUT >> F H G E A D E\n",
      "PRED >> G I H F B E F\n",
      "TRUE >> G I H F B E F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C H A D F A\n",
      "PRED >> D I B E G B\n",
      "TRUE >> D I B E G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> A C A C E B\n",
      "PRED >> B D B D F C\n",
      "TRUE >> B D B D F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E A G C E B\n",
      "PRED >> F B H D F C\n",
      "TRUE >> F B H D F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H C E G F\n",
      "PRED >> I D F H G\n",
      "TRUE >> I D F H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1706 (time: 8.46)\n",
      "Step 200 average loss = 0.1736 (time: 4.28)\n",
      "Step 300 average loss = 0.1368 (time: 4.44)\n",
      "Step 400 average loss = 0.1639 (time: 3.91)\n",
      "Step 500 average loss = 0.1671 (time: 3.83)\n",
      "\n",
      "Epoch 41 loss = 0.1562\n",
      "\n",
      "Example 1\n",
      "INPUT >> E G F F A A\n",
      "PRED >> F H G G B B\n",
      "TRUE >> F H G G B B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> F G G G H E\n",
      "PRED >> G H H H I F\n",
      "TRUE >> G H H H I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H E C H E F\n",
      "PRED >> I F D I F G\n",
      "TRUE >> I F D I F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H A D B D\n",
      "PRED >> I B E C E\n",
      "TRUE >> I B E C E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> G H C E A\n",
      "PRED >> H I D F B\n",
      "TRUE >> H I D F B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1625 (time: 7.91)\n",
      "Step 200 average loss = 0.1643 (time: 3.98)\n",
      "Step 300 average loss = 0.1784 (time: 3.94)\n",
      "Step 400 average loss = 0.1741 (time: 4.13)\n",
      "Step 500 average loss = 0.1399 (time: 4.60)\n",
      "\n",
      "Epoch 42 loss = 0.1547\n",
      "\n",
      "Example 1\n",
      "INPUT >> D G F E D B C D\n",
      "PRED >> E H G F E C D E C\n",
      "TRUE >> E H G F E C D E\n",
      "[BLEU] 0.86\n",
      "\n",
      "Example 2\n",
      "INPUT >> D E F G H H H\n",
      "PRED >> E F G H I I I\n",
      "TRUE >> E F G H I I I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B F A D D C H\n",
      "PRED >> C G B E E D I\n",
      "TRUE >> C G B E E D I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B C B C C\n",
      "PRED >> C D C D D\n",
      "TRUE >> C D C D D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D H H\n",
      "PRED >> E I I\n",
      "TRUE >> E I I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1708 (time: 8.80)\n",
      "Step 200 average loss = 0.1730 (time: 4.07)\n",
      "Step 300 average loss = 0.1462 (time: 4.28)\n",
      "Step 400 average loss = 0.1533 (time: 4.47)\n",
      "Step 500 average loss = 0.1460 (time: 4.30)\n",
      "\n",
      "Epoch 43 loss = 0.1558\n",
      "\n",
      "Example 1\n",
      "INPUT >> E D F C C H H\n",
      "PRED >> F E G D D I I I\n",
      "TRUE >> F E G D D I I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 2\n",
      "INPUT >> D H G G E B\n",
      "PRED >> E I H H F C\n",
      "TRUE >> E I H H F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E H D C A\n",
      "PRED >> F I E D B\n",
      "TRUE >> F I E D B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> G D B G\n",
      "PRED >> H E C H\n",
      "TRUE >> H E C H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D H D\n",
      "PRED >> E I E\n",
      "TRUE >> E I E\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1544 (time: 8.83)\n",
      "Step 200 average loss = 0.1486 (time: 4.60)\n",
      "Step 300 average loss = 0.1700 (time: 4.20)\n",
      "Step 400 average loss = 0.1369 (time: 4.02)\n",
      "Step 500 average loss = 0.1571 (time: 4.08)\n",
      "\n",
      "Epoch 44 loss = 0.1553\n",
      "\n",
      "Example 1\n",
      "INPUT >> H G C E H A G D\n",
      "PRED >> I H D F I B H E\n",
      "TRUE >> I H D F I B H E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> B F C C H E D\n",
      "PRED >> C G D D I F E\n",
      "TRUE >> C G D D I F E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> C G B B G C B\n",
      "PRED >> D H C C H D C\n",
      "TRUE >> D H C C H D C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C G H E D E D\n",
      "PRED >> D H I F E F E\n",
      "TRUE >> D H I F E F E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H E H\n",
      "PRED >> I F I\n",
      "TRUE >> I F I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1404 (time: 8.02)\n",
      "Step 200 average loss = 0.1593 (time: 4.02)\n",
      "Step 300 average loss = 0.1574 (time: 4.07)\n",
      "Step 400 average loss = 0.1583 (time: 4.03)\n",
      "Step 500 average loss = 0.1331 (time: 4.00)\n",
      "\n",
      "Epoch 45 loss = 0.1545\n",
      "\n",
      "Example 1\n",
      "INPUT >> H B C H A A\n",
      "PRED >> I C D I B B\n",
      "TRUE >> I C D I B B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C E C H H\n",
      "PRED >> D F D I I I\n",
      "TRUE >> D F D I I\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 3\n",
      "INPUT >> E B H D G\n",
      "PRED >> F C I E H\n",
      "TRUE >> F C I E H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D G C G\n",
      "PRED >> E H D H\n",
      "TRUE >> E H D H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> G C G C\n",
      "PRED >> H D H D\n",
      "TRUE >> H D H D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1905 (time: 8.57)\n",
      "Step 200 average loss = 0.1285 (time: 4.33)\n",
      "Step 300 average loss = 0.2067 (time: 4.10)\n",
      "Step 400 average loss = 0.1755 (time: 4.00)\n",
      "Step 500 average loss = 0.1646 (time: 3.90)\n",
      "\n",
      "Epoch 46 loss = 0.1544\n",
      "\n",
      "Example 1\n",
      "INPUT >> H H H B B G B H\n",
      "PRED >> I I I C C C H I\n",
      "TRUE >> I I I C C H C I\n",
      "[BLEU] 0.66\n",
      "\n",
      "Example 2\n",
      "INPUT >> E H F H C B A\n",
      "PRED >> F I G I D C B B\n",
      "TRUE >> F I G I D C B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 3\n",
      "INPUT >> A D G A B B\n",
      "PRED >> B E H B C C C\n",
      "TRUE >> B E H B C C\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 4\n",
      "INPUT >> H H C D G F\n",
      "PRED >> I I D E H G\n",
      "TRUE >> I I D E H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> E C F G G F\n",
      "PRED >> F D G H H G\n",
      "TRUE >> F D G H H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1278 (time: 7.95)\n",
      "Step 200 average loss = 0.1705 (time: 4.05)\n",
      "Step 300 average loss = 0.1500 (time: 3.97)\n",
      "Step 400 average loss = 0.1254 (time: 3.88)\n",
      "Step 500 average loss = 0.1474 (time: 3.98)\n",
      "\n",
      "Epoch 47 loss = 0.1539\n",
      "\n",
      "Example 1\n",
      "INPUT >> B F C G A B B\n",
      "PRED >> C G D H B C C C\n",
      "TRUE >> C G D H B C C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 2\n",
      "INPUT >> A C A C E B\n",
      "PRED >> B D B D F C\n",
      "TRUE >> B D B D F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E D G E\n",
      "PRED >> F E H F\n",
      "TRUE >> F E H F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> A H H\n",
      "PRED >> B I I\n",
      "TRUE >> B I I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> D F E\n",
      "PRED >> E G F\n",
      "TRUE >> E G F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1794 (time: 7.93)\n",
      "Step 200 average loss = 0.1894 (time: 4.01)\n",
      "Step 300 average loss = 0.1278 (time: 4.04)\n",
      "Step 400 average loss = 0.1688 (time: 4.29)\n",
      "Step 500 average loss = 0.1553 (time: 4.15)\n",
      "\n",
      "Epoch 48 loss = 0.1542\n",
      "\n",
      "Example 1\n",
      "INPUT >> G D E D G C H B\n",
      "PRED >> H E F E H D I C\n",
      "TRUE >> H E F E H D I C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> D A B H G E F\n",
      "PRED >> E B C I H F G\n",
      "TRUE >> E B C I H F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> F E C E D\n",
      "PRED >> G F D F E\n",
      "TRUE >> G F D F E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B H C D D\n",
      "PRED >> C I D E E\n",
      "TRUE >> C I D E E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H A A\n",
      "PRED >> I B B\n",
      "TRUE >> I B B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1614 (time: 8.31)\n",
      "Step 200 average loss = 0.1940 (time: 4.16)\n",
      "Step 300 average loss = 0.1571 (time: 4.09)\n",
      "Step 400 average loss = 0.1682 (time: 4.10)\n",
      "Step 500 average loss = 0.1536 (time: 4.41)\n",
      "\n",
      "Epoch 49 loss = 0.1549\n",
      "\n",
      "Example 1\n",
      "INPUT >> A D F C B F E G\n",
      "PRED >> B E G D C G F H\n",
      "TRUE >> B E G D C G F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G A F B B A B H\n",
      "PRED >> H B G C C B G\n",
      "TRUE >> H B G C C B C I\n",
      "[BLEU] 0.70\n",
      "\n",
      "Example 3\n",
      "INPUT >> C A G G E G\n",
      "PRED >> D B H H F H\n",
      "TRUE >> D B H H F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E H F G E\n",
      "PRED >> F I G H F\n",
      "TRUE >> F I G H F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> G H G C A\n",
      "PRED >> H I H D B\n",
      "TRUE >> H I H D B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1416 (time: 8.24)\n",
      "Step 200 average loss = 0.1626 (time: 3.99)\n",
      "Step 300 average loss = 0.1489 (time: 4.16)\n",
      "Step 400 average loss = 0.1675 (time: 4.05)\n",
      "Step 500 average loss = 0.1462 (time: 4.21)\n",
      "\n",
      "Epoch 50 loss = 0.1549\n",
      "\n",
      "Example 1\n",
      "INPUT >> C H E H F H\n",
      "PRED >> D I F I G I\n",
      "TRUE >> D I F I G I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> F D A D F A\n",
      "PRED >> G E B E G B\n",
      "TRUE >> G E B E G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> A F A A H\n",
      "PRED >> B G B B I\n",
      "TRUE >> B G B B I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D G C G\n",
      "PRED >> E H D H\n",
      "TRUE >> E H D H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H D E\n",
      "PRED >> I E F\n",
      "TRUE >> I E F\n",
      "[BLEU] 0.84\n",
      "\n",
      "\n",
      "Grand average loss = 0.1890\n",
      "\n",
      "Average BLEU score over 50 examples is 0.9548\n",
      "CPU times: user 2h 51min 30s, sys: 1min 20s, total: 2h 52min 51s\n",
      "Wall time: 20min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s2s = Seq2Seq(indexer, trainPairs, trainLens, testPairs, testLens, \n",
    "              batchSize=5, hiddenSize=100,\n",
    "              nLayers=2, dropout=0.3, residual=True, \n",
    "              lr=1e-5, enforcingRatio=0.8, clip=20.0,\n",
    "              resultSavePath='toy/results.txt')\n",
    "s2s.train(nEpochs=50, epochSize=600, printEvery=100)\n",
    "s2s.evaluate(nBatches=10, saveResults=True)\n",
    "torch.save(s2s, 'toy/seq2seq.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APPENDIX. SCRIPTIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', help=\"Path to formatted input to Seq2Seq. See Seq2Seq documentation.\",\n",
    "                        type=str,\n",
    "                        default='toy/toy_formatted.p')\n",
    "    parser.add_argument('--model_save_path', help=\"Path to saved Seq2Seq model.\",\n",
    "                        type=str,\n",
    "                        default='toy/seq2seq.ckpt')\n",
    "    parser.add_argument('--result_save_path', help=\"Path to save results.\",\n",
    "                        type=str, default='toy/results.txt')\n",
    "    parser.add_argument('--clear_prev_result', help=\"Delete previously output results.\",\n",
    "                        type=bool, default=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=5)\n",
    "    parser.add_argument('--hidden_size', type=int, default=10)\n",
    "    parser.add_argument('--n_layers', type=int, default=2)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--residual', type=bool, default=True)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--enforce_ratio', type=float, default=0.5)\n",
    "    parser.add_argument('--clip', type=float, default=5.0)\n",
    "    parser.add_argument('--n_epochs', type=int, default=1)\n",
    "    parser.add_argument('--epoch_size', type=int, default=10)\n",
    "    parser.add_argument('--print_every', type=int, default=5)\n",
    "    parser.add_argument('--n_eval_batches', type=int, default=10)\n",
    "    parser.add_argument('--max_decoding_length', type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    option = raw_input(\"OPTIONS: r(etrain)/c(ontinue).\\nr: Retrain new model\\nc: Continue training of saved model.\\n\")\n",
    "    if option=='r':\n",
    "        if not os.path.exists(args.data_path):\n",
    "            dataBuildMsg = \"\"\"\n",
    "                Data does not exist. Please make it in the following format:\\n\n",
    "                indexer: Indexer object which has files loaded (each is a file with space-separated words as lines).\n",
    "                trainPairs, testPairs: each is a list of pairs of word index list.\n",
    "                trainLens, testLens: each is a list of pairs of length of word index list.\\n\n",
    "                The order is indexer, trainPairs, trainLens, testPairs, testLens. Pickle it with dill.\\n\n",
    "            \"\"\"\n",
    "            raise Exception(dataBuildMsg)\n",
    "        else:\n",
    "            if os.path.exists(args.model_save_path):\n",
    "                option = raw_input(\"Model exists. Hit c(ontinue) to overwrite it, (q)uit to quit.\\n\")\n",
    "                if option=='q':\n",
    "                    exit(0)\n",
    "            indexer, trainPairs, trainLens, testPairs, testLens = dill.load(open(args.data_path, 'rb'))\n",
    "            model = Seq2Seq(indexer, trainPairs, trainLens, testPairs, testLens,\n",
    "                            args.batch_size, args.hidden_size, \n",
    "                            args.n_layers, args.dropout, args.residual,\n",
    "                            args.lr, args.enforce_ratio, args.clip,\n",
    "                            args.result_save_path)\n",
    "    elif option=='c':\n",
    "        model = torch.load(args.model_save_path)\n",
    "    else:\n",
    "        raise Exception(\"Eneter either r/c.\")\n",
    "        exit(1)\n",
    "    \n",
    "    if args.clear_prev_result and os.path.exists(args.result_save_path):\n",
    "        os.remove(args.result_save_path)\n",
    "    \n",
    "    model.train(args.n_epochs, args.epoch_size, args.print_every)\n",
    "    model.evaluate(args.n_eval_batches)\n",
    "    torch.save(model, args.model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
