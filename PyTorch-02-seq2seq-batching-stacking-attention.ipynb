{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Attention Residual Seq2Seq\n",
    "\n",
    "* Task: toy \"translation\" task --- translating a list of letters (from A to H) to the next-letter-list (e.g. ['A', 'B', 'C'] translates as ['B', 'C', 'D'].\n",
    "* Type: Bahdanau et al. (2015); Prakash et al. (2016). Bahdanau/15 attention, . Clear-to-the-boot step-by-step demo.\n",
    "* PyTorch Version: 0.3.1\n",
    "\n",
    "**NB:** the code should run _much_ faster on your machine. The printed results are done while multiple other things are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from nltk.translate.bleu_score import sentence_bleu as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "SMOOTH = SmoothingFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \"\"\"Word <-> Index mapper.\"\"\"\n",
    "\n",
    "    def __init__(self, specialTokenList=None):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.wordSet = set()\n",
    "        self.size = 0\n",
    "        if specialTokenList is not None:\n",
    "            assert type(specialTokenList)==list\n",
    "            for token in specialTokenList:\n",
    "                self.get_index(token, add=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"The indexer currently has %d words\" % self.size\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index] if index<self.size else 'UNK'\n",
    "\n",
    "    def get_index(self, word, add=True):\n",
    "        if add and word not in self.wordSet:\n",
    "            self.word2index[word] = self.size\n",
    "            self.index2word[self.size] = word\n",
    "            self.wordSet.add(word)\n",
    "            self.size += 1\n",
    "        return self.word2index[word] if word in self.wordSet else self.word2index['UNK']\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.wordSet\n",
    "\n",
    "    def add_sentence(self, sentence, returnIndices=True):\n",
    "        indices = [self.get_index(word, add=True) for word in sentence.split()]\n",
    "        return (indices,len(indices)) if returnIndices else None\n",
    "\n",
    "    def add_document(self, docPath, returnIndices=True):\n",
    "        with open(docPath, 'r') as doc:\n",
    "            if returnIndices:\n",
    "                indicesList, lengthList = [], []\n",
    "                for line in doc:\n",
    "                    indices,length = self.add_sentence(line,returnIndices)\n",
    "                    if length<=0: continue # handle bad sentences in .txt.\n",
    "                    indicesList.append(indices)\n",
    "                    lengthList.append(length)\n",
    "                return indicesList, lengthList\n",
    "            else:\n",
    "                for line in doc:\n",
    "                    self.add_sentence(line,returnIndices=False)\n",
    "                return None\n",
    "    \n",
    "    def to_words(self, indices):\n",
    "        return [self.get_word(index) for index in indices]\n",
    "    \n",
    "    def to_sent(self, indices):\n",
    "        return ' '.join(self.to_words(indices))\n",
    "    \n",
    "    def to_indices(self, words):\n",
    "        return [self.get_index(word) for word in words]\n",
    "\n",
    "class DataIterator:\n",
    "    \"\"\"Data feeder by batch.\"\"\"\n",
    "\n",
    "    def __init__(self, indexer, pairs, lengths, maxTargetLen=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            indexer: an Indexer object.\n",
    "            pairs: a list of pairs of token index lists.\n",
    "            lengths: a list of pairs of sentence length lists.\n",
    "            maxTargetLen: uniform to decoding length later (for cross entropy computing).\n",
    "        \"\"\"\n",
    "        self.indexer = indexer\n",
    "        self.pairs = pairs\n",
    "        self.lengths = lengths\n",
    "        self.maxTargetLen = maxTargetLen\n",
    "        self.size = len(pairs)\n",
    "        self.indices = range(self.size)\n",
    "\n",
    "    def _get_padded_sentence(self, index, maxSentLen, maxTargetLen):\n",
    "        \"\"\"Pad a sentence pair by EOS (pad both to the largest length of respective batch).\n",
    "\n",
    "        Args:\n",
    "            index: index of a sentence & length pair in self.pairs, self.lengths.\n",
    "            maxSentLen: the length of the longest source sentence.\n",
    "            maxTargetLen: the length of the longest target sentence.\n",
    "        Returns:\n",
    "            padded source sentence (list), its length (int), \n",
    "            padded target sentence (list), its length (int).\n",
    "        \"\"\"\n",
    "        sent1,sent2 = self.pairs[index][0], self.pairs[index][1]\n",
    "        length1,length2 = self.lengths[index][0], self.lengths[index][1]\n",
    "        paddedSent1 = sent1[:maxSentLen] if length1>maxSentLen else sent1+[self.indexer.get_index('EOS')]*(maxSentLen-length1)\n",
    "        paddedSent2 = sent2[:maxTargetLen] if length2>maxTargetLen else sent2+[self.indexer.get_index('EOS')]*(maxTargetLen-length2)\n",
    "        return paddedSent1,length1,paddedSent2,length2\n",
    "\n",
    "    def random_batch(self, batchSize):\n",
    "        \"\"\"Random batching.\n",
    "\n",
    "        Args:\n",
    "            batchSize: size of a batch of sentence pairs and respective lengths.\n",
    "        Returns:\n",
    "            the batch of source sentence (Variable(torch.LongTensor())),\n",
    "            the lengths of source sentences (numpy.array())\n",
    "            and the same for target sentences and lengths.\n",
    "        \"\"\"\n",
    "        batchIndices = np.random.choice(self.indices, size=batchSize, replace=False)\n",
    "        batchSents,batchTargets,batchSentLens,batchTargetLens = [], [], [], []\n",
    "        maxSentLen, _ = np.array([self.lengths[index] for index in batchIndices]).max(axis=0)\n",
    "        for index in batchIndices:\n",
    "            paddedSent1,length1,paddedSent2,length2 = self._get_padded_sentence(index, maxSentLen, self.maxTargetLen)\n",
    "            batchSents.append(paddedSent1)\n",
    "            batchTargets.append(paddedSent2)\n",
    "            batchSentLens.append(length1)\n",
    "            batchTargetLens.append(length2)\n",
    "        batchIndices = range(batchSize) # reindex from 0 for sorting.\n",
    "        batchIndices = [i for i,l in sorted(zip(batchIndices,batchSentLens),key=lambda p:p[1],reverse=True)]\n",
    "        batchSents = Variable(torch.LongTensor(np.array(batchSents)[batchIndices])).transpose(0,1) # <bc,mt> -> <mt,bc>\n",
    "        batchTargets = Variable(torch.LongTensor(np.array(batchTargets)[batchIndices])).transpose(0,1)\n",
    "        batchSentLens = np.array(batchSentLens)[batchIndices]\n",
    "        batchTargetLens = np.array(batchTargetLens)[batchIndices]\n",
    "        return batchSents, batchSentLens, batchTargets, batchTargetLens\n",
    "        \n",
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, dataDir):\n",
    "        self.dataDir = dataDir # str\n",
    "        self.dataDict = {path.split('.')[0]:self.dataDir+path\n",
    "                         for path in os.listdir(self.dataDir) if path.endswith('.txt')}\n",
    "        self.filenames = set(['train_source', 'train_target',\n",
    "                              'test_source', 'test_target'])\n",
    "        if self._filename_mismatch():\n",
    "            raise Exception(\"Expected filenames under the directory:\\n\"+str(self.filenames)+\n",
    "                            '\\nGot:\\n'+str(self.dataDict.keys())+'\\n')\n",
    "    \n",
    "    def _filename_mismatch(self):\n",
    "        return self.filenames - set(self.dataDict.keys()) != set([])\n",
    "    \n",
    "class TranslationDataLoader(DataLoader):\n",
    "    \n",
    "    def __init__(self, dataDir):\n",
    "        DataLoader.__init__(self, dataDir)\n",
    "    \n",
    "    def load(self, specialTokenList=None):\n",
    "        indexer = Indexer(specialTokenList)\n",
    "        print \"... loading training data.\"\n",
    "        trainPairs,trainLens = self._load_pairs(indexer,\n",
    "                                                self.dataDict['train_source'],\n",
    "                                                self.dataDict['train_target'])\n",
    "        print \"... loading test data.\"\n",
    "        testPairs,testLens = self._load_pairs(indexer,\n",
    "                                              self.dataDict['test_source'],\n",
    "                                              self.dataDict['test_target'])\n",
    "        print \"Done!\\n\"\n",
    "        return indexer,trainPairs,trainLens,testPairs,testLens\n",
    "\n",
    "    def _load_pairs(self, indexer, sourcePath, targetPath):\n",
    "        sourceSents,sourceLens = indexer.add_document(sourcePath,returnIndices=True)\n",
    "        targetSents,targetLens = indexer.add_document(targetPath,returnIndices=True)\n",
    "        return zip(sourceSents, targetSents), zip(sourceLens, targetLens)\n",
    "\n",
    "class ToyDataGenerator:\n",
    "    \"\"\"Generate toy translation dataset: translation to the next letter.\n",
    "       E.g. ['A','B','C'] -> ['B','C','D'].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, dataSize, fromLen, toLen, cutoff,\n",
    "                 trainSourcePath, trainTargetPath, testSourcePath, testTargetPath):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            vocab: a list of vocabulary words (in strings).\n",
    "            dataSize: #entries for train + test.\n",
    "            fromLen, toLen: length range of generated sentences.\n",
    "            cutoff: a float between 0. and 1., proportion of training data.\n",
    "            train/testSource/Path: path to saved generation.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.dataSize = dataSize\n",
    "        self.fromLen = fromLen\n",
    "        self.toLen = toLen\n",
    "        self.cutoffIndex = (int)(cutoff*dataSize)\n",
    "        self.trainSourcePath = trainSourcePath\n",
    "        self.trainTargetPath = trainTargetPath\n",
    "        self.testSourcePath = testSourcePath\n",
    "        self.testTargetPath = testTargetPath\n",
    "    \n",
    "    def translate_word(self, word):\n",
    "        \"\"\"Find the next letter.\n",
    "\n",
    "        Args:\n",
    "            word: a letter word (e.g. 'A').\n",
    "        Returns:\n",
    "            The next letter to word.\n",
    "        \"\"\"\n",
    "        return self.vocab[self.vocab.index(word)+1]\n",
    "    \n",
    "    def translate_sent(self, sent):\n",
    "        \"\"\"Find the next-letter translation of a sentence.\n",
    "\n",
    "        Args:\n",
    "            sent: a list of letter words.\n",
    "        Returns:\n",
    "            The next letters.\n",
    "        \"\"\"\n",
    "        return [self.translate_word(word) for word in sent]\n",
    "    \n",
    "    def writeSentToFile(self, path, sent, mode):\n",
    "        \"\"\"Write one sentence to a file (given path).\n",
    "        \n",
    "        Args:\n",
    "            path: path to file.\n",
    "            sent: a single-string sentence.\n",
    "            mode: either 'a' (append) or 'w' (write).\n",
    "        \"\"\"\n",
    "        with open(path, mode) as f:\n",
    "            f.write(sent)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    def writeDataSet(self, sourcePath, targetPath, size):\n",
    "        \"\"\"Write train/test dataset to given source/target path (given size).\n",
    "        \n",
    "        Args:\n",
    "            SourcePath, targetPath: paths to files.\n",
    "            size: #entries.\n",
    "        \"\"\"\n",
    "        for i in range(size):\n",
    "            randInput,randTarget = self.generate_pair()\n",
    "            self.writeSentToFile(sourcePath, ' '.join(randInput), mode='a' if os.path.exists(sourcePath) else 'w')\n",
    "            self.writeSentToFile(targetPath, ' '.join(randTarget), mode='a' if os.path.exists(targetPath) else 'w')            \n",
    "    \n",
    "    def generate_pair(self):\n",
    "        \"\"\"Randomly generate a pair of sentences (arg1 translates to arg2).\n",
    "\n",
    "        Returns:\n",
    "            randInput: a list of letter words.\n",
    "            randTarget: a list of translation letter words of randInput.\n",
    "        \"\"\"        \n",
    "        randInput = list(np.random.choice(self.vocab[:-1], size=random.randint(self.fromLen,self.toLen)))\n",
    "        randTarget = self.translate_sent(randInput)\n",
    "        return randInput, randTarget+[str('EOS')]\n",
    "    \n",
    "    def generate_data(self):\n",
    "        \"\"\"Randomly generate a set of pairs of sentences (arg1 translates to arg2).\n",
    "\n",
    "        Returns:\n",
    "            pairs: a pair of lists of torch Variables (torch.LongTensor).\n",
    "            lengths: lengths of the corresponding lists in pairs.\n",
    "        \"\"\"\n",
    "        print '... generating training data'\n",
    "        self.writeDataSet(self.trainSourcePath, self.trainTargetPath, self.cutoffIndex)\n",
    "        print '... generating test data'\n",
    "        self.writeDataSet(self.testSourcePath, self.testTargetPath, self.dataSize-self.cutoffIndex)\n",
    "        print 'DONE!\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... generating training data\n",
      "... generating test data\n",
      "DONE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('toy'):\n",
    "    os.makedirs('toy') \n",
    "tg = ToyDataGenerator(vocab=[chr(i) for i in range(65,74)],\n",
    "                      dataSize=3000,\n",
    "                      fromLen=3, toLen=8,\n",
    "                      cutoff=0.8,\n",
    "                      trainSourcePath='toy/train_source.txt',\n",
    "                      trainTargetPath='toy/train_target.txt',\n",
    "                      testSourcePath='toy/test_source.txt',\n",
    "                      testTargetPath='toy/test_target.txt')\n",
    "tg.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading training data.\n",
      "... loading test data.\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = TranslationDataLoader('toy/')\n",
    "indexer,trainPairs,trainLens,testPairs,testLens = trans.load(specialTokenList=['EOS','PAD','UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 4, 4, 4, 5, 6, 7], [7, 11, 11, 11, 4, 10, 9, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPairs[0][0], trainPairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A H H H G E B B I I I H F C EOS\n",
      "G D D E H E E F EOS\n"
     ]
    }
   ],
   "source": [
    "print indexer.to_sent(trainPairs[0][0]), indexer.to_sent(trainPairs[0][1])\n",
    "print indexer.to_sent(testPairs[0][0]), indexer.to_sent(testPairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, hiddenSize, nLayers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputSize: vocabulary size.\n",
    "            hiddenSize: size of RNN hidden state.\n",
    "            nLayers: number of stacked layers.\n",
    "            dropout: dropout rate.\n",
    "        \"\"\"\n",
    "        # inputSize: vocabulary size.\n",
    "        # hiddenSize: size for both embedding and GRU hidden.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.dropoutLayer = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, nLayers, dropout=dropout)\n",
    "    \n",
    "    def forward(self, inputs, inputsLen, hidden=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputs: Variable(torch.LongTensor()) of the shape <max-time,batch-size>.\n",
    "            inputsLen: a list of input lengths with the shape <batch-size,>.\n",
    "            hidden: input hidden state (initialized as None).\n",
    "        \"\"\"\n",
    "        # inputs: <mt,bc>\n",
    "        # inputsLen: <bc,> (a list).\n",
    "        # hidden: <n_layer*n_direction,bc,h>\n",
    "        embedded = self.embedding(inputs) # <mt,bc,h>\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, inputsLen)\n",
    "            # 'packed' has a 'data' and a 'batch_sizes' field.\n",
    "            #   'data' is a <sum(len),h> matrix (len is real lengths, not padded).\n",
    "            #   'batch_sizes' has the number of non-zero batches at each time-step.\n",
    "            # e.g. for this 'inputs'\n",
    "            #    2     1     3     0     2\n",
    "            #    6     8     1     6     2\n",
    "            #    0     7     0     8     8\n",
    "            #    6     4     2     1     1\n",
    "            #    1     8     1     1     1\n",
    "            #    6     1     1     1     1\n",
    "            #    0     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1\n",
    "            #    1     1     1     1     1  \n",
    "            # 'data' = 22 = 7+5+4+3+3 (1's are pads corresponding to 'EOS').\n",
    "            # 'batch_sizes' = [5, 5, 5, 3, 2, 1, 1].\n",
    "        outputs,hidden = self.gru(packed, hidden)#, dropout=dropout)\n",
    "            # outputs: same format as 'packed'.\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "        outputs, outputsLen = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "            # outputs: <mt,bc,h>\n",
    "            # outputsLen: same as the 'batch_sizes' field of 'packed'. \n",
    "        outputs = self.dropoutLayer(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "\n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"Simple GRU decoder (Bahdanau attention).\"\"\"\n",
    "    \n",
    "    def __init__(self, hiddenSize, outputSize, nLayers=2, dropout=0.1, residual=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            hiddenSize: GRU hidden state size.\n",
    "            outputSize: vocabulary size.\n",
    "            nLayers: number of stacked layers.\n",
    "            dropout: dropout rate.\n",
    "            residual: boolean, whether establish residual link or not.\n",
    "        \"\"\"\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.nLayers = nLayers\n",
    "        self.residual = residual\n",
    "        self.embedding = nn.Embedding(outputSize, hiddenSize)\n",
    "        self.dropoutLayer = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(2*hiddenSize, hiddenSize, nLayers) \n",
    "        self.out = nn.Linear(2*hiddenSize, outputSize)\n",
    "            # inputSize doubles because concatted context of same hiddenSize.\n",
    "        self.linear = nn.Linear(hiddenSize, hiddenSize)\n",
    "\n",
    "    def forward(self, inputs, hidden, context, encoderOutput):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            inputs: inputs to decoder, of the shape <batch-size,> (1 time-step).\n",
    "            hidden: <n_layers*n_directions,batch-size,hidden-size>.\n",
    "            context: context vector made using attention, <batch-size,hidden-size>.\n",
    "            encoderOutput: <max-time,batch-size,hidden-size>.\n",
    "            NB: all are Variable(torch.LongTensor()).\n",
    "        Returns:\n",
    "            output: <batch-size,vocab-size>.\n",
    "            hidden: <n_layers*n_directions,batch-size,hidden-size>.\n",
    "            context: <batch-size,hidden-size>.\n",
    "            attentionWeights: <batch-size,max-time>.\n",
    "        \"\"\"\n",
    "            # inputs: <bc,>\n",
    "            # hidden: <n_layer*n_direction,bc,h>\n",
    "            # context: <bc,h>\n",
    "            # encoderOutput: <mt,bc,h>  \n",
    "        batchSize = inputs.size(0)\n",
    "        encoderOutputLen = encoderOutput.size(0)\n",
    "        embedded = self.embedding(inputs).view(1,batchSize,self.hiddenSize) # <mt=1,bc,h>\n",
    "        inputs = torch.cat((embedded,context.unsqueeze(0)),2)\n",
    "            # unsqueeze: <bc,h> -> <mt=1,bc,h>\n",
    "            # concat: <mt,bc,h> & <mt,bc,h> @2 -> <mt,bc,2h>\n",
    "        output, hidden = self.gru(inputs, hidden)#, dropout=dropout)\n",
    "            # IN: <mt=1,bc,2h>, <n_layer*n_direction,bc,h>\n",
    "            # OUT: <mt=1,bc,h>, <n_layer*n_direction,bc,h>\n",
    "        output = self.dropoutLayer(output)\n",
    "        hidden = hidden + embedded if self.residual else hidden\n",
    "        attentionWeights = Variable(torch.zeros(batchSize,encoderOutputLen)).cuda()\n",
    "        for b in range(batchSize):\n",
    "            rawAttentionWeight = torch.mm(self.linear(encoderOutput[:,b,:]), \n",
    "                                          hidden[:,b,:][-1].unsqueeze(1)).squeeze()\n",
    "                # op1. linear transformation on encoderOutput (dot energy).\n",
    "                # op2. select <mt,h> and <1,h> slices (from <mt,bc,h> and <1,bc,h>).\n",
    "                # op3. sel hidden last dim <h,> and expand -> <mt,h> & <h,1> now.\n",
    "                # op4. matmul -> <mt,1>.\n",
    "                # op5. squeeze -> <mt,>\n",
    "            attentionWeights[b] = F.softmax(rawAttentionWeight, dim=-1)\n",
    "                # normalize to get a distribution.\n",
    "            # result: <bc,mt> attention matrix, normalized along mt.\n",
    "        multiDiag = Variable(torch.eye(batchSize).expand(self.hiddenSize,batchSize,batchSize),\n",
    "                             requires_grad=False).cuda()\n",
    "            # op1. eye -> <bc,bc> diagonal matrix mask.\n",
    "            # op2. expand -> <h,bc,bc>, same shape as attended encoderOutput.\n",
    "            # op3. Variable/grad=false: same type as attended encoderOutput.\n",
    "        context = (torch.matmul(attentionWeights, encoderOutput.permute(2,0,1)) * multiDiag).sum(dim=2).transpose(0,1)\n",
    "            # op1. masking -> <h,bc,bc>, with the last 2 dims only have non-zero diag elems.\n",
    "            # op2. compress 1 bc dimension (useless, because its diag).\n",
    "            # op3. <h,bc> -> <bc,h>, keep input shape.\n",
    "        output = output.squeeze(0)\n",
    "            # output squeeze: <mt=1,bc=1,h> -> <bc,h>, to concat with context\n",
    "        output = F.log_softmax(F.tanh(self.out(torch.cat((output,context),1))),dim=-1)\n",
    "            # concat: <bc,h> & <bc,h> @1 -> <bc,2h>\n",
    "            # linear->tahn/out: <bc,2h> * <2h,vocab> -> <bc,vocab>\n",
    "            # softmax: along dim=-1, i.e. vocab.  \n",
    "        return output, hidden, context, attentionWeights\n",
    "            # full output for visualization:\n",
    "            #   output: <bc,vocab>\n",
    "            #   hidden: <n_layer*n_direction,bc,h>\n",
    "            #   context: <bc,h>\n",
    "            #   attentionWeights: <bc,mt> \n",
    "\n",
    "class Seq2Seq:\n",
    "    \"\"\"Encoder-Decoder model with Bahdanau attention, stacking and residual links.\"\"\"\n",
    "    \n",
    "    def __init__(self, indexer, trainPairs, trainLens, testPairs, testLens, \n",
    "                 batchSize=5, hiddenSize=10,\n",
    "                 nLayers=2, dropout=0.1, residual=True, \n",
    "                 lr=1e-4, lrDecay=0.95, lrDecayFreq=100,\n",
    "                 l2Reg=0.5,\n",
    "                 enforcingRatio=0.5, clip=5.0,\n",
    "                 maxDecodingLen=10,\n",
    "                 resultSavePath='toy/results.txt'):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            indexer: an Indexer object.\n",
    "            trainPairs, testPairs: each is a list of pairs of word index list.\n",
    "            trainLens, testLens: each is a list of pairs of length of word index list.\n",
    "            batchSize: int. (default=5)\n",
    "            hiddenSize: int. (default=10)\n",
    "            nLayers: number of GRU stacking layers. (default=2)\n",
    "            dropout: dropout rate. (default=0.1)\n",
    "            residual: boolean, whether to establish residual links. (default=True)\n",
    "            lr: learning rate, float. (default=1e-4 with Adam)\n",
    "            lrDecay: rate at which lr drops per m batches.\n",
    "            lrDecayFreq: the number of batches per lr decay.\n",
    "            l2Reg: ridge regression.\n",
    "            enforcingRatio: the percentage of teacher-enforced training. (default=0.5)\n",
    "            clip: gradient clip cap, float. (default=5.0)\n",
    "            maxDecodingLen: max #tokens generated by decoder before stopping.\n",
    "            resultSavePath: (input,prediction,target) sentence triples file path.\n",
    "        \"\"\"\n",
    "        self.indexer = indexer\n",
    "        self.trainIter = DataIterator(indexer, trainPairs, trainLens, maxTargetLen=maxDecodingLen)\n",
    "        self.testIter = DataIterator(indexer, testPairs, testLens, maxTargetLen=maxDecodingLen)\n",
    "        self.batchSize = batchSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.nLayers = nLayers\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.lr = lr\n",
    "        self.lrDecay = lrDecay\n",
    "        self.lrDecayFreq = lrDecayFreq\n",
    "        self.l2Reg = l2Reg\n",
    "        self.enforcingRatio = enforcingRatio\n",
    "        self.clip = clip\n",
    "        self.maxDecodingLen = maxDecodingLen\n",
    "        self.resultSavePath = resultSavePath\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Specify computational graph.\"\"\"\n",
    "        self.encoder = EncoderRNN(self.indexer.size, self.hiddenSize, \n",
    "                                  nLayers=self.nLayers, dropout=self.dropout).cuda()\n",
    "        self.decoder = AttentionDecoderRNN(self.hiddenSize, self.indexer.size,\n",
    "                                           nLayers=self.nLayers, dropout=self.dropout, residual=self.residual).cuda()\n",
    "        self.encoderOptim = optim.Adam(self.encoder.parameters(), self.lr, weight_decay=self.l2Reg)\n",
    "        self.decoderOptim = optim.Adam(self.decoder.parameters(), self.lr, weight_decay=self.l2Reg)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "    \n",
    "    def _model_config(self):\n",
    "        return 'Vocab Size = ' + str(self.indexer.size) + '\\n' + \\\n",
    "               'Train/Test Size = ' + str(self.trainIter.size)+'/'+str(self.testIter.size) + '\\n' + \\\n",
    "               'batchSize = ' + str(self.batchSize) + '; hiddenSize = ' + str(self.hiddenSize) + '\\n' + \\\n",
    "               'nLayers = ' + str(self.nLayers) + '; dropout = ' + str(self.dropout) + '\\n' + \\\n",
    "               'residual = ' + str(self.residual) + '; learning rate = ' + str(self.lr) + '\\n' + \\\n",
    "               'learning rate decay = ' + str(self.lrDecay) + ' per ' + str(self.lrDecayFreq) + ' batches/steps\\n' + \\\n",
    "               'regularization (l2) = ' + str(self.l2Reg) + '\\n' + \\\n",
    "               'teacher enforce ratio = ' + str(self.enforcingRatio) + '; clip = ' + str(self.clip) + '\\nn'\n",
    "    \n",
    "    def _lr_decay(self, encOptim, decOptim):\n",
    "        self.lr *= self.lrDecay\n",
    "        for paramGroup in encOptim.param_groups:\n",
    "            paramGroup['lr'] = self.lr\n",
    "        for paramGroup in decOptim.param_groups:\n",
    "            paramGroup['lr'] = self.lr\n",
    "        return encOptim, decOptim\n",
    "    \n",
    "    def _train_step(self):\n",
    "        \"\"\"One step of training.\"\"\"\n",
    "        inputs, inputsLen, targets, targetsLen = self.trainIter.random_batch(self.batchSize)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        self.encoderOptim.zero_grad()\n",
    "        self.decoderOptim.zero_grad()\n",
    "        loss = 0\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(inputs, inputsLen, encoderHidden)    \n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*self.batchSize)).cuda()\n",
    "        decoderContext = Variable(torch.zeros(self.batchSize,self.decoder.hiddenSize)).cuda()\n",
    "        decoderHidden = encoderHidden\n",
    "        enforce = random.random() < self.enforcingRatio\n",
    "        decoderOutputAll = Variable(torch.zeros(self.maxDecodingLen,self.batchSize,self.decoder.outputSize)).cuda()\n",
    "            # <mt-max,bc,vocab>\n",
    "        mask = torch.LongTensor([1]*self.batchSize).cuda()\n",
    "            # start with 1, a cell turns 0 to mask out generation after an EOS is seen.\n",
    "        for di in range(self.maxDecodingLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            decoderOutputAll[di] = decoderOutput\n",
    "            if enforce:\n",
    "                decoderInput = targets[di] # <== targets is <mt,bc>\n",
    "            else:\n",
    "                topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "                topIndices = topIndices.squeeze()# topIndices = <bc,>\n",
    "                for b in range(self.batchSize):\n",
    "                    if topIndices[b] == 0: # EOS\n",
    "                        mask[b] = 0\n",
    "                topIndices = topIndices * mask\n",
    "                decoderInput = Variable(topIndices).cuda()\n",
    "        # Batch cross entropy\n",
    "            # requires arg1/pred = <#entries,vocab>, arg2/target = <#entries,>\n",
    "        decoderOutputAll = decoderOutputAll.view(-1, self.decoder.outputSize)\n",
    "            # reshape to <mt*bc,vocab>\n",
    "        targets = targets.contiguous().view(-1)\n",
    "            # reshape to <mt*bc>\n",
    "        loss = self.criterion(decoderOutputAll, targets)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.encoder.parameters(), self.clip)\n",
    "        torch.nn.utils.clip_grad_norm(self.decoder.parameters(), self.clip)\n",
    "        self.encoderOptim.step()\n",
    "        self.decoderOptim.step()\n",
    "        return loss.data[0] / targetsLen        \n",
    "    \n",
    "    def train(self, nEpochs=1, epochSize=100, printEvery=5):\n",
    "        \"\"\"Train on loaded data upon construction.\n",
    "        \n",
    "        Args:\n",
    "            nEpochs: number of epochs.\n",
    "            epochSize: number of batches trained in an epoch.\n",
    "            printEvery: frequency of results report.\n",
    "        \"\"\"\n",
    "        globalStep = 0\n",
    "        averageLoss = 0\n",
    "        start = time.time()\n",
    "        for e in range(nEpochs):\n",
    "            epochLoss = 0\n",
    "            for step in range(epochSize):\n",
    "                loss = self._train_step()\n",
    "                if step!=0 and step%printEvery==0:\n",
    "                    print(\"Step %d average loss = %.4f (time: %.2f)\" % (step, loss.mean(), # batch mean.\n",
    "                                                                        time.time()-start))\n",
    "                    start = time.time()\n",
    "                epochLoss += loss.mean()\n",
    "                globalStep += 1\n",
    "                if globalStep%self.lrDecayFreq==0:\n",
    "                    print '  [LR DECAY] from %.10f to %.10f at step %d' % (self.lr, self.lr*self.lrDecay, globalStep)\n",
    "                    self.encoderOptim, self.decoderOptim = self._lr_decay(self.encoderOptim, self.decoderOptim)\n",
    "            epochLoss /= epochSize\n",
    "            averageLoss += epochLoss\n",
    "            print(\"\\nEpoch %d loss = %.4f\\n\" % (e+1,epochLoss))\n",
    "            averageBleu = self.evaluate_random(size=self.batchSize, saveResults=False, printResults=True)\n",
    "        averageLoss /= nEpochs\n",
    "        print(\"\\nGrand average loss = %.4f\\n\" % averageLoss) \n",
    "        \n",
    "    def _clear_special_tokens(self, words):\n",
    "        \"\"\"Clear all the PAD, UNK, EOS to avoid inflated BLEU.\n",
    "        \n",
    "        Args:\n",
    "            words: a list of tokens.\n",
    "        Returns:\n",
    "            a list of tokens which are not special tokens.\n",
    "        \"\"\"\n",
    "        return [word for word in words if word not in set([\"PAD\",\"UNK\",\"EOS\"])]\n",
    "\n",
    "    def evaluate_pair(self, predWords, targetWords):\n",
    "        \"\"\"Compute the BLEU score of a prediction given a reference.\n",
    "        \n",
    "        Args:\n",
    "            predWords: predicted words (a list of strings).\n",
    "            targetWords: reference, same type as preWords.\n",
    "        Returns:\n",
    "            The BLEU score (uses = nltk.translate.bleu_score.sentence_bleu).\n",
    "        \"\"\"\n",
    "        return bleu([self._clear_special_tokens(targetWords)], \n",
    "                     self._clear_special_tokens(predWords), smoothing_function=SMOOTH.method3)\n",
    "\n",
    "        \n",
    "    def evaluate_random(self, size, saveResults, printResults=True):\n",
    "        \"\"\"Randomly evaluate samples from the test set (which is loaded upon construction).\n",
    "        \n",
    "        Args:\n",
    "            size: number of samples evaluated (as a single batch).\n",
    "            printResults: print input, prediction and gold translation to console. (default=True)\n",
    "        Returns:\n",
    "            The average BLEU score in the batch.\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        inputs, inputsLen, targets, targetsLen = self.testIter.random_batch(size)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(inputs, inputsLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*size)).cuda()\n",
    "        decoderContext = Variable(torch.zeros(size,self.decoder.hiddenSize)).cuda()\n",
    "        decoderHidden = encoderHidden\n",
    "        predictions = []\n",
    "        for di in range(self.maxDecodingLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()).cuda() # <bc,1> -> <bc,>\n",
    "            predictions.append(topIndices.view(-1).cpu().numpy())\n",
    "        inputs = inputs.data.cpu().numpy().transpose()\n",
    "        predictions = np.array(predictions).transpose() # <mt,bc> -> <bc,mt>\n",
    "        targets = targets.data.cpu().numpy().transpose()\n",
    "        bleuList = []\n",
    "        results = []\n",
    "        for i,(input,pred,target) in enumerate(zip(inputs,predictions,targets)):\n",
    "            inputWords = self._clear_special_tokens(self.indexer.to_words(input))\n",
    "            predWords = self._clear_special_tokens(self.indexer.to_words(pred))\n",
    "            targetWords = self._clear_special_tokens(self.indexer.to_words(target))\n",
    "            bleuCurr = self.evaluate_pair(predWords, targetWords)\n",
    "            bleuList.append(bleuCurr)\n",
    "            inputSent = ' '.join(inputWords)\n",
    "            predSent = ' '.join(predWords)\n",
    "            targetSent = ' '.join(targetWords)\n",
    "            results.append([inputSent, predSent, targetSent])\n",
    "            if printResults:\n",
    "                print(\"Example %d\" % (i+1))\n",
    "                print(\"INPUT >> %s\" % inputSent)\n",
    "                print(\"PRED >> %s\" % predSent)\n",
    "                print(\"TRUE >> %s\" % targetSent)\n",
    "                print(\"[BLEU] %.2f\\n\" % bleuCurr)\n",
    "        averageBleu = np.mean(bleuList)\n",
    "        if saveResults:\n",
    "            return averageBleu, results\n",
    "        return averageBleu\n",
    "\n",
    "    def evaluate(self, nBatches=10, saveResults=True):\n",
    "        \"\"\"Randomly evaluate a given number of batches.\n",
    "        \n",
    "        Args:\n",
    "            nBatches: the number of random batches to be evaluated.\n",
    "        \"\"\"\n",
    "        averageBleuList = []\n",
    "        for i in range(nBatches):\n",
    "            if saveResults:\n",
    "                averageBleu, results = self.evaluate_random(self.batchSize, saveResults, printResults=False)\n",
    "                averageBleuList.append(averageBleu)\n",
    "                with open(self.resultSavePath, 'a') as f:\n",
    "                    if i==0:\n",
    "                        f.write(self._model_config())\n",
    "                        f.write('=================================\\n')\n",
    "                    for input,pred,target in results:\n",
    "                        f.write('INPUT  >> ' + input + '\\n')\n",
    "                        f.write('PRED   >> ' + pred + '\\n')\n",
    "                        f.write('TARGET >> ' + target + '\\n\\n')\n",
    "            else:\n",
    "                averageBleuList.append(self.evaluate_random(self.batchSize, saveResults, printResults=False))\n",
    "        message = \"Average BLEU score over %d examples is %.4f\" % (self.batchSize*nBatches, \n",
    "                                                                   np.mean(averageBleuList))\n",
    "        with open(self.resultSavePath, 'a') as f:\n",
    "            f.write('=================================\\n')\n",
    "            f.write(message)\n",
    "        print message\n",
    "            \n",
    "    def evaluate_given(self, sent, maxLen=20):\n",
    "        \"\"\"Evaluate a give sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a single string. OOVs are treated as UNKs.\n",
    "            maxLen: the max number of decoding steps.\n",
    "        \"\"\"\n",
    "        sent = sent.split()\n",
    "        sentCode = [self.indexer.get_index(word,add=False) for word in sent]\n",
    "        if any(i==-1 for i in sentCode):\n",
    "            raise Exception(\"This sentence contains out of vocabulary words!\")\n",
    "        input = Variable(torch.LongTensor(sentCode)).cuda().view(-1,1)\n",
    "        inputLen = np.array([len(sentCode)])\n",
    "        # Run encoder\n",
    "        encoderHidden = None\n",
    "        encoderOutput, encoderHidden = self.encoder(input, inputLen, encoderHidden)\n",
    "        # Run decoder\n",
    "        decoderInput = Variable(torch.LongTensor([self.indexer.get_index('EOS')]*1)).cuda()\n",
    "        decoderContext = Variable(torch.zeros(1,self.decoder.hiddenSize)).cuda()\n",
    "        decoderHidden = encoderHidden\n",
    "        pred = []\n",
    "        for di in range(maxLen):\n",
    "            decoderOutput,decoderHidden,decoderContext,attentionWeights = self.decoder(decoderInput,\n",
    "                                                                                       decoderHidden,\n",
    "                                                                                       decoderContext, \n",
    "                                                                                       encoderOutput)\n",
    "            topValues,topIndices = decoderOutput.data.topk(1) # <bc,1>\n",
    "            decoderInput = Variable(topIndices.squeeze()).cuda() # <bc,1> -> <bc,>\n",
    "            predIndex = topIndices.view(-1).cpu().numpy()[0]\n",
    "            if predIndex == self.indexer.get_index('EOS'):\n",
    "                break\n",
    "            pred.append(predIndex)\n",
    "        print(\"INPUT >> %s\" % ' '.join(sent))\n",
    "        print(\"PRED >> %s\\n\" % ' '.join(self.indexer.to_words(pred))) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __init__(self, indexer, trainPairs, trainLens, testPairs, testLens, \n",
    "#                  batchSize=5, hiddenSize=10,\n",
    "#                  nLayers=2, dropout=0.1, residual=True, \n",
    "#                  lr=1e-4, lrDecay=0.95, lrDecayFreq=100,\n",
    "#                  l2Reg=0.5,trans = TranslationDataLoa\n",
    "#                  enforcingRatio=0.5, clip=5.0,\n",
    "#                  maxDecodingLen=10,\n",
    "#                  resultSavePath='toy/results.txt'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 average loss = 0.2317 (time: 7.67)\n",
      "Step 200 average loss = 0.2246 (time: 7.71)\n",
      "Step 300 average loss = 0.2093 (time: 7.72)\n",
      "Step 400 average loss = 0.2177 (time: 7.50)\n",
      "Step 500 average loss = 0.2059 (time: 7.37)\n",
      "  [LR DECAY] from 0.0010000000 to 0.0009500000 at step 600\n",
      "\n",
      "Epoch 1 loss = 0.2263\n",
      "\n",
      "Example 1\n",
      "INPUT >> D D B G B D C\n",
      "PRED >> E E C H C D\n",
      "TRUE >> E E C H C E D\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 2\n",
      "INPUT >> F H B G F\n",
      "PRED >> G I C H G\n",
      "TRUE >> G I C H G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B F G G G\n",
      "PRED >> C G H H H\n",
      "TRUE >> C G H H H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E B H F\n",
      "PRED >> F C I G\n",
      "TRUE >> F C I G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D B C\n",
      "PRED >> E C D D\n",
      "TRUE >> E C D\n",
      "[BLEU] 0.59\n",
      "\n",
      "Step 100 average loss = 0.1397 (time: 15.10)\n",
      "Step 200 average loss = 0.1761 (time: 6.88)\n",
      "Step 300 average loss = 0.1840 (time: 7.46)\n",
      "Step 400 average loss = 0.1848 (time: 7.30)\n",
      "Step 500 average loss = 0.1635 (time: 7.39)\n",
      "  [LR DECAY] from 0.0009500000 to 0.0009025000 at step 1200\n",
      "\n",
      "Epoch 2 loss = 0.1673\n",
      "\n",
      "Example 1\n",
      "INPUT >> G E D B C F\n",
      "PRED >> H F E C D G\n",
      "TRUE >> H F E C D G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> F C E E A F\n",
      "PRED >> G D F B G\n",
      "TRUE >> G D F F B G\n",
      "[BLEU] 0.52\n",
      "\n",
      "Example 3\n",
      "INPUT >> H B B H G G\n",
      "PRED >> I C C I H H\n",
      "TRUE >> I C C I H H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D C E A\n",
      "PRED >> E D F B\n",
      "TRUE >> E D F B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> C F C\n",
      "PRED >> D G D\n",
      "TRUE >> D G D\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1735 (time: 14.97)\n",
      "Step 200 average loss = 0.1716 (time: 7.58)\n",
      "Step 300 average loss = 0.1406 (time: 7.50)\n",
      "Step 400 average loss = 0.1755 (time: 7.00)\n",
      "Step 500 average loss = 0.1507 (time: 7.13)\n",
      "  [LR DECAY] from 0.0009025000 to 0.0008573750 at step 1800\n",
      "\n",
      "Epoch 3 loss = 0.1615\n",
      "\n",
      "Example 1\n",
      "INPUT >> F F B D B F B\n",
      "PRED >> G G C E C G C G C\n",
      "TRUE >> G G C E C G C\n",
      "[BLEU] 0.73\n",
      "\n",
      "Example 2\n",
      "INPUT >> B A D A G\n",
      "PRED >> C B E B H H\n",
      "TRUE >> C B E B H\n",
      "[BLEU] 0.76\n",
      "\n",
      "Example 3\n",
      "INPUT >> A G B F\n",
      "PRED >> B H C G\n",
      "TRUE >> B H C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C E F\n",
      "PRED >> D F G G\n",
      "TRUE >> D F G\n",
      "[BLEU] 0.59\n",
      "\n",
      "Example 5\n",
      "INPUT >> H D C\n",
      "PRED >> I E D\n",
      "TRUE >> I E D\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1583 (time: 15.15)\n",
      "Step 200 average loss = 0.1375 (time: 7.48)\n",
      "Step 300 average loss = 0.1943 (time: 7.25)\n",
      "Step 400 average loss = 0.1666 (time: 7.25)\n",
      "Step 500 average loss = 0.1500 (time: 7.57)\n",
      "  [LR DECAY] from 0.0008573750 to 0.0008145062 at step 2400\n",
      "\n",
      "Epoch 4 loss = 0.1568\n",
      "\n",
      "Example 1\n",
      "INPUT >> D B C G D D B E\n",
      "PRED >> E C D H E E E C F\n",
      "TRUE >> E C D H E E C F\n",
      "[BLEU] 0.82\n",
      "\n",
      "Example 2\n",
      "INPUT >> E E A D A F H\n",
      "PRED >> F F B E E B G I\n",
      "TRUE >> F F B E B G I\n",
      "[BLEU] 0.67\n",
      "\n",
      "Example 3\n",
      "INPUT >> F H F C D G\n",
      "PRED >> G I G D E H H\n",
      "TRUE >> G I G D E H\n",
      "[BLEU] 0.81\n",
      "\n",
      "Example 4\n",
      "INPUT >> D C E\n",
      "PRED >> E D F\n",
      "TRUE >> E D F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> D E A\n",
      "PRED >> E F B\n",
      "TRUE >> E F B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.2109 (time: 15.25)\n",
      "Step 200 average loss = 0.1589 (time: 7.62)\n",
      "Step 300 average loss = 0.1473 (time: 7.47)\n",
      "Step 400 average loss = 0.1725 (time: 7.38)\n",
      "Step 500 average loss = 0.1386 (time: 7.65)\n",
      "  [LR DECAY] from 0.0008145062 to 0.0007737809 at step 3000\n",
      "\n",
      "Epoch 5 loss = 0.1546\n",
      "\n",
      "Example 1\n",
      "INPUT >> E H H C D F H\n",
      "PRED >> F I I D E G I\n",
      "TRUE >> F I I D E G I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G G H C B C\n",
      "PRED >> H H I D C D\n",
      "TRUE >> H H I D C D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> A C C C\n",
      "PRED >> B D D D\n",
      "TRUE >> B D D D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H G B\n",
      "PRED >> I H C\n",
      "TRUE >> I H C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> B B B\n",
      "PRED >> C C C\n",
      "TRUE >> C C C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1561 (time: 14.81)\n",
      "Step 200 average loss = 0.1855 (time: 7.68)\n",
      "Step 300 average loss = 0.1514 (time: 7.36)\n",
      "Step 400 average loss = 0.1707 (time: 7.58)\n",
      "Step 500 average loss = 0.1751 (time: 7.59)\n",
      "  [LR DECAY] from 0.0007737809 to 0.0007350919 at step 3600\n",
      "\n",
      "Epoch 6 loss = 0.1552\n",
      "\n",
      "Example 1\n",
      "INPUT >> H G A H G F C B\n",
      "PRED >> I H B I H G D C\n",
      "TRUE >> I H B I H G D C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G A D F E B F\n",
      "PRED >> H B E G F C G\n",
      "TRUE >> H B E G F C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> D E H H D A A\n",
      "PRED >> E F I I E B B\n",
      "TRUE >> E F I I E B B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H B D D E B C\n",
      "PRED >> I C E E F C D\n",
      "TRUE >> I C E E F C D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> C F B\n",
      "PRED >> D G C\n",
      "TRUE >> D G C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1141 (time: 14.69)\n",
      "Step 200 average loss = 0.1646 (time: 7.46)\n",
      "Step 300 average loss = 0.1310 (time: 7.58)\n",
      "Step 400 average loss = 0.1634 (time: 7.45)\n",
      "Step 500 average loss = 0.1642 (time: 7.39)\n",
      "  [LR DECAY] from 0.0007350919 to 0.0006983373 at step 4200\n",
      "\n",
      "Epoch 7 loss = 0.1557\n",
      "\n",
      "Example 1\n",
      "INPUT >> D B G A D G D\n",
      "PRED >> E C H B E H E\n",
      "TRUE >> E C H B E H E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> A H H E A E E\n",
      "PRED >> B I I F B F F\n",
      "TRUE >> B I I F B F F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E B G G B B F\n",
      "PRED >> F C H H C C G\n",
      "TRUE >> F C H H C C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> G E D B A B E\n",
      "PRED >> H F E C B C F\n",
      "TRUE >> H F E C B C F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B F C E\n",
      "PRED >> C G D F\n",
      "TRUE >> C G D F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1495 (time: 14.87)\n",
      "Step 200 average loss = 0.1468 (time: 7.39)\n",
      "Step 300 average loss = 0.1570 (time: 7.54)\n",
      "Step 400 average loss = 0.1295 (time: 7.10)\n",
      "Step 500 average loss = 0.1671 (time: 6.97)\n",
      "  [LR DECAY] from 0.0006983373 to 0.0006634204 at step 4800\n",
      "\n",
      "Epoch 8 loss = 0.1535\n",
      "\n",
      "Example 1\n",
      "INPUT >> F E E F F F H\n",
      "PRED >> G F F G G G I I\n",
      "TRUE >> G F F G G G I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 2\n",
      "INPUT >> A F H C\n",
      "PRED >> B G I D\n",
      "TRUE >> B G I D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> F B H\n",
      "PRED >> G C I\n",
      "TRUE >> G C I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 4\n",
      "INPUT >> H E G\n",
      "PRED >> I F H\n",
      "TRUE >> I F H\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> G G D\n",
      "PRED >> H H E\n",
      "TRUE >> H H E\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1530 (time: 14.60)\n",
      "Step 200 average loss = 0.1478 (time: 7.61)\n",
      "Step 300 average loss = 0.1386 (time: 7.64)\n",
      "Step 400 average loss = 0.1435 (time: 6.83)\n",
      "Step 500 average loss = 0.1452 (time: 7.57)\n",
      "  [LR DECAY] from 0.0006634204 to 0.0006302494 at step 5400\n",
      "\n",
      "Epoch 9 loss = 0.1537\n",
      "\n",
      "Example 1\n",
      "INPUT >> E C D E E D B G\n",
      "PRED >> F D E F F E C H\n",
      "TRUE >> F D E F F E C H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> B D C B C A\n",
      "PRED >> C E D C D B\n",
      "TRUE >> C E D C D B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B D H A B F\n",
      "PRED >> C E I B C G\n",
      "TRUE >> C E I B C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C D H E G\n",
      "PRED >> D E I F H\n",
      "TRUE >> D E I F H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> E E G E\n",
      "PRED >> F F H F\n",
      "TRUE >> F F H F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1720 (time: 14.81)\n",
      "Step 200 average loss = 0.1230 (time: 7.63)\n",
      "Step 300 average loss = 0.1646 (time: 7.60)\n",
      "Step 400 average loss = 0.1457 (time: 7.66)\n",
      "Step 500 average loss = 0.1615 (time: 7.61)\n",
      "  [LR DECAY] from 0.0006302494 to 0.0005987369 at step 6000\n",
      "\n",
      "Epoch 10 loss = 0.1534\n",
      "\n",
      "Example 1\n",
      "INPUT >> G B G F A D A B\n",
      "PRED >> H C H G B E B C\n",
      "TRUE >> H C H G B E B C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H E E C A C E C\n",
      "PRED >> I F F D B D F D\n",
      "TRUE >> I F F D B D F D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> G G A H B B H\n",
      "PRED >> H H B I C I\n",
      "TRUE >> H H B I C C I\n",
      "[BLEU] 0.71\n",
      "\n",
      "Example 4\n",
      "INPUT >> D F D C\n",
      "PRED >> E G E D\n",
      "TRUE >> E G E D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> C A F C\n",
      "PRED >> D B G D\n",
      "TRUE >> D B G D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1522 (time: 15.25)\n",
      "Step 200 average loss = 0.1191 (time: 7.44)\n",
      "Step 300 average loss = 0.1347 (time: 7.60)\n",
      "Step 400 average loss = 0.1796 (time: 7.40)\n",
      "Step 500 average loss = 0.1737 (time: 7.60)\n",
      "  [LR DECAY] from 0.0005987369 to 0.0005688001 at step 6600\n",
      "\n",
      "Epoch 11 loss = 0.1528\n",
      "\n",
      "Example 1\n",
      "INPUT >> H H C D E D F\n",
      "PRED >> I I D E F E G\n",
      "TRUE >> I I D E F E G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> B F G G G\n",
      "PRED >> C G H H H\n",
      "TRUE >> C G H H H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> F A C E\n",
      "PRED >> G B D F\n",
      "TRUE >> G B D F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> D F E\n",
      "PRED >> E G F\n",
      "TRUE >> E G F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> C F C\n",
      "PRED >> D G D\n",
      "TRUE >> D G D\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1759 (time: 14.38)\n",
      "Step 200 average loss = 0.1815 (time: 7.17)\n",
      "Step 300 average loss = 0.1490 (time: 7.40)\n",
      "Step 400 average loss = 0.1583 (time: 7.45)\n",
      "Step 500 average loss = 0.1906 (time: 7.47)\n",
      "  [LR DECAY] from 0.0005688001 to 0.0005403601 at step 7200\n",
      "\n",
      "Epoch 12 loss = 0.1522\n",
      "\n",
      "Example 1\n",
      "INPUT >> A E D C H E E D\n",
      "PRED >> B F E D I F F E\n",
      "TRUE >> B F E D I F F E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H E H H E E F\n",
      "PRED >> I F I I F F G\n",
      "TRUE >> I F I I F F G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> G E D B C F\n",
      "PRED >> H F E C D G\n",
      "TRUE >> H F E C D G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B D H A B F\n",
      "PRED >> C E I B C G\n",
      "TRUE >> C E I B C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> G D C E F\n",
      "PRED >> H E D F G\n",
      "TRUE >> H E D F G\n",
      "[BLEU] 1.00\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 average loss = 0.1814 (time: 15.14)\n",
      "Step 200 average loss = 0.1568 (time: 7.53)\n",
      "Step 300 average loss = 0.1733 (time: 7.08)\n",
      "Step 400 average loss = 0.1570 (time: 7.24)\n",
      "Step 500 average loss = 0.1800 (time: 7.47)\n",
      "  [LR DECAY] from 0.0005403601 to 0.0005133421 at step 7800\n",
      "\n",
      "Epoch 13 loss = 0.1543\n",
      "\n",
      "Example 1\n",
      "INPUT >> E G F D F E B\n",
      "PRED >> F H G E G F C\n",
      "TRUE >> F H G E G F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> F A G C G G\n",
      "PRED >> G B H D H H\n",
      "TRUE >> G B H D H H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B H E F G B\n",
      "PRED >> C I F G H C\n",
      "TRUE >> C I F G H C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B A C C C C\n",
      "PRED >> C B D D D D\n",
      "TRUE >> C B D D D D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> F E E\n",
      "PRED >> G F F\n",
      "TRUE >> G F F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1364 (time: 14.80)\n",
      "Step 200 average loss = 0.1184 (time: 7.25)\n",
      "Step 300 average loss = 0.1643 (time: 7.62)\n",
      "Step 400 average loss = 0.1551 (time: 7.45)\n",
      "Step 500 average loss = 0.1267 (time: 7.42)\n",
      "  [LR DECAY] from 0.0005133421 to 0.0004876750 at step 8400\n",
      "\n",
      "Epoch 14 loss = 0.1543\n",
      "\n",
      "Example 1\n",
      "INPUT >> E C D E E D B G\n",
      "PRED >> F D E F F E C\n",
      "TRUE >> F D E F F E C H\n",
      "[BLEU] 0.87\n",
      "\n",
      "Example 2\n",
      "INPUT >> F G G B D G D D\n",
      "PRED >> G H H C E H E\n",
      "TRUE >> G H H C E H E E\n",
      "[BLEU] 0.87\n",
      "\n",
      "Example 3\n",
      "INPUT >> G A D F E B F\n",
      "PRED >> H B E G F C G\n",
      "TRUE >> H B E G F C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B A A G G A\n",
      "PRED >> C B B H H B\n",
      "TRUE >> C B B H H B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> H D D E E A\n",
      "PRED >> I E E F F B\n",
      "TRUE >> I E E F F B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1348 (time: 15.11)\n",
      "Step 200 average loss = 0.1615 (time: 7.63)\n",
      "Step 300 average loss = 0.1584 (time: 7.53)\n",
      "Step 400 average loss = 0.1439 (time: 7.45)\n",
      "Step 500 average loss = 0.2057 (time: 7.64)\n",
      "  [LR DECAY] from 0.0004876750 to 0.0004632912 at step 9000\n",
      "\n",
      "Epoch 15 loss = 0.1521\n",
      "\n",
      "Example 1\n",
      "INPUT >> F E C D D C F A\n",
      "PRED >> G F D E E D G B\n",
      "TRUE >> G F D E E D G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C F A E H A H\n",
      "PRED >> D G B F I B I\n",
      "TRUE >> D G B F I B I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> B E G A B C\n",
      "PRED >> C F H B C D\n",
      "TRUE >> C F H B C D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> E H C E C\n",
      "PRED >> F I D F D\n",
      "TRUE >> F I D F D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> E B H\n",
      "PRED >> F C I\n",
      "TRUE >> F C I\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1472 (time: 14.82)\n",
      "Step 200 average loss = 0.1873 (time: 7.37)\n",
      "Step 300 average loss = 0.1799 (time: 7.66)\n",
      "Step 400 average loss = 0.1549 (time: 7.68)\n",
      "Step 500 average loss = 0.1598 (time: 7.06)\n",
      "  [LR DECAY] from 0.0004632912 to 0.0004401267 at step 9600\n",
      "\n",
      "Epoch 16 loss = 0.1523\n",
      "\n",
      "Example 1\n",
      "INPUT >> H A F B C A A A\n",
      "PRED >> I B G C D B B B\n",
      "TRUE >> I B G C D B B B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> E D A A C A B\n",
      "PRED >> F E B B D B C\n",
      "TRUE >> F E B B D B C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> D A E G B E B\n",
      "PRED >> E B F H C F C\n",
      "TRUE >> E B F H C F C\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> G G H C B C\n",
      "PRED >> H H I D C D\n",
      "TRUE >> H H I D C D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B A G B F F\n",
      "PRED >> C B H C G G\n",
      "TRUE >> C B H C G G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Step 100 average loss = 0.1551 (time: 15.03)\n",
      "Step 200 average loss = 0.1458 (time: 7.57)\n",
      "Step 300 average loss = 0.1314 (time: 7.65)\n",
      "Step 400 average loss = 0.1420 (time: 7.60)\n",
      "Step 500 average loss = 0.1608 (time: 7.38)\n",
      "  [LR DECAY] from 0.0004401267 to 0.0004181203 at step 10200\n",
      "\n",
      "Epoch 17 loss = 0.1521\n",
      "\n",
      "Example 1\n",
      "INPUT >> F F H B B B A\n",
      "PRED >> G G I C C C B\n",
      "TRUE >> G G I C C C B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> G D F H B F\n",
      "PRED >> H E G I C G\n",
      "TRUE >> H E G I C G\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> D C F\n",
      "PRED >> E D G\n",
      "TRUE >> E D G\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 4\n",
      "INPUT >> C G A\n",
      "PRED >> D H B\n",
      "TRUE >> D H B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> B F A\n",
      "PRED >> C G B\n",
      "TRUE >> C G B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1269 (time: 15.27)\n",
      "Step 200 average loss = 0.1954 (time: 7.38)\n",
      "Step 300 average loss = 0.1644 (time: 7.50)\n",
      "Step 400 average loss = 0.1860 (time: 7.44)\n",
      "Step 500 average loss = 0.1316 (time: 7.60)\n",
      "  [LR DECAY] from 0.0004181203 to 0.0003972143 at step 10800\n",
      "\n",
      "Epoch 18 loss = 0.1525\n",
      "\n",
      "Example 1\n",
      "INPUT >> F A F D D F B G\n",
      "PRED >> G B G E E G C H\n",
      "TRUE >> G B G E E G C H\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> C E A D C E C\n",
      "PRED >> D F B E D F D\n",
      "TRUE >> D F B E D F D\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> H H B F A F D\n",
      "PRED >> I I C G B G E\n",
      "TRUE >> I I C G B G E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> C E G F A\n",
      "PRED >> D F H G B\n",
      "TRUE >> D F H G B\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> D E B\n",
      "PRED >> E F C\n",
      "TRUE >> E F C\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1363 (time: 15.30)\n",
      "Step 200 average loss = 0.1544 (time: 7.46)\n",
      "Step 300 average loss = 0.1502 (time: 7.33)\n",
      "Step 400 average loss = 0.1490 (time: 7.35)\n",
      "Step 500 average loss = 0.1431 (time: 6.68)\n",
      "  [LR DECAY] from 0.0003972143 to 0.0003773536 at step 11400\n",
      "\n",
      "Epoch 19 loss = 0.1528\n",
      "\n",
      "Example 1\n",
      "INPUT >> E D E B C B A H\n",
      "PRED >> F E F C D C B I\n",
      "TRUE >> F E F C D C B I\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> E G D A E G H E\n",
      "PRED >> F H E B F H I F\n",
      "TRUE >> F H E B F H I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> E D C G F D E\n",
      "PRED >> F E D H G E F\n",
      "TRUE >> F E D H G E F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> B E D D D\n",
      "PRED >> C F E E E\n",
      "TRUE >> C F E E E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 5\n",
      "INPUT >> B E A\n",
      "PRED >> C F B\n",
      "TRUE >> C F B\n",
      "[BLEU] 0.84\n",
      "\n",
      "Step 100 average loss = 0.1240 (time: 14.52)\n",
      "Step 200 average loss = 0.1478 (time: 7.35)\n",
      "Step 300 average loss = 0.1542 (time: 7.61)\n",
      "Step 400 average loss = 0.1461 (time: 7.54)\n",
      "Step 500 average loss = 0.1469 (time: 7.63)\n",
      "  [LR DECAY] from 0.0003773536 to 0.0003584859 at step 12000\n",
      "\n",
      "Epoch 20 loss = 0.1533\n",
      "\n",
      "Example 1\n",
      "INPUT >> B E C C C H E\n",
      "PRED >> C F D D D I F\n",
      "TRUE >> C F D D D I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 2\n",
      "INPUT >> H B G C F C D\n",
      "PRED >> I C H D G D E\n",
      "TRUE >> I C H D G D E\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 3\n",
      "INPUT >> G E B H A H E\n",
      "PRED >> H F C I B I F\n",
      "TRUE >> H F C I B I F\n",
      "[BLEU] 1.00\n",
      "\n",
      "Example 4\n",
      "INPUT >> H E E\n",
      "PRED >> I F F\n",
      "TRUE >> I F F\n",
      "[BLEU] 0.84\n",
      "\n",
      "Example 5\n",
      "INPUT >> F B H\n",
      "PRED >> G C I\n",
      "TRUE >> G C I\n",
      "[BLEU] 0.84\n",
      "\n",
      "\n",
      "Grand average loss = 0.1583\n",
      "\n",
      "Average BLEU score over 50 examples is 0.9523\n",
      "CPU times: user 9min 54s, sys: 4min 56s, total: 14min 50s\n",
      "Wall time: 14min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type AttentionDecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s2s = Seq2Seq(indexer, trainPairs, trainLens, testPairs, testLens, \n",
    "              batchSize=5, hiddenSize=100,\n",
    "              nLayers=2, dropout=0.3, residual=True, \n",
    "              lr=1e-3, lrDecay=0.95, lrDecayFreq=600,\n",
    "              l2Reg=0.001,\n",
    "              enforcingRatio=0.8, clip=20.0,\n",
    "              maxDecodingLen=10,\n",
    "              resultSavePath='toy/results.txt')\n",
    "s2s.train(nEpochs=20, epochSize=600, printEvery=100)\n",
    "s2s.evaluate(nBatches=10, saveResults=True)\n",
    "torch.save(s2s, 'toy/seq2seq.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APPENDIX. SCRIPTIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', help=\"Path to formatted input to Seq2Seq. See Seq2Seq documentation.\",\n",
    "                        type=str,\n",
    "                        default='toy/toy_formatted.p')\n",
    "    parser.add_argument('--model_save_path', help=\"Path to saved Seq2Seq model.\",\n",
    "                        type=str,\n",
    "                        default='toy/seq2seq.ckpt')\n",
    "    parser.add_argument('--result_save_path', help=\"Path to save results.\",\n",
    "                        type=str, default='toy/results.txt')\n",
    "    parser.add_argument('--clear_prev_result', help=\"Delete previously output results.\",\n",
    "                        type=bool, default=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=5)\n",
    "    parser.add_argument('--hidden_size', type=int, default=10)\n",
    "    parser.add_argument('--n_layers', type=int, default=2)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--residual', type=bool, default=True)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.95)\n",
    "    parser.add_argument('--lr_dacay_freq', type=int, default=500)\n",
    "    parser.add_argument('--l2_reg', type=float, default=0.5)\n",
    "    parser.add_argument('--enforce_ratio', type=float, default=0.5)\n",
    "    parser.add_argument('--clip', type=float, default=5.0)\n",
    "    parser.add_argument('--n_epochs', type=int, default=1)\n",
    "    parser.add_argument('--epoch_size', type=int, default=10)\n",
    "    parser.add_argument('--print_every', type=int, default=5)\n",
    "    parser.add_argument('--n_eval_batches', type=int, default=10)\n",
    "    parser.add_argument('--max_decoding_length', type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    option = raw_input(\"OPTIONS: r(etrain)/c(ontinue).\\nr: Retrain new model\\nc: Continue training of saved model.\\n\")\n",
    "    if option=='r':\n",
    "        if not os.path.exists(args.data_path):\n",
    "            dataBuildMsg = \"\"\"\n",
    "                Data does not exist. Please make it in the following format:\\n\n",
    "                indexer: Indexer object which has files loaded (each is a file with space-separated words as lines).\n",
    "                trainPairs, testPairs: each is a list of pairs of word index list.\n",
    "                trainLens, testLens: each is a list of pairs of length of word index list.\\n\n",
    "                The order is indexer, trainPairs, trainLens, testPairs, testLens. Pickle it with dill.\\n\n",
    "            \"\"\"\n",
    "            raise Exception(dataBuildMsg)\n",
    "        else:\n",
    "            if os.path.exists(args.model_save_path):\n",
    "                option = raw_input(\"Model exists. Hit c(ontinue) to overwrite it, (q)uit to quit.\\n\")\n",
    "                if option=='q':\n",
    "                    exit(0)\n",
    "            indexer, trainPairs, trainLens, testPairs, testLens = dill.load(open(args.data_path, 'rb'))\n",
    "            model = Seq2Seq(indexer, trainPairs, trainLens, testPairs, testLens,\n",
    "                            args.batch_size, args.hidden_size, \n",
    "                            args.n_layers, args.dropout, args.residual,\n",
    "                            args.lr, args.lr_decay, args.lr_decay_freq,\n",
    "                            args.l2_reg,\n",
    "                            args.enforce_ratio, args.clip,\n",
    "                            args.max_decoding_length,\n",
    "                            args.result_save_path)\n",
    "    elif option=='c':\n",
    "        model = torch.load(args.model_save_path)\n",
    "    else:\n",
    "        raise Exception(\"Eneter either r/c.\")\n",
    "        exit(1)\n",
    "    \n",
    "    if args.clear_prev_result and os.path.exists(args.result_save_path):\n",
    "        os.remove(args.result_save_path)\n",
    "    \n",
    "    model.train(args.n_epochs, args.epoch_size, args.print_every)\n",
    "    model.evaluate(args.n_eval_batches)\n",
    "    torch.save(model, args.model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
