{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: translating number words into numbers (e.g. one two three -> 1 2 3)\n",
    "\n",
    "* Seq2seq as in Bahdanau et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom import path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jacobsuwang/Documents/UTA2018/NEURAL-NETS/ATTENTION/CODE/01-import-folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "\n",
    "import utils\n",
    "\n",
    "vocab = set(['PAD','EOS','1','2','3','4','5','one','two','three','four','five'])\n",
    "word2idx = {'PAD':0,'EOS':1,'1':2,'2':3,'3':4,'4':5,'5':6,\n",
    "            'one':7,'two':8,'three':9,'four':10,'five':11}\n",
    "idx2word = {idx:word for word,idx in word2idx.iteritems()}\n",
    "word2digit_translate = {'one':'1','two':'2','three':'3',\n",
    "                        'four':'4','five':'5'}\n",
    "word2digit_translate_byidx = {7:2,8:3,9:4,10:5,11:6}\n",
    "\n",
    "def code_sequence(s):\n",
    "    '''\n",
    "    Take a sentence, convert it to a list of words (in vocab), \n",
    "    then return idx encoding.\n",
    "    '''\n",
    "    seq = s.split()\n",
    "    return [word2idx[word] for word in seq]\n",
    "\n",
    "def decode_sequence(l):\n",
    "    '''\n",
    "    Take a list of indices, return words.\n",
    "    '''\n",
    "    return ' '.join([idx2word[idx] for idx in l])\n",
    "\n",
    "def encode(data):\n",
    "    '''\n",
    "    Take sentence data, encode it.\n",
    "    '''\n",
    "    return [code_sequence(dat) for dat in data]\n",
    "\n",
    "def to_readable(batch):\n",
    "    '''\n",
    "    Take a time-major batch of data, \n",
    "    return a list of translated words.\n",
    "    '''\n",
    "    batch_t = batch.transpose() # time-major -> batch-major\n",
    "    return [decode_sequence(dat) for dat in batch_t]\n",
    "\n",
    "# To transform data (i.e. list of sentences as wordlists) into\n",
    "# input data, feed it to utils.batch\n",
    "# sample results:\n",
    "# utils.batch([code_sequence('1 2 3')])\n",
    "# (array([[1],\n",
    "#         [2],\n",
    "#         [3]], dtype=int32), [3])\n",
    "# that is, a tuple (time major with shape [max_time, batch_size])\n",
    "# so with a batch of two, we get\n",
    "# (array([[1, 4],\n",
    "#         [2, 5],\n",
    "#         [3, 5]], dtype=int32), [3, 3])\n",
    "\n",
    "# Data generator\n",
    "\n",
    "def random_length(len_from, len_to):\n",
    "    if len_from == len_to:\n",
    "        return len_from\n",
    "    return np.random.randint(len_from, len_to)\n",
    "\n",
    "def random_batch(input_length_from, input_length_to,\n",
    "                 output_length_from, output_length_to,\n",
    "                 seq_length_from, seq_length_to,\n",
    "                 batch_size):\n",
    "    if input_length_from > input_length_to or \\\n",
    "        output_length_from > output_length_to:\n",
    "        raise ValueError('length_from > length_to')\n",
    "\n",
    "    input_batch = [np.random.randint(low=input_length_from,\n",
    "                                     high=input_length_to,\n",
    "                                     size=random_length(seq_length_from,\n",
    "                                                        seq_length_to)).tolist()\n",
    "                   for _ in range(batch_size)]\n",
    "    output_batch = [[word2digit_translate_byidx[idx] for idx in input_dat]\n",
    "                     for input_dat in input_batch]\n",
    "    return input_batch, output_batch\n",
    "      \n",
    "# Example:\n",
    "# digit_from = 2\n",
    "# digit_to = 6+1\n",
    "# word_from = 7\n",
    "# word_to = 11+1\n",
    "# a,b = random_batch(word_from,word_to,digit_from,digit_to,batch_size=2)\n",
    "# print a\n",
    "# [[11, 7, 11, 11, 10, 11, 9, 10, 10, 8, 9], [10, 7, 7, 9, 8, 9, 7, 8, 11]] <- indices of num words\n",
    "# print b\n",
    "# [[6, 2, 6, 6, 5, 6, 4, 5, 5, 3, 4], [5, 2, 2, 4, 3, 4, 2, 3, 6]] <- indices of their digit translation\n",
    "# print [decode_sequence(a_) for a_ in a]\n",
    "# ['five one five five four five three four four two three', 'four one one three two three one two five']\n",
    "# print [decode_sequence(b_) for b_ in b]\n",
    "# ['5 1 5 5 4 5 3 4 4 2 3', '4 1 1 3 2 3 1 2 5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse as embedding_lookup_unique\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.42388939857\n",
      "  sample 1:\n",
      "    enc input     > four three five four two three three\n",
      "    dec train predicted > EOS EOS EOS 3 3 3 3 EOS\n",
      "  sample 2:\n",
      "    enc input     > one three one five five four four\n",
      "    dec train predicted > EOS EOS EOS EOS one 3 3 3\n",
      "  sample 3:\n",
      "    enc input     > two one one three PAD PAD PAD\n",
      "    dec train predicted > EOS EOS EOS EOS EOS PAD PAD PAD\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.264123678207\n",
      "  sample 1:\n",
      "    enc input     > two one two two one five PAD\n",
      "    dec train predicted > 2 1 2 2 1 5 EOS PAD\n",
      "  sample 2:\n",
      "    enc input     > one five three three PAD PAD PAD\n",
      "    dec train predicted > 1 5 3 3 EOS PAD PAD PAD\n",
      "  sample 3:\n",
      "    enc input     > four three three two three PAD PAD\n",
      "    dec train predicted > 4 3 3 2 3 EOS PAD PAD\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.266255199909\n",
      "  sample 1:\n",
      "    enc input     > four three five PAD PAD PAD PAD\n",
      "    dec train predicted > 4 3 5 EOS PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    enc input     > three five one four one PAD PAD\n",
      "    dec train predicted > 3 5 1 4 1 EOS PAD PAD\n",
      "  sample 3:\n",
      "    enc input     > four two four four PAD PAD PAD\n",
      "    dec train predicted > 4 2 4 4 4 PAD PAD PAD\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.0413800701499\n",
      "  sample 1:\n",
      "    enc input     > three five two PAD PAD PAD PAD\n",
      "    dec train predicted > 3 5 2 EOS PAD PAD PAD PAD\n",
      "  sample 2:\n",
      "    enc input     > five two two three one one two\n",
      "    dec train predicted > 5 2 2 3 1 1 2 EOS\n",
      "  sample 3:\n",
      "    enc input     > five five one one three two three\n",
      "    dec train predicted > 5 5 1 1 3 2 3 EOS\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.0264966432005\n",
      "  sample 1:\n",
      "    enc input     > four two two five three PAD PAD\n",
      "    dec train predicted > 4 2 2 5 3 EOS PAD PAD\n",
      "  sample 2:\n",
      "    enc input     > five three three PAD PAD PAD PAD\n",
      "    dec train predicted > 5 3 3 EOS PAD PAD PAD PAD\n",
      "  sample 3:\n",
      "    enc input     > five four three one four three one\n",
      "    dec train predicted > 5 4 3 1 4 3 1 EOS\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.0137165505439\n",
      "  sample 1:\n",
      "    enc input     > two two two four five five four\n",
      "    dec train predicted > 2 2 2 4 5 5 4 EOS\n",
      "  sample 2:\n",
      "    enc input     > five one three two five four four\n",
      "    dec train predicted > 5 1 3 2 5 4 4 EOS\n",
      "  sample 3:\n",
      "    enc input     > five four four one two two PAD\n",
      "    dec train predicted > 5 4 4 1 2 2 EOS PAD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Set configuration\n",
    "\n",
    "digit_from = 2\n",
    "digit_to = 6+1\n",
    "word_from = 7\n",
    "word_to = 11+1\n",
    "seqlen_from = 3\n",
    "seqlen_to = 8\n",
    "\n",
    "batch_size = 10\n",
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 10\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2 # because encoder is going to be bidirectional.\n",
    "\n",
    "attention = True     # togglable\n",
    "bidirectional = True # currently hardcoded\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "# _init_placeholder()\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None,None), dtype=tf.int32, name='encoder_inputs') # [max_time, batch_size]\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length') \n",
    "decoder_targets = tf.placeholder(shape=(None,None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_targets_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_length')\n",
    "\n",
    "# _init_decoder_train_connectors()\n",
    "\n",
    "with tf.name_scope('DecoderTrainFeeds'):\n",
    "    sequence_size, batch_size_ = tf.unstack(tf.shape(decoder_targets)) # [max_time, batch_size]\n",
    "    EOS_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * word2idx['EOS']\n",
    "    PAD_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * word2idx['PAD']\n",
    "    decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=0) # add EOS to the beginning.\n",
    "    decoder_train_length = decoder_targets_length + 1 # and adjust length accordingly.\n",
    "    decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=0) # add PAD to the end.\n",
    "    decoder_train_targets_seq_len, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "    decoder_train_targets_eos_mask = tf.one_hot(decoder_train_length - 1,\n",
    "                                                decoder_train_targets_seq_len,\n",
    "                                                on_value=word2idx['EOS'], off_value=word2idx['PAD'],\n",
    "                                                dtype=tf.int32)\n",
    "    decoder_train_targets_eos_mask = tf.transpose(decoder_train_targets_eos_mask, [1, 0]) # to [batch_size, max_time]?\n",
    "    decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                                   decoder_train_targets_eos_mask) # add EOS to end of target sequence\n",
    "    loss_weights = tf.ones([\n",
    "        batch_size,\n",
    "        tf.reduce_max(decoder_train_length)\n",
    "    ], dtype=tf.float32, name='loss_weights')\n",
    "\n",
    "# _init_embeddings()\n",
    "\n",
    "with tf.variable_scope('embedding') as scope:\n",
    "    sqrt3 = math.sqrt(3) # unif(-sqrt(3),sqrt(3)) has var = 1.\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name='embedding_matrix',\n",
    "        shape=[vocab_size, input_embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, encoder_inputs)\n",
    "    decoder_train_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, decoder_train_inputs)\n",
    "\n",
    "# _init_bidirectional_encoder()\n",
    "\n",
    "with tf.variable_scope('BidirectionalEncoder') as scope:\n",
    "    encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "    ((encoder_fw_outputs,encoder_bw_outputs), # both have [max_time, batch_size, emb_size]\n",
    "     (encoder_fw_state,encoder_bw_state)) = ( # state tuples: (c=[batch_size,emb_size],h=same)\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                            cell_bw=encoder_cell,\n",
    "                                            inputs=encoder_inputs_embedded,\n",
    "                                            sequence_length=encoder_inputs_length,\n",
    "                                            dtype=tf.float32, time_major=True)\n",
    "        )  \n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2) # concat on emb dim.\n",
    "    if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "        encoder_state_c = tf.concat((encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "        encoder_state_h = tf.concat((encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "        encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "    elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "        self.encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 1, name='bidirectional_concat')\n",
    "    \n",
    "# _init_decoder()\n",
    "\n",
    "with tf.variable_scope('Decoder') as scope:\n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    if not attention:\n",
    "        decoder_fn_train = seq2seq.simple_decoder_fn_train(encoder_state=encoder_state)\n",
    "        decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=encoder_state,\n",
    "            embeddings=embedding_matrix,\n",
    "            start_of_sequence_id=word2idx['EOS'],\n",
    "            end_of_sequence_id=word2idx['EOS'],\n",
    "            maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=vocab_size\n",
    "        )\n",
    "    else:\n",
    "        attention_states = tf.transpose(encoder_outputs, [1, 0, 2]) # -> [batch_size, max_time, num_units]\n",
    "        (attention_keys,\n",
    "         attention_values,\n",
    "         attention_score_fn,\n",
    "         attention_construct_fn) = seq2seq.prepare_attention(\n",
    "            attention_states=attention_states,\n",
    "            attention_option='bahdanau',\n",
    "            num_units=decoder_hidden_units\n",
    "        )\n",
    "        decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "            encoder_state=encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            name='attention_decoder'\n",
    "        )\n",
    "        decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "            output_fn=output_fn,\n",
    "            encoder_state=encoder_state,\n",
    "            attention_keys=attention_keys,\n",
    "            attention_values=attention_values,\n",
    "            attention_score_fn=attention_score_fn,\n",
    "            attention_construct_fn=attention_construct_fn,\n",
    "            embeddings=embedding_matrix,\n",
    "            start_of_sequence_id=word2idx['EOS'],\n",
    "            end_of_sequence_id=word2idx['EOS'],\n",
    "            maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "            num_decoder_symbols=vocab_size\n",
    "        )\n",
    "        (decoder_outputs_train,\n",
    "         decoder_state_train,\n",
    "         decoder_context_state_train) = (\n",
    "            seq2seq.dynamic_rnn_decoder(\n",
    "                cell=decoder_cell,\n",
    "                decoder_fn=decoder_fn_train,\n",
    "                inputs=decoder_train_inputs_embedded,\n",
    "                sequence_length=decoder_train_length,\n",
    "                time_major=True,\n",
    "                scope=scope\n",
    "            )\n",
    "        )\n",
    "        decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "        decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction_train')\n",
    "        scope.reuse_variables()\n",
    "        (decoder_logits_inference,\n",
    "         decoder_state_inference,\n",
    "         decoder_context_state_inference) = (\n",
    "            seq2seq.dynamic_rnn_decoder(\n",
    "                cell=decoder_cell,\n",
    "                decoder_fn=decoder_fn_inference,\n",
    "                time_major=True,\n",
    "                scope=scope\n",
    "            )\n",
    "        )\n",
    "        decoder_prediction_inference = tf.argmax(decoder_logits_inference, axis=-1, name='decoder_prediction_inference')\n",
    "    \n",
    "# _init_optimizer()\n",
    "\n",
    "logits = tf.transpose(decoder_logits_train, [1, 0, 2])\n",
    "targets = tf.transpose(decoder_train_targets, [1, 0])\n",
    "loss = seq2seq.sequence_loss(logits=logits, targets=targets, weights=loss_weights)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# run training\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "def make_train_inputs(input_seq, target_seq):\n",
    "    # batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "        # this is called in ematvey's code as:\n",
    "        # batch_data = next(batches)\n",
    "        # fd = model.make_train_inputs(batch_data, batch_data)\n",
    "    inputs_, inputs_length_ = utils.batch(input_seq) \n",
    "        # equiv encoder_inputs_, encoder_inputs_lengths_ = utils.batch(batch_enc)\n",
    "    targets_, targets_length_ = utils.batch(target_seq)\n",
    "        # equiv decoder_targets_, _ = utils.batch([seq + [word2idx['EOS']] + [word2idx['PAD']]*2 for seq in batch_dec])\n",
    "        # the EOS addition is done in a function above, so no need here.\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: targets_,\n",
    "        decoder_targets_length: targets_length_\n",
    "    }\n",
    "\n",
    "loss_track = []\n",
    "max_batches = 5000\n",
    "batches_in_epoch=1000\n",
    "try:\n",
    "    for batch in range(max_batches+1):\n",
    "        batch_enc, batch_dec = random_batch(word_from,word_to,digit_from,digit_to,seqlen_from,seqlen_to,batch_size)\n",
    "        fd = make_train_inputs(batch_enc, batch_dec) \n",
    "            # ematvey: ..(batch_data, batch_data)\n",
    "            # because he does copy task, and i do translation.\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            for i, (e_in, dt_pred) in enumerate(zip(\n",
    "                    fd[encoder_inputs].T,\n",
    "                    sess.run(decoder_prediction_train, fd).T\n",
    "                )):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    enc input     > {}'.format(decode_sequence(e_in)))\n",
    "                print('    dec train predicted > {}'.format(decode_sequence(dt_pred)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
